<!DOCTYPE html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.w3.org/2002/06/xhtml2/ http://www.w3.org/MarkUp/SCHEMA/xhtml2.xsd" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<link href="Styles/Style00.css" rel="stylesheet" type="text/css" />
<link href="Styles/Style01.css" rel="stylesheet" type="text/css" />

<style type="text/css">body{margin:1em;background-color:transparent!important;}#sbo-rt-content *{text-indent:0pt!important;}#sbo-rt-content .bq{margin-right:1em!important;}#sbo-rt-content *{word-wrap:break-word!important;word-break:break-word!important;}#sbo-rt-content table,#sbo-rt-content pre{overflow-x:unset!important;overflow:unset!important;overflow-y:unset!important;white-space:pre-wrap!important;}</style></head>
<body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 13. Loading and Preprocessing Data with TensorFlow"><div class="chapter" id="data_chapter">
<h1><span class="label">Chapter 13. </span>Loading and Preprocessing Data <span class="keep-together">with TensorFlow</span></h1>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46324181676640">
<h5>A Note for Early Release Readers</h5>
<p>With Early Release ebooks, you get books in their earliest form—the author’s raw and unedited content as they write—so you can take advantage of these technologies long before the official release of these titles.</p>

<p>This will be the 13th chapter of the final book. Notebooks are available on GitHub at <a href="https://github.com/ageron/handson-ml3"><em class="hyperlink">https://github.com/ageron/handson-ml3</em></a>. Datasets are available at <a href="https://github.com/ageron/data"><em class="hyperlink">https://github.com/ageron/data</em></a>.</p>

<p>If you have comments about how we might improve the content and/or examples in this book, or if you notice missing material within this chapter, please reach out to the editor at <a href="mailto:mcronin@oreilly.com">mcronin@oreilly.com</a>.</p>
</div></aside>

<p>In <a data-type="xref" href="ch02.xhtml#project_chapter">Chapter 2</a>, we saw that loading and preprocessing data is an important part of any Machine Learning project. We used Pandas to load and explore the (modified) California housing dataset—which was stored in a CSV file—and we used Scikit-Learn’s transformers for preprocessing. These tools are quite convenient, and you will probably be using them often, especially when exploring and experimenting with data.</p>

<p>However, when training TensorFlow models on large datasets, you may prefer to use TensorFlow’s own data loading and preprocessing API, called <em>tf.data</em>. Indeed, it is capable of loading and preprocessing data extremely efficiently, reading from multiple files in parallel using multithreading and queuing, shuffling and batching samples, and more. Plus, it can do all of this on the fly, loading and preprocessing the next batch of data across multiple CPU cores while your GPU is busy training the current batch. The tf.data API lets you handle datasets that don’t fit in memory, and it allows you to make full use of your hardware resources, thereby speeding up training. Off the shelf, the tf.data API can read from text files (such as CSV files), binary files with fixed-size records, and binary files that use TensorFlow’s TFRecord format, which supports records of varying sizes. TFRecord is a flexible and efficient binary format usually containing protocol buffers (an open source binary format). The tf.data API also has support for reading from SQL databases. Moreover, many open source extensions are available to read from all sorts of data sources, such as Google’s BigQuery service (see <a href="https://www.tensorflow.org/io"><em class="hyperlink">https://www.tensorflow.org/io</em></a>).</p>

<p>Keras also comes with powerful yet easy-to-use preprocessing layers that can be embedded in your models: this way, when you deploy a model to production, it will be able to ingest raw data directly, without having to add any additional preprocessing code. This eliminates the risk of mismatch between the preprocessing code used during training and the preprocessing code used in production, which would likely cause <em>training/serving skew</em>. And if you deploy your model in multiple apps coded in different programming languages, you won’t have to reimplement the same preprocessing code multiple times, which also reduces the risk of mismatch.</p>

<p>As we will see, both APIs can be used jointly, for example to benefit from the efficient data loading offered by tf.data, and the convenience of the Keras preprocessing layers.</p>

<p>In this chapter, we will first cover the tf.data API and the TFRecord format, then we will explore the Keras preprocessing layers, and how to use them with the tf.data API. Lastly, we will take a quick look at a few related libraries that you may find useful to load and preprocess data, including TensorFlow Datasets and TensorFlow Hub. So let’s get started!</p>






<section data-type="sect1" data-pdf-bookmark="The tf.data API"><div class="sect1" id="idm46324181664704">
<h1>The tf.data API</h1>

<p>The<a data-type="indexterm" data-primary="TensorFlow, data loading and preprocessing" data-secondary="Data API" id="TFload13"/><a data-type="indexterm" data-primary="Data API (TensorFlow)" data-secondary="overview of" id="idm46324181661664"/><a data-type="indexterm" data-primary="datasets, defined" id="idm46324181660720"/> whole tf.data API revolves around the concept of a <em>dataset</em>: as you might suspect, this represents a sequence of data items. Usually you will use datasets that gradually read data from disk, but for simplicity let’s create a dataset entirely in RAM using <code>tf.data.Dataset.from_tensor_slices()</code>:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">tensorflow</code> <code class="k">as</code> <code class="nn">tf</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">X</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">range</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code>  <code class="c1"># any data tensor</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">dataset</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">Dataset</code><code class="o">.</code><code class="n">from_tensor_slices</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">dataset</code>
<code class="go">&lt;TensorSliceDataset shapes: (), types: tf.int32&gt;</code></pre>

<p>The <code>from_tensor_slices()</code> function takes a tensor and creates a <code>tf.data.Dataset</code> whose elements are all the slices of <code>X</code> along the first dimension, so this dataset contains 10 items: tensors 0, 1, 2, …, 9. In this case we would have obtained the same dataset if we had used <code>tf.data.Dataset.range(10)</code> (except the elements would be 64-bit integers instead of 32-bit integers).</p>

<p>You can simply iterate over a dataset’s items like this:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">item</code> <code class="ow">in</code> <code class="n">dataset</code><code class="p">:</code>
<code class="gp">... </code>    <code class="nb">print</code><code class="p">(</code><code class="n">item</code><code class="p">)</code>
<code class="gp">...</code>
<code class="go">tf.Tensor(0, shape=(), dtype=int32)</code>
<code class="go">tf.Tensor(1, shape=(), dtype=int32)</code>
<code class="go">[...]</code>
<code class="go">tf.Tensor(9, shape=(), dtype=int32)</code></pre>

<p>A dataset may also contain tuples of tensors, or dictionaries of name/tensor pairs, or even nested tuples and dictionaries of tensors. When slicing a tuple, a dictionary, or a nested structure, the dataset will only slice the tensors it contains, but it will preserve the tuple/dictionary structure. For example:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">X_nested</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"a"</code><code class="p">:</code> <code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code> <code class="p">[</code><code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">6</code><code class="p">]),</code> <code class="s2">"b"</code><code class="p">:</code> <code class="p">[</code><code class="mi">7</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="mi">9</code><code class="p">]}</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">dataset</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">Dataset</code><code class="o">.</code><code class="n">from_tensor_slices</code><code class="p">(</code><code class="n">X_nested</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">item</code> <code class="ow">in</code> <code class="n">dataset</code><code class="p">:</code>
<code class="gp">... </code>    <code class="nb">print</code><code class="p">(</code><code class="n">item</code><code class="p">)</code>
<code class="gp">...</code>
<code class="go">{'a': (&lt;tf.Tensor: [...]=1&gt;, &lt;tf.Tensor: [...]=4&gt;), 'b': &lt;tf.Tensor: [...]=7&gt;}</code>
<code class="go">{'a': (&lt;tf.Tensor: [...]=2&gt;, &lt;tf.Tensor: [...]=5&gt;), 'b': &lt;tf.Tensor: [...]=8&gt;}</code>
<code class="go">{'a': (&lt;tf.Tensor: [...]=3&gt;, &lt;tf.Tensor: [...]=6&gt;), 'b': &lt;tf.Tensor: [...]=9&gt;}</code></pre>








<section data-type="sect2" data-pdf-bookmark="Chaining Transformations"><div class="sect2" id="idm46324181525968">
<h2>Chaining Transformations</h2>

<p>Once<a data-type="indexterm" data-primary="Data API (TensorFlow)" data-secondary="chaining transformations" id="idm46324181507344"/><a data-type="indexterm" data-primary="transformations" data-secondary="chaining" id="idm46324181506400"/><a data-type="indexterm" data-primary="chaining transformations" id="idm46324181505456"/> you have a dataset, you can apply all sorts of transformations to it by calling its transformation methods. Each method returns a new dataset, so you can chain transformations like this (this chain is illustrated in <a data-type="xref" href="#chaining_transformations_diagram">Figure 13-1</a>):</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">dataset</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">Dataset</code><code class="o">.</code><code class="n">from_tensor_slices</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">range</code><code class="p">(</code><code class="mi">10</code><code class="p">))</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">repeat</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="mi">7</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">item</code> <code class="ow">in</code> <code class="n">dataset</code><code class="p">:</code>
<code class="gp">... </code>    <code class="nb">print</code><code class="p">(</code><code class="n">item</code><code class="p">)</code>
<code class="gp">...</code>
<code class="go">tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)</code>
<code class="go">tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)</code>
<code class="go">tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)</code>
<code class="go">tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)</code>
<code class="go">tf.Tensor([8 9], shape=(2,), dtype=int32)</code></pre>

<figure><div id="chaining_transformations_diagram" class="figure">
<img src="Images/mls3_1301.png" alt="mls3 1301" width="1706" height="646"/>
<h6><span class="label">Figure 13-1. </span>Chaining dataset transformations</h6>
</div></figure>

<p>In this example, we first call the <code>repeat()</code> method on the original dataset, and it returns a new dataset that will repeat the items of the original dataset three times. Of course, this will not copy all the data in memory three times! (If you call this method with no arguments, the new dataset will repeat the source dataset forever, so the code that iterates over the dataset will have to decide when to stop.) Then we call the <code>batch()</code> method on this new dataset, and again this creates a new dataset. This one will group the items of the previous dataset in batches of seven items. Finally, we iterate over the items of this final dataset. As you can see, the <code>batch()</code> method had to output a final batch of size two instead of seven, but you can call <code>batch()</code> with <code>drop_remainder=True</code> if you want it to drop this final batch so that all batches have the exact same size.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>The dataset methods do <em>not</em> modify datasets, they create new ones, so make sure to keep a reference to these new datasets (e.g., with <code>dataset = ...</code>), or else nothing will happen.</p>
</div>

<p>You can also transform the items by calling the <code>map()</code> method. For example, this creates a new dataset with all batches multiplied by two:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code> <code class="o">*</code> <code class="mi">2</code><code class="p">)</code>  <code class="c1"># x is a batch</code>
<code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">item</code> <code class="ow">in</code> <code class="n">dataset</code><code class="p">:</code>
<code class="gp">... </code>    <code class="nb">print</code><code class="p">(</code><code class="n">item</code><code class="p">)</code>
<code class="gp">...</code>
<code class="go">tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)</code>
<code class="go">tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)</code>
<code class="go">[...]</code></pre>

<p>This function is the one you will call to apply any preprocessing you want to your data. Sometimes this will include computations that can be quite intensive, such as reshaping or rotating an image, so you will usually want to spawn multiple threads to speed things up: it’s as simple as setting the <code>num_parallel_calls</code> argument to define the number of threads to run. Note that the function you pass to the <code>map()</code> method must be convertible to a TF Function (see <a data-type="xref" href="ch12.xhtml#tensorflow_chapter">Chapter 12</a>).</p>

<p>It is also possible to simply filter the dataset using the <code>filter()</code> method. For example, this code creates a dataset that only contains the batchs whose sum is greater than 50:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">filter</code><code class="p">(</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">tf</code><code class="o">.</code><code class="n">reduce_sum</code><code class="p">(</code><code class="n">x</code><code class="p">)</code> <code class="o">&gt;</code> <code class="mi">50</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">item</code> <code class="ow">in</code> <code class="n">dataset</code><code class="p">:</code>
<code class="gp">... </code>    <code class="nb">print</code><code class="p">(</code><code class="n">item</code><code class="p">)</code>
<code class="gp">...</code>
<code class="go">tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)</code>
<code class="go">tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)</code>
<code class="go">tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)</code></pre>

<p>You will often want to look at just a few items from a dataset. You can use the <code>take()</code> method for that:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">item</code> <code class="ow">in</code> <code class="n">dataset</code><code class="o">.</code><code class="n">take</code><code class="p">(</code><code class="mi">2</code><code class="p">):</code>
<code class="gp">... </code>    <code class="nb">print</code><code class="p">(</code><code class="n">item</code><code class="p">)</code>
<code class="gp">...</code>
<code class="go">tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)</code>
<code class="go">tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)</code></pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Shuffling the Data"><div class="sect2" id="idm46324181192368">
<h2>Shuffling the Data</h2>

<p>As<a data-type="indexterm" data-primary="Data API (TensorFlow)" data-secondary="shuffling data" id="idm46324181212336"/><a data-type="indexterm" data-primary="data" data-secondary="shuffling" id="idm46324181211328"/> you know, Gradient Descent works best when the instances in the training set are independent and identically distributed (see <a data-type="xref" href="ch04.xhtml#linear_models_chapter">Chapter 4</a>). A simple way to ensure this is to shuffle the instances, using the <code>shuffle()</code> method. It will create a new dataset that will start by filling up a buffer with the first items of the source dataset. Then, whenever it is asked for an item, it will pull one out randomly from the buffer and replace it with a fresh one from the source dataset, until it has iterated entirely through the source dataset. At this point it will continue to pull out items randomly from the buffer until it is empty. You must specify the buffer size, and it is important to make it large enough, or else shuffling will not be very effective.⁠<sup><a data-type="noteref" id="idm46324181209072-marker" href="ch13.xhtml#idm46324181209072">1</a></sup> Just don’t exceed the amount of RAM you have, and even if you have plenty of it, there’s no need to go beyond the dataset’s size. You can provide a random seed if you want the same random order every time you run your program. For example, the following code creates and displays a dataset containing the integers 0 to 9, repeated twice, shuffled using a buffer of size 4 and a random seed of 42, and batched with a batch size of 7:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">dataset</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">Dataset</code><code class="o">.</code><code class="n">range</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code><code class="o">.</code><code class="n">repeat</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">shuffle</code><code class="p">(</code><code class="n">buffer_size</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">seed</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="mi">7</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">item</code> <code class="ow">in</code> <code class="n">dataset</code><code class="p">:</code>
<code class="gp">... </code>    <code class="nb">print</code><code class="p">(</code><code class="n">item</code><code class="p">)</code>
<code class="gp">...</code>
<code class="go">tf.Tensor([3 0 1 6 2 5 7], shape=(7,), dtype=int64)</code>
<code class="go">tf.Tensor([8 4 1 9 4 2 3], shape=(7,), dtype=int64)</code>
<code class="go">tf.Tensor([7 5 0 8 9 6], shape=(6,), dtype=int64)</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>If you call <code>repeat()</code> on a shuffled dataset, by default it will generate a new order at every iteration. This is generally a good idea, but if you prefer to reuse the same order at each iteration (e.g., for tests or debugging), you can set <code>reshuffle_each_iteration=False</code> when calling <code>shuffle()</code>.</p>
</div>

<p>For<a data-type="indexterm" data-primary="shuffling-buffer approach" id="idm46324181135792"/> a large dataset that does not fit in memory, this simple shuffling-buffer approach may not be sufficient, since the buffer will be small compared to the dataset. One solution is to shuffle the source data itself (for example, on Linux you can shuffle text files using the <code>shuf</code> command). This will definitely improve shuffling a lot! Even if the source data is shuffled, you will usually want to shuffle it some more, or else the same order will be repeated at each epoch, and the model may end up being biased (e.g., due to some spurious patterns present by chance in the source data’s order). To shuffle the instances some more, a common approach is to split the source data into multiple files, then read them in a random order during training. However, instances located in the same file will still end up close to each other. To avoid this you can pick multiple files randomly and read them simultaneously, interleaving their records. Then on top of that you can add a shuffling buffer using the <code>shuffle()</code> method. If all this sounds like a lot of work, don’t worry: the tf.data API makes all this possible in just a few lines of code. Let’s see how to do this.</p>










<section data-type="sect3" data-pdf-bookmark="Interleaving lines from multiple files"><div class="sect3" id="idm46324181098640">
<h3>Interleaving lines from multiple files</h3>

<p>First, let’s suppose that you’ve loaded the California housing dataset, shuffled it (unless it was already shuffled), and split it into a training set, a validation set, and a test set. Then you split each set into many CSV files that each look like this (each row contains eight input features plus the target median house value):</p>

<pre data-type="programlisting">MedInc,HouseAge,AveRooms,AveBedrms,Popul…,AveOccup,Lat…,Long…,MedianHouseValue
3.5214,15.0,3.050,1.107,1447.0,1.606,37.63,-122.43,1.442
5.3275,5.0,6.490,0.991,3464.0,3.443,33.69,-117.39,1.687
3.1,29.0,7.542,1.592,1328.0,2.251,38.44,-122.98,1.621
[...]</pre>

<p>Let’s also suppose <code>train_filepaths</code> contains the list of training file paths (and you also have <code>valid_filepaths</code> and <code>test_filepaths</code>):</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">train_filepaths</code>
<code class="go">['datasets/housing/my_train_00.csv', 'datasets/housing/my_train_01.csv', ...]</code></pre>

<p>Alternatively, you could use file patterns; for example, <code>train_filepaths = "datasets/housing/my_train_*.csv"</code>. Now let’s create a dataset containing only these file paths:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">filepath_dataset</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">Dataset</code><code class="o">.</code><code class="n">list_files</code><code class="p">(</code><code class="n">train_filepaths</code><code class="p">,</code> <code class="n">seed</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code></pre>

<p>By default, the <code>list_files()</code> function returns a dataset that shuffles the file paths. In general this is a good thing, but you can set <code>shuffle=False</code> if you do not want that for some reason.</p>

<p>Next, you can call the <code>interleave()</code> method to read from five files at a time and interleave their lines. You can also skip the first line of each file—which is the header row—using the <code>skip()</code> method):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">n_readers</code> <code class="o">=</code> <code class="mi">5</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">filepath_dataset</code><code class="o">.</code><code class="n">interleave</code><code class="p">(</code>
    <code class="k">lambda</code> <code class="n">filepath</code><code class="p">:</code> <code class="n">tf</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">TextLineDataset</code><code class="p">(</code><code class="n">filepath</code><code class="p">)</code><code class="o">.</code><code class="n">skip</code><code class="p">(</code><code class="mi">1</code><code class="p">),</code>
    <code class="n">cycle_length</code><code class="o">=</code><code class="n">n_readers</code><code class="p">)</code></pre>

<p>The <code>interleave()</code> method will create a dataset that will pull five file paths from the <code>filepath_dataset</code>, and for each one it will call the function you gave it (a lambda in this example) to create a new dataset (in this case a <code>TextLineDataset</code>). To be clear, at this stage there will be seven datasets in all: the filepath dataset, the interleave dataset, and the five <code>TextLineDataset</code>s created internally by the interleave dataset. When we iterate over the interleave dataset, it will cycle through these five <code>TextLineDataset</code>s, reading one line at a time from each until all datasets are out of items. Then it will fetch the next five file paths from the <code>filepath_dataset</code> and interleave them the same way, and so on until it runs out of file paths. For interleaving to work best, it is preferable to have files of identical length; otherwise the end of the longest file will not be interleaved.</p>

<p>By default, <code>interleave()</code> does not use parallelism; it just reads one line at a time from each file, sequentially. If you want it to actually read files in parallel, you can set the <code>interleave()</code> method’s <code>num_parallel_calls</code> argument to the number of threads you want (recall that the <code>map()</code> method also has this argument). You can even set it to <code>tf.data.AUTOTUNE</code> to make TensorFlow choose the right number of threads dynamically based on the available CPU. Let’s look at what the dataset contains now:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">line</code> <code class="ow">in</code> <code class="n">dataset</code><code class="o">.</code><code class="n">take</code><code class="p">(</code><code class="mi">5</code><code class="p">):</code>
<code class="gp">... </code>    <code class="nb">print</code><code class="p">(</code><code class="n">line</code><code class="p">)</code>
<code class="gp">...</code>
<code class="go">tf.Tensor(b'4.5909,16.0,[...],33.63,-117.71,2.418', shape=(), dtype=string)</code>
<code class="go">tf.Tensor(b'2.4792,24.0,[...],34.18,-118.38,2.0', shape=(), dtype=string)</code>
<code class="go">tf.Tensor(b'4.2708,45.0,[...],37.48,-122.19,2.67', shape=(), dtype=string)</code>
<code class="go">tf.Tensor(b'2.1856,41.0,[...],32.76,-117.12,1.205', shape=(), dtype=string)</code>
<code class="go">tf.Tensor(b'4.1812,52.0,[...],33.73,-118.31,3.215', shape=(), dtype=string)</code></pre>

<p>These are the first rows (ignoring the header row) of five CSV files, chosen randomly. Looks good!</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>It’s possible to pass a list of file paths to the <code>TextLineDataset</code> constructor: it will go through each file in order, line by line. If you also set the <code>num_parallel_reads</code> argument to a number greater than one, then the dataset will read that number of files in parallel and interleave their lines (without having to call the <code>interleave()</code> method). However, it will <em>not</em> shuffle the files, nor will it skip the header lines.</p>
</div>

<p>Now that we have a dataset that returns each instance as a tensor containing a byte string, we need to do a bit of preprocessing, including parsing the strings and scaling the data.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Preprocessing the Data"><div class="sect2" id="idm46324180988976">
<h2>Preprocessing the Data</h2>

<p>Let’s<a data-type="indexterm" data-primary="Data API (TensorFlow)" data-secondary="preprocessing data" id="idm46324180942048"/><a data-type="indexterm" data-primary="data" data-secondary="preprocessing" id="idm46324180941040"/> implement a small function that will perform this preprocessing:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">X_mean</code><code class="p">,</code> <code class="n">X_std</code> <code class="o">=</code> <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># mean and scale of each feature in the training set</code>
<code class="n">n_inputs</code> <code class="o">=</code> <code class="mi">8</code>

<code class="k">def</code> <code class="nf">parse_csv_line</code><code class="p">(</code><code class="n">line</code><code class="p">):</code>
    <code class="n">defs</code> <code class="o">=</code> <code class="p">[</code><code class="mf">0.</code><code class="p">]</code> <code class="o">*</code> <code class="n">n_inputs</code> <code class="o">+</code> <code class="p">[</code><code class="n">tf</code><code class="o">.</code><code class="n">constant</code><code class="p">([],</code> <code class="n">dtype</code><code class="o">=</code><code class="n">tf</code><code class="o">.</code><code class="n">float32</code><code class="p">)]</code>
    <code class="n">fields</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">io</code><code class="o">.</code><code class="n">decode_csv</code><code class="p">(</code><code class="n">line</code><code class="p">,</code> <code class="n">record_defaults</code><code class="o">=</code><code class="n">defs</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">tf</code><code class="o">.</code><code class="n">stack</code><code class="p">(</code><code class="n">fields</code><code class="p">[:</code><code class="o">-</code><code class="mi">1</code><code class="p">]),</code> <code class="n">tf</code><code class="o">.</code><code class="n">stack</code><code class="p">(</code><code class="n">fields</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">:])</code>

<code class="k">def</code> <code class="nf">preprocess</code><code class="p">(</code><code class="n">line</code><code class="p">):</code>
    <code class="n">x</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">parse_csv_line</code><code class="p">(</code><code class="n">line</code><code class="p">)</code>
    <code class="k">return</code> <code class="p">(</code><code class="n">x</code> <code class="o">-</code> <code class="n">X_mean</code><code class="p">)</code> <code class="o">/</code> <code class="n">X_std</code><code class="p">,</code> <code class="n">y</code></pre>

<p>Let’s walk through this code:</p>

<ul>
<li>
<p>First, the code assumes that we have precomputed the mean and standard deviation of each feature in the training set. <code>X_mean</code> and <code>X_std</code> are just 1D tensors (or NumPy arrays) containing eight floats, one per input feature. This can be done using a <code>StandardScaler</code> on a large enough random sample of the dataset. Later in this chapter, we will use a Keras preprocessing layer instead.</p>
</li>
<li>
<p>The <code>parse_csv_line()</code> function takes one CSV line and parses it. For this it uses the <code>tf.io.decode_csv()</code> function, which takes two arguments: the first is the line to parse, and the second is an array containing the default value for each column in the CSV file. This array tells TensorFlow not only the default value for each column, but also the number of columns and their types. In this example, we tell it that all feature columns are floats and that missing values should default to 0, but we provide an empty array of type <code>tf.float32</code> as the default value for the last column (the target): the array tells TensorFlow that this column contains floats, but that there is no default value, so it will raise an exception if it encounters a missing value.</p>
</li>
<li>
<p>The <code>decode_csv()</code> function returns a list of scalar tensors (one per column), but we need to return a 1D tensor array. So we call <code>tf.stack()</code> on all tensors except for the last one (the target): this will stack these tensors into a 1D array. We then do the same for the target value: this makes it a 1D tensor array with a single value, rather than a scalar tensor. The <code>decode_csv()</code> function is done, so it returns the input features and the target.</p>
</li>
<li>
<p>Finally, the <code>preprocess()</code> function just calls the <code>parse_csv_line()</code> function, then it scales the input features by subtracting the feature means and then dividing by the feature standard deviations, and it returns a tuple containing the scaled features and the target.</p>
</li>
</ul>

<p>Let’s test this preprocessing function:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">preprocess</code><code class="p">(</code><code class="sa">b</code><code class="s1">'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782'</code><code class="p">)</code>
<code class="go">(&lt;tf.Tensor: shape=(8,), dtype=float32, numpy=</code>
<code class="go"> array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,</code>
<code class="go">        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)&gt;,</code>
<code class="go"> &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)&gt;)</code></pre>

<p>Looks good! We can now use the dataset’s <code>map()</code> method to apply the <code>preprocess()</code> function to each sample in the dataset.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Putting Everything Together"><div class="sect2" id="idm46324180761472">
<h2>Putting Everything Together</h2>

<p>To<a data-type="indexterm" data-primary="Data API (TensorFlow)" data-secondary="helper function creation" id="idm46324180760000"/><a data-type="indexterm" data-primary="helper functions" id="idm46324180759024"/><a data-type="indexterm" data-primary="data" data-secondary="helper function creation" id="idm46324180758352"/> make the code more reusable, let’s put together everything we have discussed so far into a small helper function: it will create and return a dataset that will efficiently load California housing data from multiple CSV files, preprocess it, shuffle it, and batch it (see <a data-type="xref" href="#input_pipeline_diagram">Figure 13-2</a>):</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">csv_reader_dataset</code><code class="p">(</code><code class="n">filepaths</code><code class="p">,</code> <code class="n">n_readers</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">n_read_threads</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code>
                       <code class="n">n_parse_threads</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">shuffle_buffer_size</code><code class="o">=</code><code class="mi">10</code><code class="n">_000</code><code class="p">,</code> <code class="n">seed</code><code class="o">=</code><code class="mi">42</code><code class="p">,</code>
                       <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">):</code>
    <code class="n">dataset</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">Dataset</code><code class="o">.</code><code class="n">list_files</code><code class="p">(</code><code class="n">filepaths</code><code class="p">,</code> <code class="n">seed</code><code class="o">=</code><code class="n">seed</code><code class="p">)</code>
    <code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">interleave</code><code class="p">(</code>
        <code class="k">lambda</code> <code class="n">filepath</code><code class="p">:</code> <code class="n">tf</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">TextLineDataset</code><code class="p">(</code><code class="n">filepath</code><code class="p">)</code><code class="o">.</code><code class="n">skip</code><code class="p">(</code><code class="mi">1</code><code class="p">),</code>
        <code class="n">cycle_length</code><code class="o">=</code><code class="n">n_readers</code><code class="p">,</code> <code class="n">num_parallel_calls</code><code class="o">=</code><code class="n">n_read_threads</code><code class="p">)</code>
    <code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">preprocess</code><code class="p">,</code> <code class="n">num_parallel_calls</code><code class="o">=</code><code class="n">n_parse_threads</code><code class="p">)</code>
    <code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">shuffle</code><code class="p">(</code><code class="n">shuffle_buffer_size</code><code class="p">,</code> <code class="n">seed</code><code class="o">=</code><code class="n">seed</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">dataset</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="n">batch_size</code><code class="p">)</code><code class="o">.</code><code class="n">prefetch</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code></pre>

<p>Everything should make sense in this code, except the very last line (<code>prefetch(1)</code>), which is important for performance, as we will see now.</p>

<figure><div id="input_pipeline_diagram" class="figure">
<img src="Images/mls3_1302.png" alt="mls3 1302" width="1892" height="1237"/>
<h6><span class="label">Figure 13-2. </span>Loading and preprocessing data from multiple CSV files</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Prefetching"><div class="sect2" id="idm46324180607104">
<h2>Prefetching</h2>

<p>By<a data-type="indexterm" data-primary="Data API (TensorFlow)" data-secondary="prefetching data" id="idm46324180605312"/><a data-type="indexterm" data-primary="data" data-secondary="prefetching" id="idm46324180604304"/> calling <code>prefetch(1)</code> at the end, we are creating a dataset that will do its best to always be one batch ahead.⁠<sup><a data-type="noteref" id="idm46324180602704-marker" href="ch13.xhtml#idm46324180602704">2</a></sup> In other words, while our training algorithm is working on one batch, the dataset will already be working in parallel on getting the next batch ready (e.g., reading the data from disk and preprocessing it). This can improve performance dramatically, as is illustrated in <a data-type="xref" href="#prefetching_diagram">Figure 13-3</a>. If we also ensure that loading and preprocessing are multithreaded (by setting <code>num_parallel_calls</code> when calling <code>interleave()</code> and <code>map()</code>), we can exploit multiple CPU cores and hopefully make preparing one batch of data shorter than running a training step on the GPU: this way the GPU will be almost 100% utilized (except for the data transfer time from the CPU to the GPU⁠<sup><a data-type="noteref" id="idm46324180555728-marker" href="ch13.xhtml#idm46324180555728">3</a></sup>), and training will run much faster.</p>

<figure><div id="prefetching_diagram" class="figure">
<img src="Images/mls3_1303.png" alt="mls3 1303" width="1711" height="1308"/>
<h6><span class="label">Figure 13-3. </span>With prefetching, the CPU and the GPU work in parallel: as the GPU works on one batch, the CPU works on the next</h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>
<p>If<a data-type="indexterm" data-primary="memory bandwidth" id="idm46324180551936"/> you plan to purchase a GPU card, its processing power and its memory size are of course very important (in particular, a large amount of RAM is crucial for large computer vision or natural language processing models). Just as important to get good performance is its <em>memory bandwidth</em>; this is the number of gigabytes of data it can get into or out of its RAM per <span class="keep-together">second</span>.</p>
</div>

<p>If the dataset is small enough to fit in memory, you can significantly speed up training by using the dataset’s <code>cache()</code> method to cache its content to RAM. You should generally do this after loading and preprocessing the data, but before shuffling, repeating, batching, and prefetching. This way, each instance will only be read and preprocessed once (instead of once per epoch), but the data will still be shuffled differently at each epoch, and the next batch will still be prepared in advance.</p>

<p>You now know how to build efficient input pipelines to load and preprocess data from multiple text files. We have discussed the most common dataset methods, but there are a few more you may want to look at, such as <code>concatenate()</code>, <code>zip()</code>, <code>window()</code>, <code>reduce()</code>, <code>shard()</code>, <code>flat_map()</code>, <code>apply()</code>, <code>unbatch()</code>, and <code>padded_batch()</code>. There are also a few more class methods, such as <code>from_generator()</code> and <code>from_tensors()</code>, which create a new dataset from a Python generator or a list of tensors, respectively. Please check the API documentation for more details. Also note that there are experimental features available in <code>tf.data.experimental</code>, many of which will likely make it to the core API in future releases (e.g., check out the <code>CsvDataset</code> class, as well as the <code>make_csv_dataset()</code> method, which takes care of inferring the type of each column).</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Using the Dataset with Keras"><div class="sect2" id="idm46324180548128">
<h2>Using the Dataset with Keras</h2>

<p>Now<a data-type="indexterm" data-primary="Data API (TensorFlow)" data-secondary="using datasets with Keras" id="idm46324180540352"/><a data-type="indexterm" data-primary="data" data-secondary="using datasets with tf.Keras" id="idm46324180539376"/> we can use the <code>csv_reader_dataset()</code> function to create a dataset for the training set, and for the validation set and the test set. The training set will be shuffled at each epoch (note that the validation set and the test set will also be shuffled, even though we don’t really need that).</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">train_set</code> <code class="o">=</code> <code class="n">csv_reader_dataset</code><code class="p">(</code><code class="n">train_filepaths</code><code class="p">)</code>
<code class="n">valid_set</code> <code class="o">=</code> <code class="n">csv_reader_dataset</code><code class="p">(</code><code class="n">valid_filepaths</code><code class="p">)</code>
<code class="n">test_set</code> <code class="o">=</code> <code class="n">csv_reader_dataset</code><code class="p">(</code><code class="n">test_filepaths</code><code class="p">)</code></pre>

<p>And now you can simply build and train a Keras model using these datasets. When you call the <code>fit()</code> method, you can pass <code>train_set</code> instead of <code>X_train, y_train</code>, and pass <code>validation_data=valid_set</code> instead of <code>validation_data=(X_valid, y_valid)</code>. The <code>fit()</code> method will take care of repeating the training dataset once per epoch, using a different random order at each epoch.</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code><code class="o">...</code><code class="p">])</code>
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"mse"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="s2">"sgd"</code><code class="p">)</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">train_set</code><code class="p">,</code> <code class="n">validation_data</code><code class="o">=</code><code class="n">valid_set</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code></pre>

<p>Similarly, you can pass a dataset to the <code>evaluate()</code> and <code>predict()</code> methods:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">test_mse</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">evaluate</code><code class="p">(</code><code class="n">test_set</code><code class="p">)</code>
<code class="n">new_set</code> <code class="o">=</code> <code class="n">test_set</code><code class="o">.</code><code class="n">take</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code>  <code class="c1"># pretend we have 3 new samples</code>
<code class="n">y_pred</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">new_set</code><code class="p">)</code>  <code class="c1"># or you could just pass a NumPy array</code></pre>

<p>Unlike the other sets, the <code>new_set</code> will usually not contain labels. If it does, as is the case here, Keras will ignore them. Note that in all these cases, you can still use NumPy arrays instead of datasets if you prefer (but of course they need to have been loaded and preprocessed first).</p>

<p>If you want to build your own custom training loop (as in <a data-type="xref" href="ch12.xhtml#tensorflow_chapter">Chapter 12</a>), you can just iterate over the training set, very naturally:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">n_epochs</code> <code class="o">=</code> <code class="mi">5</code>
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_epochs</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">X_batch</code><code class="p">,</code> <code class="n">y_batch</code> <code class="ow">in</code> <code class="n">train_set</code><code class="p">:</code>
        <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># perform one Gradient Descent step</code></pre>

<p>In fact, it is even possible to create a TF Function (see <a data-type="xref" href="ch12.xhtml#tensorflow_chapter">Chapter 12</a>) that trains the model for a whole epoch. This can really speed up training:</p>

<pre data-type="programlisting" data-code-language="python"><code class="nd">@tf.function</code>
<code class="k">def</code> <code class="nf">train_one_epoch</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">,</code> <code class="n">loss_fn</code><code class="p">,</code> <code class="n">train_set</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">X_batch</code><code class="p">,</code> <code class="n">y_batch</code> <code class="ow">in</code> <code class="n">train_set</code><code class="p">:</code>
        <code class="k">with</code> <code class="n">tf</code><code class="o">.</code><code class="n">GradientTape</code><code class="p">()</code> <code class="k">as</code> <code class="n">tape</code><code class="p">:</code>
            <code class="n">y_pred</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">X_batch</code><code class="p">)</code>
            <code class="n">main_loss</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">reduce_mean</code><code class="p">(</code><code class="n">loss_fn</code><code class="p">(</code><code class="n">y_batch</code><code class="p">,</code> <code class="n">y_pred</code><code class="p">))</code>
            <code class="n">loss</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">add_n</code><code class="p">([</code><code class="n">main_loss</code><code class="p">]</code> <code class="o">+</code> <code class="n">model</code><code class="o">.</code><code class="n">losses</code><code class="p">)</code>
        <code class="n">gradients</code> <code class="o">=</code> <code class="n">tape</code><code class="o">.</code><code class="n">gradient</code><code class="p">(</code><code class="n">loss</code><code class="p">,</code> <code class="n">model</code><code class="o">.</code><code class="n">trainable_variables</code><code class="p">)</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">apply_gradients</code><code class="p">(</code><code class="nb">zip</code><code class="p">(</code><code class="n">gradients</code><code class="p">,</code> <code class="n">model</code><code class="o">.</code><code class="n">trainable_variables</code><code class="p">))</code>

<code class="n">optimizer</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">optimizers</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">learning_rate</code><code class="o">=</code><code class="mf">0.01</code><code class="p">)</code>
<code class="n">loss_fn</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">losses</code><code class="o">.</code><code class="n">mean_squared_error</code>
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_epochs</code><code class="p">):</code>
    <code class="k">print</code><code class="p">(</code><code class="s2">"</code><code class="se">\r</code><code class="s2">Epoch {}/{}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">epoch</code> <code class="o">+</code> <code class="mi">1</code><code class="p">,</code> <code class="n">n_epochs</code><code class="p">),</code> <code class="n">end</code><code class="o">=</code><code class="s2">""</code><code class="p">)</code>
    <code class="n">train_one_epoch</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">,</code> <code class="n">loss_fn</code><code class="p">,</code> <code class="n">train_set</code><code class="p">)</code></pre>

<p>Congratulations, you now know how to build powerful<a data-type="indexterm" data-primary="pipelines" id="idm46324180178128"/> input pipelines using the tf.data API! However, so far we have used CSV files, which are common, simple, and convenient but not really efficient, and do not support large or complex data structures (such as images or audio) very well. So let’s see how to use TFRecords instead.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If you are happy with CSV files (or whatever other format you are using), you do not <em>have</em> to use TFRecords. As the saying goes, if it ain’t broke, don’t fix it! TFRecords are useful when the bottleneck during training is loading and parsing the data.<a data-type="indexterm" data-primary="" data-startref="TFload13" id="idm46324180163072"/></p>
</div>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="The TFRecord Format"><div class="sect1" id="idm46324180541488">
<h1>The TFRecord Format</h1>

<p>The<a data-type="indexterm" data-primary="TFRecord format" data-secondary="overview of" id="idm46324180122640"/><a data-type="indexterm" data-primary="TensorFlow, data loading and preprocessing" data-secondary="TFRecord format" id="TFloadrecord13"/> TFRecord format is TensorFlow’s preferred format for storing large amounts of data and reading it efficiently. It is a very simple binary format that just contains a sequence of binary records of varying sizes (each record is comprised of a length, a CRC checksum to check that the length was not corrupted, then the actual data, and finally a CRC checksum for the data). You can easily create a TFRecord file using the <code>tf.io.TFRecordWriter</code> class:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">with</code> <code class="n">tf</code><code class="o">.</code><code class="n">io</code><code class="o">.</code><code class="n">TFRecordWriter</code><code class="p">(</code><code class="s2">"my_data.tfrecord"</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
    <code class="n">f</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="sa">b</code><code class="s2">"This is the first record"</code><code class="p">)</code>
    <code class="n">f</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="sa">b</code><code class="s2">"And this is the second record"</code><code class="p">)</code></pre>

<p>And you can then use a <code>tf.data.TFRecordDataset</code> to read one or more TFRecord files:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">filepaths</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"my_data.tfrecord"</code><code class="p">]</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">TFRecordDataset</code><code class="p">(</code><code class="n">filepaths</code><code class="p">)</code>
<code class="k">for</code> <code class="n">item</code> <code class="ow">in</code> <code class="n">dataset</code><code class="p">:</code>
    <code class="k">print</code><code class="p">(</code><code class="n">item</code><code class="p">)</code></pre>

<p>This will output:</p>

<pre data-type="programlisting">tf.Tensor(b'This is the first record', shape=(), dtype=string)
tf.Tensor(b'And this is the second record', shape=(), dtype=string)</pre>
<div data-type="tip"><h6>Tip</h6>
<p>By default, a <code>TFRecordDataset</code> will read files one by one, but you can make it read multiple files in parallel and interleave their records by passing the constructor a list of file paths and setting <code>num_parallel_reads</code> to a number greater than one. Alternatively, you could obtain the same result by using <code>list_files()</code> and <code>interleave()</code> as we did earlier to read multiple CSV files.</p>
</div>








<section data-type="sect2" data-pdf-bookmark="Compressed TFRecord Files"><div class="sect2" id="idm46324180021072">
<h2>Compressed TFRecord Files</h2>

<p>It<a data-type="indexterm" data-primary="TFRecord format" data-secondary="compressed TFRecord files" id="idm46324180019536"/> can sometimes be useful to compress your TFRecord files, especially if they need to be loaded via a network connection. You can create a compressed TFRecord file by setting the <code>options</code> argument:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">options</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">io</code><code class="o">.</code><code class="n">TFRecordOptions</code><code class="p">(</code><code class="n">compression_type</code><code class="o">=</code><code class="s2">"GZIP"</code><code class="p">)</code>
<code class="k">with</code> <code class="n">tf</code><code class="o">.</code><code class="n">io</code><code class="o">.</code><code class="n">TFRecordWriter</code><code class="p">(</code><code class="s2">"my_compressed.tfrecord"</code><code class="p">,</code> <code class="n">options</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
    <code class="n">f</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="sa">b</code><code class="s2">"Compress, compress, compress!"</code><code class="p">)</code></pre>

<p>When reading a compressed TFRecord file, you need to specify the compression type:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">dataset</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">TFRecordDataset</code><code class="p">([</code><code class="s2">"my_compressed.tfrecord"</code><code class="p">],</code>
                                  <code class="n">compression_type</code><code class="o">=</code><code class="s2">"GZIP"</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="A Brief Introduction to Protocol Buffers"><div class="sect2" id="idm46324180020736">
<h2>A Brief Introduction to Protocol Buffers</h2>

<p>Even<a data-type="indexterm" data-primary="TFRecord format" data-secondary="protocol buffers (protobufs)" id="idm46324179947088"/><a data-type="indexterm" data-primary="protocol buffers (protobufs)" id="idm46324179946144"/> though each record can use any binary format you want, TFRecord files usually contain serialized protocol buffers (also called <em>protobufs</em>). This is a portable, extensible, and efficient binary format developed at Google back in 2001 and made open source in 2008; protobufs are now widely used, in particular in <a href="https://grpc.io">gRPC</a>, Google’s remote procedure call system. They are defined using a simple language that looks like this:</p>

<pre data-type="programlisting" data-code-language="java"><code class="n">syntax</code> <code class="o">=</code> <code class="s">"proto3"</code><code class="o">;</code>
<code class="n">message</code> <code class="n">Person</code> <code class="o">{</code>
    <code class="n">string</code> <code class="n">name</code> <code class="o">=</code> <code class="mi">1</code><code class="o">;</code>
    <code class="n">int32</code> <code class="n">id</code> <code class="o">=</code> <code class="mi">2</code><code class="o">;</code>
    <code class="n">repeated</code> <code class="n">string</code> <code class="n">email</code> <code class="o">=</code> <code class="mi">3</code><code class="o">;</code>
<code class="o">}</code></pre>

<p>This protobuf definition says we are using version 3 of the protobuf format, and it specifies that each <code>Person</code> object⁠<sup><a data-type="noteref" id="idm46324179876704-marker" href="ch13.xhtml#idm46324179876704">4</a></sup> may (optionally) have a <code>name</code> of type <code>string</code>, an <code>id</code> of type <code>int32</code>, and zero or more <code>email</code> fields, each of type <code>string</code>. The numbers <code>1</code>, <code>2</code>, and <code>3</code> are the field identifiers: they will be used in each record’s binary representation. Once you have a definition in a <em>.proto</em> file, you can compile it. This requires <code>protoc</code>, the protobuf compiler, to generate access classes in Python (or some other language). Note that the protobuf definitions you will generally use in TensorFlow have already been compiled for you, and their Python classes are part of the TensorFlow library, so you will not need to use <code>protoc</code>. All you need to know is how to <em>use</em> protobuf access classes in Python. To illustrate the basics, let’s look at a simple example that uses the access classes generated for the <span class="keep-together"><code>Person</code></span> protobuf (the code is explained in the comments):</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">person_pb2</code> <code class="kn">import</code> <code class="n">Person</code>  <code class="c1"># import the generated access class</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">person</code> <code class="o">=</code> <code class="n">Person</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s2">"Al"</code><code class="p">,</code> <code class="nb">id</code><code class="o">=</code><code class="mi">123</code><code class="p">,</code> <code class="n">email</code><code class="o">=</code><code class="p">[</code><code class="s2">"a@b.com"</code><code class="p">])</code>  <code class="c1"># create a Person</code>
<code class="gp">&gt;&gt;&gt; </code><code class="nb">print</code><code class="p">(</code><code class="n">person</code><code class="p">)</code>  <code class="c1"># display the Person</code>
<code class="go">name: "Al"</code>
<code class="go">id: 123</code>
<code class="go">email: "a@b.com"</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">person</code><code class="o">.</code><code class="n">name</code>  <code class="c1"># read a field</code>
<code class="go">'Al'</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">person</code><code class="o">.</code><code class="n">name</code> <code class="o">=</code> <code class="s2">"Alice"</code>  <code class="c1"># modify a field</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">person</code><code class="o">.</code><code class="n">email</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>  <code class="c1"># repeated fields can be accessed like arrays</code>
<code class="go">'a@b.com'</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">person</code><code class="o">.</code><code class="n">email</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="s2">"c@d.com"</code><code class="p">)</code>  <code class="c1"># add an email address</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">serialized</code> <code class="o">=</code> <code class="n">person</code><code class="o">.</code><code class="n">SerializeToString</code><code class="p">()</code>  <code class="c1"># serialize person to a byte string</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">serialized</code>
<code class="go">b'\n\x05Alice\x10{\x1a\x07a@b.com\x1a\x07c@d.com'</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">person2</code> <code class="o">=</code> <code class="n">Person</code><code class="p">()</code>  <code class="c1"># create a new Person</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">person2</code><code class="o">.</code><code class="n">ParseFromString</code><code class="p">(</code><code class="n">serialized</code><code class="p">)</code>  <code class="c1"># parse the byte string (27 bytes long)</code>
<code class="go">27</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">person</code> <code class="o">==</code> <code class="n">person2</code>  <code class="c1"># now they are equal</code>
<code class="go">True</code></pre>

<p>In short, we import the <code>Person</code> class generated by <code>protoc</code>, we create an instance and play with it, visualizing it and reading and writing some fields, then we serialize it using the <code>SerializeToString()</code> method. This is the binary data that is ready to be saved or transmitted over the network. When reading or receiving this binary data, we can parse it using the <code>ParseFromString()</code> method, and we get a copy of the object that was serialized.⁠<sup><a data-type="noteref" id="idm46324179784528-marker" href="ch13.xhtml#idm46324179784528">5</a></sup></p>

<p>We could save the serialized <code>Person</code> object to a TFRecord file, then we could load and parse it: everything would work fine. However, <code>ParseFromString()</code> is not a TensorFlow operations, so you couldn’t use it in a preprocessing function in a tf.data pipeline (except by wrapping it in a <code>tf.py_function()</code> operation, which would make the code slower and less portable, as we saw in <a data-type="xref" href="ch12.xhtml#tensorflow_chapter">Chapter 12</a>). However, you could use the <code>tf.io.decode_proto()</code> function, which can parse any protobuf you want, provided you give it the protobuf definition (see the notebook for an example). That said, in practice you will generally want to use instead the predefined protobufs for which TensorFlow provides dedicated parsing operations. Let’s look at these predefined protobufs now.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="TensorFlow Protobufs"><div class="sect2" id="idm46324179779552">
<h2>TensorFlow Protobufs</h2>

<p>The<a data-type="indexterm" data-primary="TFRecord format" data-secondary="TensorFlow protobufs" id="idm46324179777984"/> main protobuf typically used in a TFRecord file is the <code>Example</code> protobuf, which represents one instance in a dataset. It contains a list of named features, where each feature can either be a list of byte strings, a list of floats, or a list of integers. Here is the protobuf definition:</p>

<pre data-type="programlisting" data-code-language="java"><code class="n">syntax</code> <code class="o">=</code> <code class="s">"proto3"</code><code class="o">;</code>
<code class="n">message</code> <code class="n">BytesList</code> <code class="o">{</code> <code class="n">repeated</code> <code class="n">bytes</code> <code class="n">value</code> <code class="o">=</code> <code class="mi">1</code><code class="o">;</code> <code class="o">}</code>
<code class="n">message</code> <code class="n">FloatList</code> <code class="o">{</code> <code class="n">repeated</code> <code class="kt">float</code> <code class="n">value</code> <code class="o">=</code> <code class="mi">1</code> <code class="o">[</code><code class="n">packed</code> <code class="o">=</code> <code class="kc">true</code><code class="o">];</code> <code class="o">}</code>
<code class="n">message</code> <code class="n">Int64List</code> <code class="o">{</code> <code class="n">repeated</code> <code class="n">int64</code> <code class="n">value</code> <code class="o">=</code> <code class="mi">1</code> <code class="o">[</code><code class="n">packed</code> <code class="o">=</code> <code class="kc">true</code><code class="o">];</code> <code class="o">}</code>
<code class="n">message</code> <code class="n">Feature</code> <code class="o">{</code>
    <code class="n">oneof</code> <code class="n">kind</code> <code class="o">{</code>
        <code class="n">BytesList</code> <code class="n">bytes_list</code> <code class="o">=</code> <code class="mi">1</code><code class="o">;</code>
        <code class="n">FloatList</code> <code class="n">float_list</code> <code class="o">=</code> <code class="mi">2</code><code class="o">;</code>
        <code class="n">Int64List</code> <code class="n">int64_list</code> <code class="o">=</code> <code class="mi">3</code><code class="o">;</code>
    <code class="o">}</code>
<code class="o">};</code>
<code class="n">message</code> <code class="n">Features</code> <code class="o">{</code> <code class="n">map</code><code class="o">&lt;</code><code class="n">string</code><code class="o">,</code> <code class="n">Feature</code><code class="o">&gt;</code> <code class="n">feature</code> <code class="o">=</code> <code class="mi">1</code><code class="o">;</code> <code class="o">};</code>
<code class="n">message</code> <code class="n">Example</code> <code class="o">{</code> <code class="n">Features</code> <code class="n">features</code> <code class="o">=</code> <code class="mi">1</code><code class="o">;</code> <code class="o">};</code></pre>

<p>The definitions of <code>BytesList</code>, <code>FloatList</code>, and <code>Int64List</code> are straightforward enough. Note that <code>[packed = true]</code> is used for repeated numerical fields, for a more efficient <span class="keep-together">encoding. A</span> <code>Feature</code> contains either a <code>BytesList</code>, a <code>FloatList</code>, or an <code>Int64List</code>. A <code>Features</code> (with an <code>s</code>) contains a dictionary that maps a feature name to the corresponding feature value. And finally, an <code>Example</code> contains only a <code>Features</code> object.⁠<sup><a data-type="noteref" id="idm46324179538752-marker" href="ch13.xhtml#idm46324179538752">6</a></sup> Here is how you could create a <code>tf.train.Example</code> representing the same person as earlier:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">tensorflow.train</code> <code class="kn">import</code> <code class="n">BytesList</code><code class="p">,</code> <code class="n">FloatList</code><code class="p">,</code> <code class="n">Int64List</code>
<code class="kn">from</code> <code class="nn">tensorflow.train</code> <code class="kn">import</code> <code class="n">Feature</code><code class="p">,</code> <code class="n">Features</code><code class="p">,</code> <code class="n">Example</code>

<code class="n">person_example</code> <code class="o">=</code> <code class="n">Example</code><code class="p">(</code>
    <code class="n">features</code><code class="o">=</code><code class="n">Features</code><code class="p">(</code>
        <code class="n">feature</code><code class="o">=</code><code class="p">{</code>
            <code class="s2">"name"</code><code class="p">:</code> <code class="n">Feature</code><code class="p">(</code><code class="n">bytes_list</code><code class="o">=</code><code class="n">BytesList</code><code class="p">(</code><code class="n">value</code><code class="o">=</code><code class="p">[</code><code class="sa">b</code><code class="s2">"Alice"</code><code class="p">])),</code>
            <code class="s2">"id"</code><code class="p">:</code> <code class="n">Feature</code><code class="p">(</code><code class="n">int64_list</code><code class="o">=</code><code class="n">Int64List</code><code class="p">(</code><code class="n">value</code><code class="o">=</code><code class="p">[</code><code class="mi">123</code><code class="p">])),</code>
            <code class="s2">"emails"</code><code class="p">:</code> <code class="n">Feature</code><code class="p">(</code><code class="n">bytes_list</code><code class="o">=</code><code class="n">BytesList</code><code class="p">(</code><code class="n">value</code><code class="o">=</code><code class="p">[</code><code class="sa">b</code><code class="s2">"a@b.com"</code><code class="p">,</code>
                                                          <code class="sa">b</code><code class="s2">"c@d.com"</code><code class="p">]))</code>
        <code class="p">}))</code></pre>

<p>The code is a bit verbose and repetitive, but it’s rather straightforward (and you could easily wrap it inside a small helper function). Now that we have an <code>Example</code> protobuf, we can serialize it by calling its <code>SerializeToString()</code> method, then write the resulting data to a TFRecord file. Let’s write it five times to pretend we have several contacts:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">with</code> <code class="n">tf</code><code class="o">.</code><code class="n">io</code><code class="o">.</code><code class="n">TFRecordWriter</code><code class="p">(</code><code class="s2">"my_contacts.tfrecord"</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
    <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">5</code><code class="p">):</code>
        <code class="n">f</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="n">person_example</code><code class="o">.</code><code class="n">SerializeToString</code><code class="p">())</code></pre>

<p>Normally you would write much more than five <code>Example</code>s! Typically, you would create a conversion script that reads from your current format (say, CSV files), creates an <code>Example</code> protobuf for each instance, serializes them, and saves them to several TFRecord files, ideally shuffling them in the process. This requires a bit of work, so once again make sure it is really necessary (perhaps your pipeline works fine with CSV files).</p>

<p>Now that we have a nice TFRecord file containing several serialized <code>Example</code>s, let’s try to load it.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Loading and Parsing Examples"><div class="sect2" id="idm46324179370720">
<h2>Loading and Parsing Examples</h2>

<p>To<a data-type="indexterm" data-primary="TFRecord format" data-secondary="loading and parsing examples" id="idm46324179397888"/> load the serialized <code>Example</code> protobufs, we will use a <code>tf.data.TFRecordDataset</code> once again, and we will parse each <code>Example</code> using <code>tf.io.parse_single_example()</code>. It requires at least two arguments: a string scalar tensor containing the serialized data, and a description of each feature. The description is a dictionary that maps each feature name to either a <code>tf.io.FixedLenFeature</code> descriptor indicating the feature’s shape, type, and default value, or a <code>tf.io.VarLenFeature</code> descriptor indicating only the type if the length of the feature’s list may vary (such as for the <code>"emails"</code> feature).</p>

<p>The following code defines a description dictionary, then it creates a <code>TFRecordDataset</code> and applies a small preprocessing function to it to parse each serialized <code>Example</code> protobuf this dataset contains:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">feature_description</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"name"</code><code class="p">:</code> <code class="n">tf</code><code class="o">.</code><code class="n">io</code><code class="o">.</code><code class="n">FixedLenFeature</code><code class="p">([],</code> <code class="n">tf</code><code class="o">.</code><code class="n">string</code><code class="p">,</code> <code class="n">default_value</code><code class="o">=</code><code class="s2">""</code><code class="p">),</code>
    <code class="s2">"id"</code><code class="p">:</code> <code class="n">tf</code><code class="o">.</code><code class="n">io</code><code class="o">.</code><code class="n">FixedLenFeature</code><code class="p">([],</code> <code class="n">tf</code><code class="o">.</code><code class="n">int64</code><code class="p">,</code> <code class="n">default_value</code><code class="o">=</code><code class="mi">0</code><code class="p">),</code>
    <code class="s2">"emails"</code><code class="p">:</code> <code class="n">tf</code><code class="o">.</code><code class="n">io</code><code class="o">.</code><code class="n">VarLenFeature</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">string</code><code class="p">),</code>
<code class="p">}</code>

<code class="k">def</code> <code class="nf">parse</code><code class="p">(</code><code class="n">serialized_example</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">tf</code><code class="o">.</code><code class="n">io</code><code class="o">.</code><code class="n">parse_single_example</code><code class="p">(</code><code class="n">serialized_example</code><code class="p">,</code> <code class="n">feature_description</code><code class="p">)</code>

<code class="n">dataset</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">TFRecordDataset</code><code class="p">([</code><code class="s2">"my_contacts.tfrecord"</code><code class="p">])</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">parse</code><code class="p">)</code>
<code class="k">for</code> <code class="n">parsed_example</code> <code class="ow">in</code> <code class="n">dataset</code><code class="p">:</code>
    <code class="k">print</code><code class="p">(</code><code class="n">parsed_example</code><code class="p">)</code></pre>

<p>The fixed-length features are parsed as regular tensors, but the variable-length features are parsed as sparse tensors. You can convert a sparse tensor to a dense tensor using <code>tf.sparse.to_dense()</code>, but in this case it is simpler to just access its values:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">tf</code><code class="o">.</code><code class="n">sparse</code><code class="o">.</code><code class="n">to_dense</code><code class="p">(</code><code class="n">parsed_example</code><code class="p">[</code><code class="s2">"emails"</code><code class="p">],</code> <code class="n">default_value</code><code class="o">=</code><code class="sa">b</code><code class="s2">""</code><code class="p">)</code>
<code class="go">&lt;tf.Tensor: [...] dtype=string, numpy=array([b'a@b.com', b'c@d.com'], [...])&gt;</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">parsed_example</code><code class="p">[</code><code class="s2">"emails"</code><code class="p">]</code><code class="o">.</code><code class="n">values</code>
<code class="go">&lt;tf.Tensor: [...] dtype=string, numpy=array([b'a@b.com', b'c@d.com'], [...])&gt;</code></pre>

<p>Instead of parsing examples one by one using <code>tf.io.parse_single_example()</code>, you may want to parse them batch by batch using <code>tf.io.parse_example()</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">parse</code><code class="p">(</code><code class="n">serialized_examples</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">tf</code><code class="o">.</code><code class="n">io</code><code class="o">.</code><code class="n">parse_example</code><code class="p">(</code><code class="n">serialized_examples</code><code class="p">,</code> <code class="n">feature_description</code><code class="p">)</code>

<code class="n">dataset</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">TFRecordDataset</code><code class="p">([</code><code class="s2">"my_contacts.tfrecord"</code><code class="p">])</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">parse</code><code class="p">)</code>
<code class="k">for</code> <code class="n">parsed_examples</code> <code class="ow">in</code> <code class="n">dataset</code><code class="p">:</code>
    <code class="k">print</code><code class="p">(</code><code class="n">parsed_examples</code><code class="p">)</code>  <code class="c1"># two examples at a time</code></pre>

<p>Lastly, a <code>BytesList</code> can contain any binary data you want, including any serialized object. For example, you can use <code>tf.io.encode_jpeg()</code> to encode an image using the JPEG format and put this binary data in a <code>BytesList</code>. Later, when your code reads the TFRecord, it will start by parsing the <code>Example</code>, then it will need to call <code>tf.io.decode_jpeg()</code> to parse the data and get the original image (or you can use <code>tf.io.decode_image()</code>, which can decode any BMP, GIF, JPEG, or PNG image). You can also store any tensor you want in a <code>BytesList</code> by serializing the tensor using <code>tf.io.serialize_tensor()</code> then putting the resulting byte string in a <code>BytesList</code> feature. Later, when you parse the TFRecord, you can parse this data using <code>tf.io.parse_tensor()</code>. See the notebook for examples of storing images and tensors in a TFRecord file.</p>

<p>As you can see, the <code>Example</code> protobuf will probably be sufficient for most use cases. However, it may be a bit cumbersome to use when you are dealing with lists of lists. For example, suppose you want to classify text documents. Each document may be represented as a list of sentences, where each sentence is represented as a list of words. And perhaps each document also has a list of comments, where each comment is represented as a list of words. There may be some contextual data too, such as the document’s author, title, and publication date. TensorFlow’s <code>SequenceExample</code> protobuf is designed for such use cases.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Handling Lists of Lists Using the SequenceExample Protobuf"><div class="sect2" id="idm46324179370128">
<h2>Handling Lists of Lists Using the <code>SequenceExample</code> Protobuf</h2>

<p>Here<a data-type="indexterm" data-primary="SequenceExample protobuf (TensorFlow)" id="idm46324179092592"/><a data-type="indexterm" data-primary="TFRecord format" data-secondary="lists of lists using SequenceExample Protobuf" id="idm46324179091984"/><a data-type="indexterm" data-primary="lists of lists, using SequenceExample Protobuf" id="idm46324179091136"/> is the definition of the <code>SequenceExample</code> protobuf:</p>

<pre data-type="programlisting" data-code-language="java"><code class="n">message</code> <code class="n">FeatureList</code> <code class="o">{</code> <code class="n">repeated</code> <code class="n">Feature</code> <code class="n">feature</code> <code class="o">=</code> <code class="mi">1</code><code class="o">;</code> <code class="o">};</code>
<code class="n">message</code> <code class="n">FeatureLists</code> <code class="o">{</code> <code class="n">map</code><code class="o">&lt;</code><code class="n">string</code><code class="o">,</code> <code class="n">FeatureList</code><code class="o">&gt;</code> <code class="n">feature_list</code> <code class="o">=</code> <code class="mi">1</code><code class="o">;</code> <code class="o">};</code>
<code class="n">message</code> <code class="n">SequenceExample</code> <code class="o">{</code>
    <code class="n">Features</code> <code class="n">context</code> <code class="o">=</code> <code class="mi">1</code><code class="o">;</code>
    <code class="n">FeatureLists</code> <code class="n">feature_lists</code> <code class="o">=</code> <code class="mi">2</code><code class="o">;</code>
<code class="o">};</code></pre>

<p>A <code>SequenceExample</code> contains a <code>Features</code> object for the contextual data and a <code>FeatureLists</code> object that contains one or more named <code>FeatureList</code> objects (e.g., a <code>FeatureList</code> named <code>"content"</code> and another named <code>"comments"</code>). Each <code>FeatureList</code> contains a list of <code>Feature</code> objects, each of which may be a list of byte strings, a list of 64-bit integers, or a list of floats (in this example, each <code>Feature</code> would represent a sentence or a comment, perhaps in the form of a list of word identifiers). Building a <code>SequenceExample</code>, serializing it, and parsing it is similar to building, serializing, and parsing an <code>Example</code>, but you must use <code>tf.io.parse_single_sequence_example()</code> to parse a single <code>SequenceExample</code> or <code>tf.io.parse_sequence_example()</code> to parse a batch. Both functions return a tuple containing the context features (as a dictionary) and the feature lists (also as a dictionary). If the feature lists contain sequences of varying sizes (as in the preceding example), you may want to convert them to ragged tensors, using <code>tf.RaggedTensor.from_sparse()</code> (see the notebook for the full code):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">parsed_context</code><code class="p">,</code> <code class="n">parsed_feature_lists</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">io</code><code class="o">.</code><code class="n">parse_single_sequence_example</code><code class="p">(</code>
    <code class="n">serialized_sequence_example</code><code class="p">,</code> <code class="n">context_feature_descriptions</code><code class="p">,</code>
    <code class="n">sequence_feature_descriptions</code><code class="p">)</code>
<code class="n">parsed_content</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">RaggedTensor</code><code class="o">.</code><code class="n">from_sparse</code><code class="p">(</code><code class="n">parsed_feature_lists</code><code class="p">[</code><code class="s2">"content"</code><code class="p">])</code></pre>

<p>Now that you know how to efficiently store, load, parse, and preprocess the data using the tf.data API, TFRecords, and protobufs, it’s time to turn our attention to the Keras preprocessing layers.<a data-type="indexterm" data-primary="" data-startref="TFloadrecord13" id="idm46324179002816"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Keras Preprocessing Layers"><div class="sect1" id="idm46324180123616">
<h1>Keras Preprocessing Layers</h1>

<p>Preparing<a data-type="indexterm" data-primary="data" data-secondary="preprocessing" id="Dpre13"/><a data-type="indexterm" data-primary="preprocessing" id="preproc13"/><a data-type="indexterm" data-primary="TensorFlow, data loading and preprocessing" data-secondary="preprocessing input features" id="TFloadinput13"/> your data for a neural network typically requires normalizing the numerical features, encoding the categorical features and text, cropping and resizing images, and more. This preprocessing can be done ahead of time when preparing your training data files, using any tools you like, such as NumPy, Pandas, or Scikit-Learn. Alternatively, you can preprocess your data on the fly while loading it with tf.data, by applying a preprocessing function to every element of a dataset using that dataset’s <code>map()</code> method, as we did earlier in this chapter. In both these cases, you will need to apply the exact same preprocessing steps in production, to ensure your production model receives preprocessed inputs similar to the ones it was trained on. One last approach is to include preprocessing layers directly inside your model so it can preprocess all the input data on the fly, then use the same preprocessing layers in production. The rest of this chapter will look at this last approach.</p>

<p>Keras offers many preprocessing layers that you can include in your models: they can preprocess numerical features, categorical features, images, and text. We’ll look at numerical and categorical features in this chapter, as well as basic text preprocessing, and we will cover image preprocessing in &lt;cnn_chapter&gt;&gt;, and more advanced text preprocessing in Chapter 16.</p>








<section data-type="sect2" data-pdf-bookmark="The Normalization Layer"><div class="sect2" id="idm46324178947696">
<h2>The <code>Normalization</code> Layer</h2>

<p>As we saw in <a data-type="xref" href="ch10.xhtml#ann_chapter">Chapter 10</a>, Keras provides a <code>Normalization</code> layer that you can use to standardize the input features. You can either specify the mean and variance of each feature when creating the layer, or—more simply—you can pass the training set to the layer’s <code>adapt()</code> method before fitting the model, so the layer can measure the feature means and variances on its own before training:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">norm_layer</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Normalization</code><code class="p">()</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">norm_layer</code><code class="p">,</code>
    <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
<code class="p">])</code>
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"mse"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">optimizers</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">learning_rate</code><code class="o">=</code><code class="mf">2e-3</code><code class="p">))</code>
<code class="n">norm_layer</code><code class="o">.</code><code class="n">adapt</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>  <code class="c1"># computes the mean and variance of every feature</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">validation_data</code><code class="o">=</code><code class="p">(</code><code class="n">X_valid</code><code class="p">,</code> <code class="n">y_valid</code><code class="p">),</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>The data sample passed to the <code>adapt()</code> method must be large enough to be representative of your dataset, but it does not have to be the full training set: for the <code>Normalization</code> layer, a few hundred instances randomly sampled from the training set will generally be sufficient to get a good estimate of the feature means and variances.</p>
</div>

<p>Since we included the <code>Normalization</code> layer inside the model, we can now safely deploy this model to production without having to worry about normalization again: the model will just handle it (see <a data-type="xref" href="#preprocessing_in_model_diagram">Figure 13-4</a>). Fantastic! This approach completely eliminates the risk of <em>preprocessing mismatch</em>: this happens when people try to maintain different preprocessing code for training and for production, but they update one and forget to update the other. The production model ends up receiving data preprocessed in a way it doesn’t expect. If they’re lucky, they get a clear bug. If not, the model’s accuracy just silently degrades.</p>

<figure><div id="preprocessing_in_model_diagram" class="figure">
<img src="Images/mls3_1304.png" alt="mls3 1304" width="1465" height="525"/>
<h6><span class="label">Figure 13-4. </span>Including preprocessing layers inside a model</h6>
</div></figure>

<p>Including the preprocessing layer directly in the model is nice and straightforward, but it will slow down training (only very slightly in the case of the <code>Normalization</code> layer): indeed, since preprocessing is performed on the fly during training, it happens once per epoch. We can do better: let’s normalize the whole training set just once before training. For this, let’s use the <code>Normalization</code> layer in a standalone fashion (much like a <code>StandardScaler</code>):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">norm_layer</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Normalization</code><code class="p">()</code>
<code class="n">norm_layer</code><code class="o">.</code><code class="n">adapt</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>
<code class="n">X_train_scaled</code> <code class="o">=</code> <code class="n">norm_layer</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>
<code class="n">X_valid_scaled</code> <code class="o">=</code> <code class="n">norm_layer</code><code class="p">(</code><code class="n">X_valid</code><code class="p">)</code></pre>

<p>Now we can train a model on the scaled data, but this time without any <code>Normalization</code> layer inside the model:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code><code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">)])</code>
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"mse"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">optimizers</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">learning_rate</code><code class="o">=</code><code class="mf">2e-3</code><code class="p">))</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_scaled</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code>
          <code class="n">validation_data</code><code class="o">=</code><code class="p">(</code><code class="n">X_valid_scaled</code><code class="p">,</code> <code class="n">y_valid</code><code class="p">))</code></pre>

<p>Good! This should speed up training a bit. But now the model won’t preprocess its inputs when we deploy it to production. To fix this, we just need to create a new model that wraps both the adapted <code>Normalization</code> layer and the model we just trained. We can then deploy this final model to production, and it will take care of both preprocessing its inputs and making predictions (see <a data-type="xref" href="#optimized_preprocessing_in_model_diagram">Figure 13-5</a>):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">final_model</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code><code class="n">norm_layer</code><code class="p">,</code> <code class="n">model</code><code class="p">])</code>
<code class="n">X_new</code> <code class="o">=</code> <code class="n">X_test</code><code class="p">[:</code><code class="mi">3</code><code class="p">]</code>  <code class="c1"># pretend we have a few new instances (unscaled)</code>
<code class="n">y_pred</code> <code class="o">=</code> <code class="n">final_model</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code>  <code class="c1"># preprocesses the data and makes predictions</code></pre>

<figure><div id="optimized_preprocessing_in_model_diagram" class="figure">
<img src="Images/mls3_1305.png" alt="mls3 1305" width="2015" height="507"/>
<h6><span class="label">Figure 13-5. </span>Preprocessing the data just once before training using preprocessing layers, then deploying these layers inside the final model</h6>
</div></figure>

<p>Now we have the best of both worlds: training is fast because we only preprocess the data once before training begins, and the final model can preprocess its inputs on the fly without any risk of preprocessing mismatch.</p>

<p>Moreover, the Keras preprocessing layers play nicely with the tf.data API. For example, it’s possible to pass a <code>tf.data.Dataset</code> to the <code>adapt()</code> method. It’s also possible to apply a Keras preprocessing layer to a <code>tf.data.Dataset</code> using the dataset’s <code>map()</code> method. For example, here’s how you could apply an adapted <code>Normalization</code> layer to the input features of each batch in a dataset:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="k">lambda</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">:</code> <code class="p">(</code><code class="n">norm_layer</code><code class="p">(</code><code class="n">X</code><code class="p">),</code> <code class="n">y</code><code class="p">))</code></pre>

<p>Lastly, if you ever need more features than the Keras preprocessing layers provide, you can always write your own Keras layer, just like we discussed in <a data-type="xref" href="ch12.xhtml#tensorflow_chapter">Chapter 12</a>. For example, if the <code>Normalization</code> layer didn’t exist, you could get a similar result using the following custom layer:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">numpy</code> <code class="kn">as</code> <code class="nn">np</code>

<code class="k">class</code> <code class="nc">MyNormalization</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Layer</code><code class="p">):</code>
    <code class="k">def</code> <code class="nf">adapt</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">mean_</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">keepdims</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">std_</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">std</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">keepdims</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">call</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">):</code>
        <code class="n">eps</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">backend</code><code class="o">.</code><code class="n">epsilon</code><code class="p">()</code>  <code class="c1"># a small smoothing term</code>
        <code class="k">return</code> <code class="p">(</code><code class="n">inputs</code> <code class="o">-</code> <code class="bp">self</code><code class="o">.</code><code class="n">mean_</code><code class="p">)</code> <code class="o">/</code> <code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">std_</code> <code class="o">+</code> <code class="n">eps</code><code class="p">)</code></pre>

<p>Next, let’s look at another preprocessing layer for numerical features: the <code>Discretization</code> layer.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="The Discretization Layer"><div class="sect2" id="idm46324178947104">
<h2>The <code>Discretization</code> Layer</h2>

<p>This layer’s goal is to transform a numerical feature into a categorical feature by mapping value ranges (called bins) to categories. This is sometimes useful for features with multimodal distributions, or with features that have a highly non-linear relationship with the target. For example, the following code maps a numerical <code>age</code> feature to 3 categories: less than 18, 18 to 50 (not included), and 50 or over.</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">age</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">constant</code><code class="p">([[</code><code class="mf">10.</code><code class="p">],</code> <code class="p">[</code><code class="mf">93.</code><code class="p">],</code> <code class="p">[</code><code class="mf">57.</code><code class="p">],</code> <code class="p">[</code><code class="mf">18.</code><code class="p">],</code> <code class="p">[</code><code class="mf">37.</code><code class="p">],</code> <code class="p">[</code><code class="mf">5.</code><code class="p">]])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">discretize_layer</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Discretization</code><code class="p">(</code><code class="n">bin_boundaries</code><code class="o">=</code><code class="p">[</code><code class="mf">18.</code><code class="p">,</code> <code class="mf">50.</code><code class="p">])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">age_categories</code> <code class="o">=</code> <code class="n">discretize_layer</code><code class="p">(</code><code class="n">age</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">age_categories</code>
<code class="go">&lt;tf.Tensor: shape=(6, 1), dtype=int64, numpy=array([[0],[2],[2],[1],[1],[0]])&gt;</code></pre>

<p>In this example, we provided the desired bin boundaries. If you prefer, you can instead provide the number of bins you want, then call the layer’s <code>adapt()</code> method to let it find the appropriate bin boundaries, based on the value percentiles. For example, if we set <code>num_bins=3</code>, then the bin boundaries will be located at the values just below the 33<sup>rd</sup> and 66<sup>th</sup> percentiles (in this example, at the values 10 and 37).</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">discretize_layer</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Discretization</code><code class="p">(</code><code class="n">num_bins</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">discretize_layer</code><code class="o">.</code><code class="n">adapt</code><code class="p">(</code><code class="n">age</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">age_categories</code> <code class="o">=</code> <code class="n">discretize_layer</code><code class="p">(</code><code class="n">age</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">age_categories</code>
<code class="go">&lt;tf.Tensor: shape=(6, 1), dtype=int64, numpy=array([[1],[2],[2],[1],[2],[0]])&gt;</code></pre>

<p>Category identifiers such as these should generally not be passed directly to a neural network, as their values cannot be meaningfully compared. Instead, they should be encoded, for example using one-hot encoding. Let’s look at how to do this now.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="The CategoryEncoding Layer"><div class="sect2" id="idm46324178302592">
<h2>The <code>CategoryEncoding</code> Layer</h2>

<p>When there are few categories (e.g., less than a dozen or two), then one-hot encoding is often a good option (as discussed in <a data-type="xref" href="ch02.xhtml#project_chapter">Chapter 2</a>). For this, Keras provides the <code>CategoryEncoding</code> layer. For example, let’s one-hot encode the <code>age_categories</code> feature we just created:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">onehot_layer</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">CategoryEncoding</code><code class="p">(</code><code class="n">num_tokens</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">onehot_layer</code><code class="p">(</code><code class="n">age_categories</code><code class="p">)</code>
<code class="go">&lt;tf.Tensor: shape=(6, 3), dtype=float32, numpy=</code>
<code class="go">array([[0., 1., 0.],</code>
<code class="go">       [0., 0., 1.],</code>
<code class="go">       [0., 0., 1.],</code>
<code class="go">       [0., 1., 0.],</code>
<code class="go">       [0., 0., 1.],</code>
<code class="go">       [1., 0., 0.]], dtype=float32)&gt;</code></pre>

<p>If you try to encode more than one categorical feature at a time (which only makes sense if they all use the same categories), the <code>CategoryEncoding</code> class will perform <em>multi-hot encoding</em> by default: the output tensor will contain a 1 for each category present in <em>any</em> input feature. For example:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">two_age_categories</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">0</code><code class="p">]])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">onehot_layer</code><code class="p">(</code><code class="n">two_age_categories</code><code class="p">)</code>
<code class="go">&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=</code>
<code class="go">array([[1., 1., 0.],</code>
<code class="go">       [0., 0., 1.],</code>
<code class="go">       [1., 0., 1.]], dtype=float32)&gt;</code></pre>

<p>If you believe it’s useful to know how many times each category occurred, you can set <code>output_mode="count"</code> when creating the <code>CategoryEncoding</code> layer, in which case the output tensor will contain the number of occurrences of each category. In the preceding example, the output would be the same except for the second row, which would become <code>[0., 0., 2.]</code>.</p>

<p>Note that both multi-hot encoding and count encoding lose information, since it’s not possible to know which feature each active category came from. For example, both <code>[0, 1]</code> and <code>[1, 0]</code> are encoded as <code>[1., 1., 0.]</code>. If you want to avoid this, then you need to one-hot encode each feature separately and concatenate the outputs. This way, <code>[0, 1]</code> would get encoded as <code>[1., 0., 0., 0., 1., 0.]</code> and <code>[1, 0]</code> would get encoded as <code>[0., 1., 0., 1., 0., 0.]</code>. You can get the same result by tweaking the category identifiers so they don’t overlap, for example:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">onehot_layer</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">CategoryEncoding</code><code class="p">(</code><code class="n">num_tokens</code><code class="o">=</code><code class="mi">3</code> <code class="o">+</code> <code class="mi">3</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">onehot_layer</code><code class="p">(</code><code class="n">two_age_categories</code> <code class="o">+</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">3</code><code class="p">])</code>  <code class="c1"># adds 3 to the second feature</code>
<code class="go">&lt;tf.Tensor: shape=(3, 6), dtype=float32, numpy=</code>
<code class="go">array([[0., 1., 0., 1., 0., 0.],</code>
<code class="go">       [0., 0., 1., 0., 0., 1.],</code>
<code class="go">       [0., 0., 1., 1., 0., 0.]], dtype=float32)&gt;</code></pre>

<p>In this output, the first 3 columns correspond to the first feature, and the last 3 correspond to the second feature. This allows the model to distinguish the two features. However, it also increases the number of features fed to the model, and thereby requires more model parameters. It’s hard to know in advance whether a single multi-hot encoding or a per-feature one-hot encoding will work best: it depends on the task, and you may need to test both options.</p>

<p>Now you can encode categorical integer features using one-hot or multi-hot encoding. But what about categorical text features? For this, you can use the <code>StringLookup</code> layer.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="The StringLookup Layer"><div class="sect2" id="idm46324178302000">
<h2>The <code>StringLookup</code> Layer</h2>

<p>Let’s use the <code>StringLookup</code> layer to one-hot encode a <code>cities</code> feature:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">cities</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"Auckland"</code><code class="p">,</code> <code class="s2">"Paris"</code><code class="p">,</code> <code class="s2">"Paris"</code><code class="p">,</code> <code class="s2">"San Francisco"</code><code class="p">]</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">str_lookup_layer</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">StringLookup</code><code class="p">()</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">str_lookup_layer</code><code class="o">.</code><code class="n">adapt</code><code class="p">(</code><code class="n">cities</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">str_lookup_layer</code><code class="p">([[</code><code class="s2">"Paris"</code><code class="p">],</code> <code class="p">[</code><code class="s2">"Auckland"</code><code class="p">],</code> <code class="p">[</code><code class="s2">"Auckland"</code><code class="p">],</code> <code class="p">[</code><code class="s2">"Montreal"</code><code class="p">]])</code>
<code class="go">&lt;tf.Tensor: shape=(4, 1), dtype=int64, numpy=array([[1], [3], [3], [0]])&gt;</code></pre>

<p>We first create a <code>StringLookup</code> layer, then we adapt it to the data: it finds that there are three distinct categories. Then we use the layer to encode a few cities: they are encoded as integers by default. Unknown categories get mapped to 0, as is the case for “Montreal” in this example. The known categories are numbered starting at 1, from the most frequent categories to the least frequent.</p>

<p>Conveniently, if you set <code>output_mode="one_hot"</code> when creating the <code>StringLookup</code> layer, it will output a one-hot vector for each category, instead of an integer:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">str_lookup_layer</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">StringLookup</code><code class="p">(</code><code class="n">output_mode</code><code class="o">=</code><code class="s2">"one_hot"</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">str_lookup_layer</code><code class="o">.</code><code class="n">adapt</code><code class="p">(</code><code class="n">cities</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">str_lookup_layer</code><code class="p">([[</code><code class="s2">"Paris"</code><code class="p">],</code> <code class="p">[</code><code class="s2">"Auckland"</code><code class="p">],</code> <code class="p">[</code><code class="s2">"Auckland"</code><code class="p">],</code> <code class="p">[</code><code class="s2">"Montreal"</code><code class="p">]])</code>
<code class="go">&lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy=</code>
<code class="go">array([[0., 1., 0., 0.],</code>
<code class="go">       [0., 0., 0., 1.],</code>
<code class="go">       [0., 0., 0., 1.],</code>
<code class="go">       [1., 0., 0., 0.]], dtype=float32)&gt;</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>Keras also includes an <code>IntegerLookup</code> layer which acts much like the <code>StringLookup</code> layer but takes integers as input, rather than strings.</p>
</div>

<p>If the training set is very large, it may be convenient to adapt the layer to just a random subset of the training set. In this case, the layer’s <code>adapt()</code> method may miss some of the rarer categories. By default, it would then map them all to category 0, making them indistinguishable by the model. To reduce this risk (while still adapting the layer only on a subset of the training set), you can set <code>num_oov_indices</code> to an integer greater than 1. This is the number of out-of-vocabulary (oov) buckets to use: each unknown category will get mapped pseudo-randomly to one of the oov buckets, using a hash function modulo the number of oov buckets. This will allow the model to distinguish at least some of the rare categories. For example:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">str_lookup_layer</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">StringLookup</code><code class="p">(</code><code class="n">num_oov_indices</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">str_lookup_layer</code><code class="o">.</code><code class="n">adapt</code><code class="p">(</code><code class="n">cities</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">str_lookup_layer</code><code class="p">([[</code><code class="s2">"Paris"</code><code class="p">],</code> <code class="p">[</code><code class="s2">"Auckland"</code><code class="p">],</code> <code class="p">[</code><code class="s2">"Foo"</code><code class="p">],</code> <code class="p">[</code><code class="s2">"Bar"</code><code class="p">],</code> <code class="p">[</code><code class="s2">"Baz"</code><code class="p">]])</code>
<code class="go">&lt;tf.Tensor: shape=(4, 1), dtype=int64, numpy=array([[5], [7], [4], [3], [4]])&gt;</code></pre>

<p>Since there are 5 oov buckets, the first known category’s id is now 5 (“Paris”). But “Foo”, “Bar”, and “Baz” are unknown, so they each get mapped to one of the oov buckets. “Bar” gets its own dedicated bucket (with id 3), but sadly “Foo” and “Baz” happen to be mapped to the same bucket (with id 4), so they remain indistinguishable by the model. This is called a <em>hashing collision</em>. The only way to reduce the risk of collision is to increase the number of oov buckets. However, this will also increase the total number of categories, which will require more RAM and extra model parameters once the categories are one-hot encoded. So don’t increase that number too much.</p>

<p>This idea of mapping categories pseudo-randomly to buckets is called the <em>hashing trick</em>. Keras provides a dedicated layer which does just that: the <code>Hashing</code> layer.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="The Hashing Layer"><div class="sect2" id="idm46324177896368">
<h2>The <code>Hashing</code> Layer</h2>

<p>For each category, the <code>Hashing</code> layer computes a hash, modulo the number of buckets (or “bins”). The mapping is entirely pseudo-random, but stable across runs and platforms (i.e., the same category will always be mapped to the same integer, as long as the number of bins is unchanged). For example, let’s use the <code>Hashing</code> layer to encode a few cities:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">hashing_layer</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Hashing</code><code class="p">(</code><code class="n">num_bins</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">hashing_layer</code><code class="p">([[</code><code class="s2">"Paris"</code><code class="p">],</code> <code class="p">[</code><code class="s2">"Tokyo"</code><code class="p">],</code> <code class="p">[</code><code class="s2">"Auckland"</code><code class="p">],</code> <code class="p">[</code><code class="s2">"Montreal"</code><code class="p">]])</code>
<code class="go">&lt;tf.Tensor: shape=(4, 1), dtype=int64, numpy=array([[0], [1], [9], [1]])&gt;</code></pre>

<p>The benefit of this layer is that it does not need to be adapted at all, which may sometimes be useful, especially in an out-of-core setting (when the dataset is too large to fit in memory). However, we once again get a hashing collision: “Tokyo” and “Montreal” are mapped to the same id, making them indistinguishable by the model. So it’s usually preferable to stick to the <code>StringLookup</code> layer.</p>

<p>Let’s now look at another way to encode categories: trainable embeddings.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Encoding Categorical Features Using Embeddings"><div class="sect2" id="idm46324177810752">
<h2>Encoding Categorical Features Using Embeddings</h2>

<p>An<a data-type="indexterm" data-primary="categorical features" data-secondary="encoding using embeddings" id="idm46324177808864"/> embedding is a dense representation of some higher-dimensional data, such as a category, or a word in a vocabulary. If there are 50,000 possible categories, then one-hot encoding would produce a 50,000-dimensional sparse vector (i.e., containing mostly zeroes). In contrast, an embedding would be a comparatively small dense vector, for example with just 100 dimensions. In Deep Learning, embeddings are usually initialized randomly, and they are then trained by Gradient Descent, along with the other model parameters. For example the <code>"NEAR BAY"</code> category in the California housing dataset could be represented initially by a random vector such as <code>[0.131, 0.890]</code>, while the <code>"NEAR OCEAN"</code> category might be represented by another random vector such as <code>[0.631, 0.791]</code>. In this example, we use 2D embeddings, but the number of dimensions is a hyperparameter you can tweak. Since these embeddings are trainable, they will gradually improve during training; and as they represent fairly similar categories in this case, Gradient Descent will certainly end up pushing them closer together, while it will tend to move them away from the <code>"INLAND"</code> category’s embedding (see <a data-type="xref" href="#embedding_diagram">Figure 13-6</a>). Indeed, the better the representation, the easier it will be for the neural network to make accurate predictions, so training tends to make embeddings useful representations of the categories. This<a data-type="indexterm" data-primary="representation learning" id="idm46324177804736"/> is called <em>representation learning</em> (we will see other types of representation learning in Chapter 17).</p>

<figure class="smallereighty"><div id="embedding_diagram" class="figure">
<img src="Images/mls3_1306.png" alt="mls3 1306" width="1510" height="748"/>
<h6><span class="label">Figure 13-6. </span>Embeddings will gradually improve during training</h6>
</div></figure>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46324177797168">
<h5>Word Embeddings</h5>
<p>Not<a data-type="indexterm" data-primary="word embeddings" id="idm46324177795792"/> only will embeddings generally be useful representations for the task at hand, but quite often these same embeddings can be reused successfully for other tasks. The most common example of this is <em>word embeddings</em> (i.e., embeddings of individual words): when you are working on a natural language processing task, you are often better off reusing pretrained word embeddings than training your own.</p>

<p>The idea of using vectors to represent words dates back to the 1960s, and many sophisticated techniques have been used to generate useful vectors, including using neural networks. But things really took off in 2013, when Tomáš Mikolov and other Google researchers published a <a href="https://homl.info/word2vec">paper</a>⁠<sup><a data-type="noteref" id="idm46324177793536-marker" href="ch13.xhtml#idm46324177793536">7</a></sup> describing an efficient technique to learn word embeddings using neural networks, significantly outperforming previous attempts. This allowed them to learn embeddings on a very large corpus of text: they trained a neural network to predict the words near any given word, and obtained astounding word embeddings. For example, synonyms had very close embeddings, and semantically related words such as France, Spain, and Italy ended up clustered together.</p>

<p>It’s not just about proximity, though: word embeddings were also organized along meaningful axes in the embedding space. Here is a famous example: if you compute King – Man + Woman (adding and subtracting the embedding vectors of these words), then the result will be very close to the embedding of the word Queen (see <a data-type="xref" href="#word_embedding_diagram">Figure 13-7</a>). In other words, the word embeddings encode the concept of gender! Similarly, you can compute Madrid – Spain + France, and the result is close to Paris, which seems to show that the notion of capital city was also encoded in the <span class="keep-together">embeddings</span>.</p>

<figure class="smallereighty"><div id="word_embedding_diagram" class="figure">
<img src="Images/mls3_1307.png" alt="mls3 1307" width="1288" height="798"/>
<h6><span class="label">Figure 13-7. </span>Word embeddings of similar words tend to be close, and some axes seem to encode meaningful concepts</h6>
</div></figure>

<p>Unfortunately, word embeddings sometimes capture our worst biases. For example, although they correctly learn that Man is to King as Woman is to Queen, they also seem to learn that Man is to Doctor as Woman is to Nurse: quite a sexist bias! To be fair, this particular example is probably exaggerated, as was pointed out in a <a href="https://homl.info/fairembeds">2019 paper</a>⁠<sup><a data-type="noteref" id="idm46324177786832-marker" href="ch13.xhtml#idm46324177786832">8</a></sup> by Malvina Nissim et al. Nevertheless, ensuring fairness in Deep Learning algorithms is an important and active research topic.</p>
</div></aside>

<p>Keras provides an <code>Embedding</code> layer, which wraps an <em>embedding matrix</em>: this matrix has one row per category, and one column per embedding dimension. By default, it is initialized randomly. To convert a category id to an embedding, the <code>Embedding</code> layer just looks up and returns the row that corresponds to that category. That’s all there is to it! For example, let’s initialize an <code>Embedding</code> layer with 5 rows, and 2D embeddings, and let’s use it to encode some categories:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">tf</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">set_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">embedding_layer</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Embedding</code><code class="p">(</code><code class="n">input_dim</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">output_dim</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">embedding_layer</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mi">2</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">2</code><code class="p">]))</code>
<code class="go">&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=</code>
<code class="go">array([[-0.04663396,  0.01846724],</code>
<code class="go">       [-0.02736737, -0.02768031],</code>
<code class="go">       [-0.04663396,  0.01846724]], dtype=float32)&gt;</code></pre>

<p>As you can see, category 2 gets encoded (twice) as the 2D vector <code>[-0.04663396,  0.01846724]</code>, while category 4 gets encoded as <code>[-0.02736737, -0.02768031]</code>. Since the layer is not trained yet, these encodings are just random.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>An <code>Embedding</code> layer is initialized randomly, so it does not make sense to use it outside of a model as a standalone preprocessing layer, unless you initialize it with pretrained weights.</p>
</div>

<p>If you want to embed a categorical text attribute, you can simply chain a <code>StringLookup</code> layer and an <code>Embedding</code> layer, like this:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">tf</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">set_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">ocean_prox</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"&lt;1H OCEAN"</code><code class="p">,</code> <code class="s2">"INLAND"</code><code class="p">,</code> <code class="s2">"NEAR OCEAN"</code><code class="p">,</code> <code class="s2">"NEAR BAY"</code><code class="p">,</code> <code class="s2">"ISLAND"</code><code class="p">]</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">str_lookup_layer</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">StringLookup</code><code class="p">()</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">str_lookup_layer</code><code class="o">.</code><code class="n">adapt</code><code class="p">(</code><code class="n">ocean_prox</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">lookup_and_embed</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
<code class="gp">... </code>    <code class="n">str_lookup_layer</code><code class="p">,</code>
<code class="gp">... </code>    <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Embedding</code><code class="p">(</code><code class="n">input_dim</code><code class="o">=</code><code class="n">str_lookup_layer</code><code class="o">.</code><code class="n">vocabulary_size</code><code class="p">(),</code>
<code class="gp">... </code>                              <code class="n">output_dim</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="gp">... </code><code class="p">])</code>
<code class="gp">...</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">lookup_and_embed</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="s2">"&lt;1H OCEAN"</code><code class="p">],</code> <code class="p">[</code><code class="s2">"ISLAND"</code><code class="p">],</code> <code class="p">[</code><code class="s2">"&lt;1H OCEAN"</code><code class="p">]]))</code>
<code class="go">&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=</code>
<code class="go">array([[-0.01896119,  0.02223358],</code>
<code class="go">       [ 0.02401174,  0.03724445],</code>
<code class="go">       [-0.01896119,  0.02223358]], dtype=float32)&gt;</code></pre>

<p>Note that the number of rows in the embedding matrix needs to be equal to the vocabulary size: that’s the total number of categories, including the known categories plus the oov buckets (just one by default). The <code>vocabulary_size()</code> method of the <code>StringLookup</code> class conveniently returns this number.</p>
<div data-type="tip"><h6>Tip</h6>
<p>In this example we used 2D embeddings, but as a rule of thumb embeddings typically have 10 to 300 dimensions, depending on the task, the vocabulary size, and the size of your training set. You will have to tune this hyperparameter.</p>
</div>

<p>Putting everything together, we can now create a Keras model that can process a categorical text feature along with regular numerical features and learn an embedding for each category (as well as for each oov bucket):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">X_train_num</code><code class="p">,</code> <code class="n">X_train_cat</code><code class="p">,</code> <code class="n">y_train</code> <code class="o">=</code> <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># load the training set</code>
<code class="n">X_valid_num</code><code class="p">,</code> <code class="n">X_valid_cat</code><code class="p">,</code> <code class="n">y_valid</code> <code class="o">=</code> <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># and the validation set</code>

<code class="n">num_input</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Input</code><code class="p">(</code><code class="n">shape</code><code class="o">=</code><code class="p">[</code><code class="mi">8</code><code class="p">],</code> <code class="n">name</code><code class="o">=</code><code class="s2">"num"</code><code class="p">)</code>
<code class="n">cat_input</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Input</code><code class="p">(</code><code class="n">shape</code><code class="o">=</code><code class="p">[],</code> <code class="n">dtype</code><code class="o">=</code><code class="n">tf</code><code class="o">.</code><code class="n">string</code><code class="p">,</code> <code class="n">name</code><code class="o">=</code><code class="s2">"cat"</code><code class="p">)</code>
<code class="n">cat_embeddings</code> <code class="o">=</code> <code class="n">lookup_and_embed</code><code class="p">(</code><code class="n">cat_input</code><code class="p">)</code>
<code class="n">encoded_inputs</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">concatenate</code><code class="p">([</code><code class="n">num_input</code><code class="p">,</code> <code class="n">cat_embeddings</code><code class="p">])</code>
<code class="n">outputs</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">)(</code><code class="n">encoded_inputs</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Model</code><code class="p">(</code><code class="n">inputs</code><code class="o">=</code><code class="p">[</code><code class="n">num_input</code><code class="p">,</code> <code class="n">cat_input</code><code class="p">],</code> <code class="n">outputs</code><code class="o">=</code><code class="p">[</code><code class="n">outputs</code><code class="p">])</code>
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"mse"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="s2">"sgd"</code><code class="p">)</code>
<code class="n">history</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">((</code><code class="n">X_train_num</code><code class="p">,</code> <code class="n">X_train_cat</code><code class="p">),</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code>
                    <code class="n">validation_data</code><code class="o">=</code><code class="p">((</code><code class="n">X_valid_num</code><code class="p">,</code> <code class="n">X_valid_cat</code><code class="p">),</code> <code class="n">y_valid</code><code class="p">))</code></pre>

<p>This model takes two inputs: <code>num_input</code> which contains eight numerical features per instance, plus <code>cat_input</code> which contains a single categorical text input per instance. The model uses the <code>lookup_and_embed</code> model we created earlier to encode each ocean-proximity category as the corresponding trainable embedding. Next, it concatenates the numerical inputs and the embeddings to produce the complete encoded inputs, which are ready to be fed to a neural network. We could add any kind of neural network at this point, but for simplicity we just add a single dense output layer, and we create the Keras <code>Model</code>. Next we compile it, and train it, passing both the numerical and categorical inputs.</p>

<p>As we saw in <a data-type="xref" href="ch10.xhtml#ann_chapter">Chapter 10</a>, since the <code>Input</code> layers are named <code>"num"</code> and <code>"cat"</code>, we could also have passed the training data to the <code>fit()</code> method using a dictionary instead of a tuple: <code>{"num": X_train_num, "cat": X_train_cat}</code>. Or we could have passed a <code>tf.data.Dataset</code> containing batches, each represented as <code>((X_batch_num, X_batch_cat), y_batch)</code> or as <code>({"num": X_batch_num, "cat": X_batch_cat}, y_batch)</code>. And of course the same goes for the validation data.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>One-hot encoding followed by a <code>Dense</code> layer (with no activation function and no biases) is equivalent to an <code>Embedding</code> layer. However, the <code>Embedding</code> layer uses way fewer computations as it avoids many multiplications by zero—the performance difference becomes clear when the size of the embedding matrix grows. The <code>Dense</code> layer’s weight matrix plays the role of the embedding matrix. For example, using one-hot vectors of size 20 and a <code>Dense</code> layer with 10 units is equivalent to using an <code>Embedding</code> layer with <code>input_dim=20</code> and <code>output_dim=10</code>. As a result, it would be wasteful to use more embedding dimensions than the number of units in the layer that follows the <code>Embedding</code> layer.</p>
</div>

<p>OK, now that you know how to encode categorical features, it’s time to turn our attention to text preprocessing.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Text Preprocessing"><div class="sect2" id="idm46324177810160">
<h2>Text Preprocessing</h2>

<p>Keras provides a <code>TextVectorization</code> layer for basic text preprocessing. Much like the <code>StringLookup</code> layer, you must either pass it a vocabulary upon creation, or let it learn the vocabulary from some training data using the <code>adapt()</code> method. Let’s look at an example:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">train_data</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"To be"</code><code class="p">,</code> <code class="s2">"!(to be)"</code><code class="p">,</code> <code class="s2">"That's the question"</code><code class="p">,</code> <code class="s2">"Be, be, be."</code><code class="p">]</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">text_vec_layer</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">TextVectorization</code><code class="p">()</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">text_vec_layer</code><code class="o">.</code><code class="n">adapt</code><code class="p">(</code><code class="n">train_data</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">text_vec_layer</code><code class="p">([</code><code class="s2">"Be good!"</code><code class="p">,</code> <code class="s2">"Question: be or be?"</code><code class="p">])</code>
<code class="go">&lt;tf.Tensor: shape=(2, 4), dtype=int64, numpy=</code>
<code class="go">array([[2, 1, 0, 0],</code>
<code class="go">       [6, 2, 1, 2]])&gt;</code></pre>

<p>The two sentences “Be good!” and “Question: be or be?” were encoded as <code>[2, 1, 0, 0]</code> and <code>[6, 2, 1, 2]</code>, respectively. The vocabulary was learned from the 4 sentences in the training data: “be” = 2, “to” = 3, etc. To construct the vocabulary, the <code>adapt()</code> method first converted the training sentences to lowercase and removed punctuation, which is why “Be”, “be,” and “be?” are all encoded as “be” = 2. Next, the sentences were split at the whitespaces, and the resulting words were sorted by descending frequency, producing the final vocabulary. When encoding sentences, unknown words get encoded as 1s. Lastly, since the first sentence is shorter than the second, it was padded with 0s.</p>
<div data-type="tip"><h6>Tip</h6>
<p>The <code>TextVectorization</code> layer has many options. For example, you can preserve the case and punctuation if you want, by setting <code>standardize=None</code>, or you can pass any standardization function you please as the <code>standardize</code> argument. You can prevent splitting by setting <code>split=None</code>, or you can even pass your own splitting function instead. You can set the <code>output_sequence_length</code> argument to ensure that the output sequences all get cropped or padded to the desired length. Or, you can set <code>ragged=True</code> to get a ragged tensor instead of a regular tensor. Please check out the documentation for more options.</p>
</div>

<p>The word ids must be encoded, typically using an <code>Embedding</code> layer: we will do this in Chapter 16. Alternatively, you can set the <code>TextVectorization</code> layer’s <code>output_mode</code> argument to <code>"multi_hot"</code> or <code>"count"</code> to get the corresponding encodings. However, simply counting words is usually not ideal: indeed, words like “to” or “the” are so frequent that they hardly matter at all. Conversely rarer words such as “basketball” are much more informative. So rather than setting <code>output_mode</code> to <code>"multi_hot"</code> or <code>"count"</code>, it is usually preferable to set it to <code>"tf_idf"</code>, which stands for <em>Term-Frequency</em> × <em>Inverse-Document-Frequency</em> (TF-IDF). This is similar to the count encoding, but words that occur frequently in the training data are downweighted, and conversely, rare words are upweighted. For example:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">text_vec_layer</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">TextVectorization</code><code class="p">(</code><code class="n">output_mode</code><code class="o">=</code><code class="s2">"tf_idf"</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">text_vec_layer</code><code class="o">.</code><code class="n">adapt</code><code class="p">(</code><code class="n">train_data</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">text_vec_layer</code><code class="p">([</code><code class="s2">"Be good!"</code><code class="p">,</code> <code class="s2">"Question: be or be?"</code><code class="p">])</code>
<code class="go">&lt;tf.Tensor: shape=(2, 6), dtype=float32, numpy=</code>
<code class="go">array([[0.96725637, 0.6931472 , 0. , 0. , 0. , 0.        ],</code>
<code class="go">       [0.96725637, 1.3862944 , 0. , 0. , 0. , 1.0986123 ]], dtype=float32)&gt;</code></pre>

<p>There are many TF-IDF variants, but the way the <code>TextVectorization</code> layer implements it is by multiplying each word count by a weight equal to log(1 + <em>d</em> / (<em>f</em> + 1)) where <em>d</em> is the total number of sentences (a.k.a., documents) in the training data, and <em>f</em> counts how many of these training sentences contain the given word. For example, there are <em>d</em> = 4 sentences in the training data, and the word “be” appears in <em>f</em> = 3 of these. Since the word “be” occurs twice in the sentence “Question: be or be?”, it gets encoded as 2 × log(1 + 4 / (1 + 3)) ≈ 1.3862944. The word “question” only appears once, but since it is a less common word, its encoding is almost as high: 1 × log(1 + 4 / (1 + 1)) ≈ 1.0986123. Note that the average weight is used for unknown words.</p>

<p>This approach to text encoding is straightforward to use and it can give fairly good results for basic natural language processing tasks, but it has several important limitations: it only works with languages that separate words with spaces, it doesn’t distinguish between homonyms (e.g., “to bear” versus “teddy bear”), it gives no hint to your model that words like “evolution” and “evolutionary” are related, etc. And if you use multi-hot, count or TF-IDF encoding, then the order of the words is lost. So what are the other options?</p>

<p>One option is to use the <a href="https://www.tensorflow.org/text">TensorFlow Text library</a>, which provides more advanced text preprocessing features than the <code>TextVectorization</code> layer. For example, it includes several subword tokenizers capable of splitting the text into tokens smaller than words, which makes it possible for the model to more easily detect that “evolution” and “evolutionary” have something in common (more on subword tokenization in Chapter 16).</p>

<p>Another option is to use pretrained language model components. Let’s look at this now.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Using Pretrained Language Model Components"><div class="sect2" id="idm46324177333888">
<h2>Using Pretrained Language Model Components</h2>

<p>The <a href="https://www.tensorflow.org/hub">TensorFlow Hub library</a> makes it easy to reuse pretrained model components in your own models, for text, image, audio, or more. These model components are called <em>modules</em>. Simply browse the <a href="https://tfhub.dev">TF Hub repository</a>, find the one you need, and copy the code example into your project, and the module will be automatically downloaded and bundled into a Keras layer that you can simply include in your model. Modules typically contain both preprocessing code and pretrained weights, and they generally require no extra training (but of course, the rest of your model will certainly require training).</p>

<p>For example, some powerful pretrained language models are available. Let’s use the <code>nnlm-en-dim50</code> module, version 2, to produce 50-dimensional sentence embeddings. TensorFlow Hub is not bundled with TensorFlow, but if you are running on Colab or if you followed the installation instructions at <a href="https://homl.info/install"><em class="hyperlink">https://homl.info/install</em></a>, then it’s already installed. Let’s import it and use it:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">tensorflow_hub</code> <code class="k">as</code> <code class="nn">hub</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">hub_layer</code> <code class="o">=</code> <code class="n">hub</code><code class="o">.</code><code class="n">KerasLayer</code><code class="p">(</code><code class="s2">"https://tfhub.dev/google/nnlm-en-dim50/2"</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">sentence_embeddings</code> <code class="o">=</code> <code class="n">hub_layer</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">constant</code><code class="p">([</code><code class="s2">"To be"</code><code class="p">,</code> <code class="s2">"Not to be"</code><code class="p">]))</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">sentence_embeddings</code><code class="o">.</code><code class="n">numpy</code><code class="p">()</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code>
<code class="go">array([[-0.25,  0.28,  0.01,  0.1 ,  [...] ,  0.05,  0.31],</code>
<code class="go">       [-0.2 ,  0.2 , -0.08,  0.02,  [...] , -0.04,  0.15]], dtype=float32)</code></pre>

<p>You just need to include this <code>hub_layer</code> in your model, and you’re ready to go. Note that this particular language model was trained on the English language, but many other languages are available, as well as international models.</p>

<p>Last but not least, the excellent open source <a href="https://huggingface.co/docs/transformers/">Transformers library by Hugging Face</a> also makes it easy to include powerful language model components inside your own models. You can browse the <a href="https://huggingface.co/models">HuggingFace Hub</a>, choose the model you want, and use the provided code examples to get started. It used to contain only language models, but it has now expanded to image models and more.</p>

<p>We will come back to natural language processing in more depth in Chapter 16. Let’s now look at Keras’s image preprocessing layers.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Image Preprocessing Layers"><div class="sect1" id="idm46324179001376">
<h1>Image Preprocessing Layers</h1>

<p>The Keras preprocessing API includes three image preprocessing layers:</p>

<ul>
<li>
<p><code>tf.keras.layers.Resizing</code> resizes the input images to the desired size. For example <code>Resizing(height=100, width=200)</code> resizes the image to 100 × 200, possibly distorting the image. If you set <code>crop_to_aspect_ratio=True</code>, then the image will be cropped to the target image ratio, to avoid distortion.</p>
</li>
<li>
<p><code>tf.keras.layers.Rescaling</code> rescales the pixel values, for example <code>Rescaling(scale=2/255, offset=-1)</code> scales the values from 0 → 255 to –1 → 1.</p>
</li>
<li>
<p><code>tf.keras.layers.CenterCrop</code> crops the image, keeping only a center patch of the desired height and width.</p>
</li>
</ul>

<p>For example, let’s load a couple of sample images and center-crop them. For this, we will use Scikit-Learn’s <code>load_sample_images()</code> function, which loads two color images: one of a Chinese temple, and the other of a flower (this requires the Pillow library, which should already be installed if you are using Colab or if you followed the installation instructions).</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_sample_images</code>

<code class="n">images</code> <code class="o">=</code> <code class="n">load_sample_images</code><code class="p">()[</code><code class="s2">"images"</code><code class="p">]</code>
<code class="n">crop_image_layer</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">CenterCrop</code><code class="p">(</code><code class="n">height</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">width</code><code class="o">=</code><code class="mi">100</code><code class="p">)</code>
<code class="n">cropped_images</code> <code class="o">=</code> <code class="n">crop_image_layer</code><code class="p">(</code><code class="n">images</code><code class="p">)</code></pre>

<p>Keras also includes several layers for data augmentation: <code>RandomCrop</code>, <code>RandomFlip</code>, <code>RandomTranslation</code>, <code>RandomRotation</code>, <code>RandomZoom</code>, <code>RandomHeight</code>, <code>RandomWidth</code>, and <code>RandomContrast</code>. These layers are only active during training, and they randomly apply some transformation to the input images (their names are self-explanatory). Data augmentation will artificially increase the size of the training set, which often leads to improved performance, as long as the transformed images look like realistic (non-augmented) images. We’ll look at image processing more closely in the next chapter.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Under the hood, the Keras preprocessing layers are based on TensorFlow’s low-level API. For example, the <code>Normalization</code> layer uses <code>tf.nn.moments()</code> to compute both the mean and variance, the <code>Discretization</code> layer uses <code>tf.raw_ops.Bucketize()</code>, <code>CategoricalEncoding</code> uses <code>tf.math.bincount()</code>, <code>IntegerLookup</code> and <code>StringLookup</code> use the <code>tf.lookup</code> package, <code>Hashing</code> and <code>TextVectorization</code> use several ops from the <code>tf.strings</code> package, <code>Embedding</code> uses <code>tf.nn.embedding_lookup()</code>, and the image preprocessing layers use the ops from the <code>tf.image</code> package. If the Keras preprocessing API isn’t sufficient for your needs, you may occasionally need to use TensorFlow’s low-level API directly.</p>
</div>

<p>Now let’s look at another way to load data easily and efficiently in TensorFlow.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="The TensorFlow Datasets (TFDS) Project"><div class="sect1" id="idm46324177021648">
<h1>The TensorFlow Datasets (TFDS) Project</h1>

<p>The<a data-type="indexterm" data-primary="TensorFlow, data loading and preprocessing" data-secondary="TensorFlow Datasets (TFDS) Project" id="idm46324177019856"/><a data-type="indexterm" data-primary="TensorFlow, data loading and preprocessing" data-secondary="TensorFlow Datasets (TFDS) Project" id="idm46324177019008"/><a data-type="indexterm" data-primary="TF Datasets (TFDS)" id="idm46324177018160"/> <a href="https://tensorflow.org/datasets">TensorFlow Datasets</a> project makes it very easy to load common datasets, from small ones like MNIST or Fashion MNIST to huge datasets like ImageNet (you will need quite a bit of disk space!). The list includes image datasets, text datasets (including translation datasets), audio and video datasets, time series, and much more. You can visit <a href="https://homl.info/tfds"><em class="hyperlink">https://homl.info/tfds</em></a> to view the full list, along with a description of each dataset. You can also check out <a href="https://knowyourdata.withgoogle.com/">Know Your Data</a>, which is a tool to explore and understand many of the datasets provided by TFDS.</p>

<p>TFDS is not bundled with TensorFlow, but if you are running on Colab or if you followed the installation instructions at <a href="https://homl.info/install"><em class="hyperlink">https://homl.info/install</em></a>, then it’s already installed. You can then import <code>tensorflow_datasets</code>, usually as <code>tfds</code>, then call the <code>tfds.load()</code> function: it will download the data you want (unless it was already downloaded earlier) and return the data as a dictionary of datasets (typically one for training and one for testing, but this depends on the dataset you choose). For example, let’s download MNIST:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">tensorflow_datasets</code> <code class="kn">as</code> <code class="nn">tfds</code>

<code class="n">datasets</code> <code class="o">=</code> <code class="n">tfds</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s2">"mnist"</code><code class="p">)</code>
<code class="n">mnist_train</code><code class="p">,</code> <code class="n">mnist_test</code> <code class="o">=</code> <code class="n">datasets</code><code class="p">[</code><code class="s2">"train"</code><code class="p">],</code> <code class="n">datasets</code><code class="p">[</code><code class="s2">"test"</code><code class="p">]</code></pre>

<p>You can then apply any transformation you want (typically shuffling, batching, and prefetching), and you’re ready to train your model. Here is a simple example:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">batch</code> <code class="ow">in</code> <code class="n">mnist_train</code><code class="o">.</code><code class="n">shuffle</code><code class="p">(</code><code class="mi">10</code><code class="n">_000</code><code class="p">,</code> <code class="n">seed</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="mi">32</code><code class="p">)</code><code class="o">.</code><code class="n">prefetch</code><code class="p">(</code><code class="mi">1</code><code class="p">):</code>
    <code class="n">images</code> <code class="o">=</code> <code class="n">batch</code><code class="p">[</code><code class="s2">"image"</code><code class="p">]</code>
    <code class="n">labels</code> <code class="o">=</code> <code class="n">batch</code><code class="p">[</code><code class="s2">"label"</code><code class="p">]</code>
    <code class="c1"># [...] do something with the images and labels</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>The <code>load()</code> function can shuffle the files it downloads: just set <code>shuffle_files=True</code>. However, this may be insufficient, so it’s best to shuffle the training data some more.</p>
</div>

<p>Note that each item in the dataset is a dictionary containing both the features and the labels. But Keras expects each item to be a tuple containing two elements (again, the features and the labels). You could transform the dataset using the <code>map()</code> method, like this:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">mnist_train</code> <code class="o">=</code> <code class="n">mnist_train</code><code class="o">.</code><code class="n">shuffle</code><code class="p">(</code><code class="mi">10</code><code class="n">_000</code><code class="p">,</code> <code class="n">seed</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="mi">32</code><code class="p">)</code>
<code class="n">mnist_train</code> <code class="o">=</code> <code class="n">mnist_train</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="k">lambda</code> <code class="n">items</code><code class="p">:</code> <code class="p">(</code><code class="n">items</code><code class="p">[</code><code class="s2">"image"</code><code class="p">],</code> <code class="n">items</code><code class="p">[</code><code class="s2">"label"</code><code class="p">]))</code>
<code class="n">mnist_train</code> <code class="o">=</code> <code class="n">mnist_train</code><code class="o">.</code><code class="n">prefetch</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code></pre>

<p>But it’s simpler to ask the <code>load()</code> function to do this for you by setting <code>as_supervised=True</code> (obviously this works only for labeled datasets).</p>

<p>Lastly, TFDS provides a convenient way to split the data using the <code>split</code> argument. For example, if you want to use the first 90% of the training set for training, and the remaining 10% for validation, and the whole test set for testing, then you can set <code>split=["train[:90%]", "train[90%:]", "test"]</code>. The <code>load()</code> function will return all three sets. Here is a complete example, loading and splitting the MNIST dataset using TFDS, then using these sets to train and evaluate a simple Keras model:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">train_set</code><code class="p">,</code> <code class="n">valid_set</code><code class="p">,</code> <code class="n">test_set</code> <code class="o">=</code> <code class="n">tfds</code><code class="o">.</code><code class="n">load</code><code class="p">(</code>
    <code class="n">name</code><code class="o">=</code><code class="s2">"mnist"</code><code class="p">,</code>
    <code class="n">split</code><code class="o">=</code><code class="p">[</code><code class="s2">"train[:90%]"</code><code class="p">,</code> <code class="s2">"train[90%:]"</code><code class="p">,</code> <code class="s2">"test"</code><code class="p">],</code>
    <code class="n">as_supervised</code><code class="o">=</code><code class="bp">True</code>
<code class="p">)</code>
<code class="n">train_set</code> <code class="o">=</code> <code class="n">train_set</code><code class="o">.</code><code class="n">shuffle</code><code class="p">(</code><code class="mi">10</code><code class="n">_000</code><code class="p">,</code> <code class="n">seed</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="mi">32</code><code class="p">)</code><code class="o">.</code><code class="n">prefetch</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
<code class="n">valid_set</code> <code class="o">=</code> <code class="n">valid_set</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="mi">32</code><code class="p">)</code><code class="o">.</code><code class="n">cache</code><code class="p">()</code>
<code class="n">test_set</code> <code class="o">=</code> <code class="n">test_set</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="mi">32</code><code class="p">)</code><code class="o">.</code><code class="n">cache</code><code class="p">()</code>
<code class="n">tf</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">set_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(</code><code class="n">input_shape</code><code class="o">=</code><code class="p">(</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">)),</code>
    <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"softmax"</code><code class="p">)</code>
<code class="p">])</code>
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"sparse_categorical_crossentropy"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="s2">"nadam"</code><code class="p">,</code>
              <code class="n">metrics</code><code class="o">=</code><code class="p">[</code><code class="s2">"accuracy"</code><code class="p">])</code>
<code class="n">history</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">train_set</code><code class="p">,</code> <code class="n">validation_data</code><code class="o">=</code><code class="n">valid_set</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>
<code class="n">test_loss</code><code class="p">,</code> <code class="n">test_accuracy</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">evaluate</code><code class="p">(</code><code class="n">test_set</code><code class="p">)</code></pre>

<p>Congratulations, you reached the end of this quite technical chapter! You may feel that it is a bit far from the abstract beauty of neural networks, but the fact is Deep Learning often involves large amounts of data, and knowing how to load, parse, and preprocess it efficiently is a crucial skill to have. In the next chapter, we will look at convolutional neural networks, which are among the most successful neural net architectures for image processing and many other applications.<a data-type="indexterm" data-primary="" data-startref="Dloadtensor13" id="idm46324176611104"/></p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="idm46324177021056">
<h1>Exercises</h1>
<ol>
<li>
<p>Why would you want to use the tf.data API?</p>
</li>
<li>
<p>What are the benefits of splitting a large dataset into multiple files?</p>
</li>
<li>
<p>During training, how can you tell that your input pipeline is the bottleneck? What can you do to fix it?</p>
</li>
<li>
<p>Can you save any binary data to a TFRecord file, or only serialized protocol <span class="keep-together">buffers</span>?</p>
</li>
<li>
<p>Why would you go through the hassle of converting all your data to the <code>Example</code> protobuf format? Why not use your own protobuf definition?</p>
</li>
<li>
<p>When using TFRecords, when would you want to activate compression? Why not do it systematically?</p>
</li>
<li>
<p>Data can be preprocessed directly when writing the data files, or within the tf.data pipeline, or in preprocessing layers within your model. Can you list a few pros and cons of each option?</p>
</li>
<li>
<p>Name a few common ways you can encode categorical text features. What about text?</p>
</li>
<li>
<p>Load the Fashion MNIST dataset (introduced in <a data-type="xref" href="ch10.xhtml#ann_chapter">Chapter 10</a>); split it into a training set, a validation set, and a test set; shuffle the training set; and save each <span class="keep-together">dataset</span> to multiple TFRecord files. Each record should be a serialized <code>Example</code> protobuf with two features: the serialized image (use <code>tf.io.serialize_tensor()</code> to serialize each image), and the label.⁠<sup><a data-type="noteref" id="idm46324176716768-marker" href="ch13.xhtml#idm46324176716768">9</a></sup> Then use tf.data to create an efficient dataset for each set. Finally, use a Keras model to train these datasets, including a preprocessing layer to standardize each input feature. Try to make the input pipeline as efficient as possible, using TensorBoard to visualize profiling data.</p>
</li>
<li>
<p>In this exercise you will download a dataset, split it, create a <code>tf.data.Dataset</code> to load it and preprocess it efficiently, then build and train a binary classification model containing an <code>Embedding</code> layer:</p>
<ol>
<li>
<p>Download the <a href="https://homl.info/imdb">Large Movie Review Dataset</a>, which contains 50,000 movies reviews from the <a href="https://imdb.com/">Internet Movie Database</a>. The data is organized in two directories, <em>train</em> and <em>test</em>, each containing a <em>pos</em> subdirectory with 12,500 positive reviews and a <em>neg</em> subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words), but we will ignore them in this exercise.</p>
</li>
<li>
<p>Split the test set into a validation set (15,000) and a test set (10,000).</p>
</li>
<li>
<p>Use tf.data to create an efficient dataset for each set.</p>
</li>
<li>
<p>Create a binary classification model, using a <code>TextVectorization</code> layer to preprocess each review.</p>
</li>
<li>
<p>Add an <code>Embedding</code> layer and compute the mean embedding for each review, multiplied by the square root of the number of words (see Chapter 16). This rescaled mean embedding can then be passed to the rest of your model.</p>
</li>
<li>
<p>Train the model and see what accuracy you get. Try to optimize your pipelines to make training as fast as possible.</p>
</li>
<li>
<p>Use TFDS to load the same dataset more easily: <code>tfds.load("imdb_reviews")</code>.</p>
</li>

</ol>
</li>

</ol>

<p>Solutions to these exercises are available at the end of this chapter’s notebook, at <a href="https://homl.info/colab3"><em class="hyperlink">https://homl.info/colab3</em></a>.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm46324181209072"><sup><a href="ch13.xhtml#idm46324181209072-marker">1</a></sup> Imagine a sorted deck of cards on your left: suppose you just take the top three cards and shuffle them, then pick one randomly and put it to your right, keeping the other two in your hands. Take another card on your left, shuffle the three cards in your hands and pick one of them randomly, and put it on your right. When you are done going through all the cards like this, you will have a deck of cards on your right: do you think it will be perfectly shuffled?</p><p data-type="footnote" id="idm46324180602704"><sup><a href="ch13.xhtml#idm46324180602704-marker">2</a></sup> In general, just prefetching one batch is fine, but in some cases you may need to prefetch a few more. Alternatively, you can let TensorFlow decide automatically by passing <code>tf.data.AUTOTUNE</code> to <code>prefetch()</code>.</p><p data-type="footnote" id="idm46324180555728"><sup><a href="ch13.xhtml#idm46324180555728-marker">3</a></sup> But check out the experimental <code>tf.data.experimental.prefetch_to_device()</code> function, which can prefetch data directly to the GPU.</p><p data-type="footnote" id="idm46324179876704"><sup><a href="ch13.xhtml#idm46324179876704-marker">4</a></sup> Since protobuf objects are meant to be serialized and transmitted, they are called <em>messages</em>.</p><p data-type="footnote" id="idm46324179784528"><sup><a href="ch13.xhtml#idm46324179784528-marker">5</a></sup> This chapter contains the bare minimum you need to know about protobufs to use TFRecords. To learn more about protobufs, please visit <a href="https://homl.info/protobuf"><em class="hyperlink">https://homl.info/protobuf</em></a>.</p><p data-type="footnote" id="idm46324179538752"><sup><a href="ch13.xhtml#idm46324179538752-marker">6</a></sup> Why was <code>Example</code> even defined, since it contains no more than a <code>Features</code> object? Well, TensorFlow’s developers may one day decide to add more fields to it. As long as the new <code>Example</code> definition still contains the <code>features</code> field, with the same ID, it will be backward compatible. This extensibility is one of the great features of protobufs.</p><p data-type="footnote" id="idm46324177793536"><sup><a href="ch13.xhtml#idm46324177793536-marker">7</a></sup> Tomas Mikolov et al., “Distributed Representations of Words and Phrases and Their Compositionality,” <em>Proceedings of the 26th International Conference on Neural Information Processing Systems</em> 2 (2013): 3111–3119.</p><p data-type="footnote" id="idm46324177786832"><sup><a href="ch13.xhtml#idm46324177786832-marker">8</a></sup> Malvina Nissim et al., “Fair Is Better Than Sensational: Man Is to Doctor as Woman Is to Doctor,” arXiv preprint arXiv:1905.09866 (2019).</p><p data-type="footnote" id="idm46324176716768"><sup><a href="ch13.xhtml#idm46324176716768-marker">9</a></sup> For large images, you could use <code>tf.io.encode_jpeg()</code> instead. This would save a lot of space, but it would lose a bit of image quality.</p></div></div></section></div></body>
</html>
<!DOCTYPE html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.w3.org/2002/06/xhtml2/ http://www.w3.org/MarkUp/SCHEMA/xhtml2.xsd" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<link href="Styles/Style00.css" rel="stylesheet" type="text/css" />
<link href="Styles/Style01.css" rel="stylesheet" type="text/css" />

<style type="text/css">body{margin:1em;background-color:transparent!important;}#sbo-rt-content *{text-indent:0pt!important;}#sbo-rt-content .bq{margin-right:1em!important;}#sbo-rt-content *{word-wrap:break-word!important;word-break:break-word!important;}#sbo-rt-content table,#sbo-rt-content pre{overflow-x:unset!important;overflow:unset!important;overflow-y:unset!important;white-space:pre-wrap!important;}</style></head>
<body><div id="sbo-rt-content"><section data-type="preface" epub:type="preface" data-pdf-bookmark="Preface"><div class="preface" id="idm46324234951136">
<h1>Preface</h1>







<section data-type="sect1" data-pdf-bookmark="The Machine Learning Tsunami"><div class="sect1" id="idm46324234950144">
<h1>The Machine Learning Tsunami</h1>

<p>In 2006, Geoffrey Hinton<a data-type="indexterm" data-primary="Hinton, Geoffrey" id="idm46324235664816"/> et al. published <a href="https://homl.info/136">a paper</a>⁠⁠<sup><a data-type="noteref" id="idm46324235039456-marker" href="preface01.xhtml#idm46324235039456">1</a></sup> showing how to train a deep neural network capable of recognizing handwritten digits with state-of-the-art precision (&gt;98%). They branded this technique “Deep Learning.” A deep neural network<a data-type="indexterm" data-primary="deep neural networks (DNNs)" data-secondary="defined" id="idm46324231654992"/> is a (very) simplified model of our cerebral cortex, composed of a stack of layers of artificial neurons. Training a deep neural net was widely considered impossible at the time,⁠⁠<sup><a data-type="noteref" id="idm46324234934272-marker" href="preface01.xhtml#idm46324234934272">2</a></sup> and most researchers had abandoned the idea in the late 1990s. This paper revived the interest of the scientific community, and before long many new papers demonstrated that Deep Learning was not only possible, but capable of mind-blowing achievements that no other Machine Learning (ML)<a data-type="indexterm" data-primary="Machine Learning (ML)" data-secondary="history of" id="idm46324234933488"/> technique could hope to match (with the help of tremendous computing power and great amounts of data). This enthusiasm soon extended to many other areas of Machine Learning.</p>

<p>A decade later, Machine Learning had conquered the industry, and today it is at the heart of much of the magic in high-tech products, ranking your web search results, powering your smartphone’s speech recognition, recommending videos, and beating the world champion at the game of Go. Before you know it, it will be driving your car.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Machine Learning in Your Projects"><div class="sect1" id="idm46324234937056">
<h1>Machine Learning in Your Projects</h1>

<p>So, naturally<a data-type="indexterm" data-primary="Machine Learning (ML)" data-secondary="applications for" id="idm46324234910016"/> you are excited about Machine Learning and would love to join the party!</p>

<p>Perhaps you would like to give your homemade robot a brain of its own? Make it recognize faces? Or learn to walk around?</p>

<p>Or maybe your company has tons of data (user logs, financial data, production data, machine sensor data, hotline stats, HR reports, etc.), and more than likely you could unearth some hidden gems if you just knew where to look. With Machine Learning, you could accomplish the following <a href="https://homl.info/usecases">and much more</a>:</p>

<ul>
<li>
<p>Segment customers and find the best marketing strategy for each group.</p>
</li>
<li>
<p>Recommend products for each client based on what similar clients bought.</p>
</li>
<li>
<p>Detect which transactions are likely to be fraudulent.</p>
</li>
<li>
<p>Forecast next year’s revenue.</p>
</li>
</ul>

<p>Whatever the reason, you have decided to learn Machine Learning and implement it in your projects. Great idea!</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Objective and Approach"><div class="sect1" id="idm46324234915856">
<h1>Objective and Approach</h1>

<p>This<a data-type="indexterm" data-primary="Machine Learning (ML)" data-secondary="approach to learning" id="idm46324234443520"/> book assumes that you know close to nothing about Machine Learning. Its goal is to give you the concepts, tools, and intuition you need to implement programs capable of <em>learning from data</em>.</p>

<p>We will cover a large number of techniques, from the simplest and most commonly used (such as Linear Regression) to some of the Deep Learning techniques that regularly win competitions.</p>

<p>Rather than implementing our own toy versions of each algorithm, we will be using production-ready Python frameworks:</p>

<ul>
<li>
<p><a href="https://scikit-learn.org/">Scikit-Learn</a> is<a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="benefits of" id="idm46324234684272"/> very easy to use, yet it implements many Machine Learning algorithms efficiently, so it makes for a great entry point to learning Machine Learning. It was created by David Cournapeau in 2007, and is now led by a team of researchers at the French Institute for Research in Computer Science and Automation (Inria).</p>
</li>
<li>
<p><a href="https://tensorflow.org/">TensorFlow</a> is<a data-type="indexterm" data-primary="TensorFlow, basics of" data-secondary="benefits" id="idm46324234925600"/> a more complex library for distributed numerical computation. It makes it possible to train and run very large neural networks efficiently by distributing the computations across potentially hundreds of multi-GPU (graphics processing unit) servers. TensorFlow (TF) was created at Google and supports many of its large-scale Machine Learning applications. It was open sourced in November 2015, and version 2.0 was released in September 2019.</p>
</li>
<li>
<p><a href="https://keras.io/">Keras</a> is<a data-type="indexterm" data-primary="Keras" data-secondary="benefits of" id="idm46324234962672"/> a high-level Deep Learning API that makes it very simple to train and run neural networks. Keras comes bundled with TensorFlow, and it relies on TensorFlow for all the intensive computations.</p>
</li>
</ul>

<p>The book favors a hands-on approach, growing an intuitive understanding of Machine Learning through concrete working examples and just a little bit of theory.</p>
<div data-type="tip"><h6>Tip</h6>
<p>While you can read this book without picking up your laptop, I highly recommend you experiment with the code examples available online. See the <em>Code examples</em> section later in this preface.</p>
</div>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Prerequisites"><div class="sect1" id="idm46324234685488">
<h1>Prerequisites</h1>

<p>This<a data-type="indexterm" data-primary="Machine Learning (ML)" data-secondary="prerequisites to learning" id="idm46324234945312"/> book assumes that you have some Python programming experience. If you don’t know Python yet, <a href="https://learnpython.org/"><em class="hyperlink">https://learnpython.org/</em></a> is a great place to start. The official tutorial on <a href="https://docs.python.org/3/tutorial/">Python.org</a> is also quite good.</p>

<p>This book also assumes that you are familiar with Python’s main scientific libraries—in particular, <a href="https://numpy.org/">NumPy</a>, <a href="https://pandas.pydata.org/">Pandas</a>, and <a href="https://matplotlib.org/">Matplotlib</a>. If you have never used these libraries, don’t worry, they’re easy to learn, and I created a tutorial for each of them. You can access them online at <a href="https://homl.info/tutorials"><em class="hyperlink">https://homl.info/tutorials</em></a>.</p>

<p>Moreover, if you want to fully understand how the Machine Learning algorithms work (not just how to use them), then you should have at least a basic understanding of a few math concepts, especially linear algebra. Specifically, you should know what vectors and matrices are, and how to perform some simple operations like adding vectors, or transposing and multiplying matrices. If you need a quick introduction to linear algebra (it’s really not rocket science!), I created a tutorial available at <a href="https://homl.info/tutorials"><em class="hyperlink">https://homl.info/tutorials</em></a>. You will also find a tutorial on differential calculus, which may be helpful to understand how neural networks are trained, but it’s not entirely essential to grasp the important concepts. This book also uses other mathematical concepts occasionally, such as exponentials and logarithms, a bit of probability theory and some basic statistics concepts, but nothing too advanced. If you need help on any of these, please check out <a href="https://khanacademy.org/"><em class="hyperlink">https://khanacademy.org/</em></a>, which offers many excellent and free math courses online.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Roadmap"><div class="sect1" id="idm46324226933392">
<h1>Roadmap</h1>

<p>This<a data-type="indexterm" data-primary="Machine Learning (ML)" data-secondary="topics covered" id="idm46324234851504"/> book is organized in two parts. <a data-type="xref" data-xrefstyle="chap-num-title" href="part01.xhtml#fundamentals_part">Part I, “The Fundamentals of <span class="keep-together">Machine Learning</span>”</a>, covers the following topics:</p>

<ul>
<li>
<p>What Machine Learning is, what problems it tries to solve, and the main categories and fundamental concepts of its systems</p>
</li>
<li>
<p>The steps in a typical Machine Learning project</p>
</li>
<li>
<p>Learning by fitting a model to data</p>
</li>
<li>
<p>Optimizing a cost function</p>
</li>
<li>
<p>Handling, cleaning, and preparing data</p>
</li>
<li>
<p>Selecting and engineering features</p>
</li>
<li>
<p>Selecting a model and tuning hyperparameters using cross-validation</p>
</li>
<li>
<p>The challenges of Machine Learning, in particular underfitting and overfitting (the bias/variance trade-off)</p>
</li>
<li>
<p>The most common learning algorithms: Linear and Polynomial Regression, Logistic Regression, k-Nearest Neighbors, Support Vector Machines, Decision Trees, Random Forests, and Ensemble methods</p>
</li>
<li>
<p>Reducing the dimensionality of the training data to fight the “curse of dimensionality”</p>
</li>
<li>
<p>Other unsupervised learning techniques, including clustering, density estimation, and anomaly detection</p>
</li>
</ul>

<p><a data-type="xref" data-xrefstyle="chap-num-title" href="part02.xhtml#neural_nets_part">Part II, “Neural Networks and Deep Learning”</a>, covers the following topics:</p>

<ul>
<li>
<p>What neural nets are and what they’re good for</p>
</li>
<li>
<p>Building and training neural nets using TensorFlow and Keras</p>
</li>
<li>
<p>The most important neural net architectures: feedforward neural nets for tabular data, convolutional nets for computer vision, recurrent nets and long short-term memory (LSTM) nets for sequence processing, encoder/decoders and Transformers for natural language processing, autoencoders and generative adversarial networks (GANs) for generative learning</p>
</li>
<li>
<p>Techniques for training deep neural nets</p>
</li>
<li>
<p>How to build an agent (e.g., a bot in a game) that can learn good strategies through trial and error, using Reinforcement Learning</p>
</li>
<li>
<p>Loading and preprocessing large amounts of data efficiently</p>
</li>
<li>
<p>Training and deploying TensorFlow models at scale</p>
</li>
</ul>

<p>The first part is based mostly on Scikit-Learn, while the second part uses TensorFlow and Keras.</p>
<div data-type="caution"><h6>Caution</h6>
<p>Don’t jump into deep waters too hastily: while Deep Learning is no doubt one of the most exciting areas in Machine Learning, you should master the fundamentals first. Moreover, most problems can be solved quite well using simpler techniques such as Random Forests and Ensemble methods (discussed in <a data-type="xref" href="part01.xhtml#fundamentals_part">Part I</a>). Deep Learning is best suited for complex problems such as image recognition, speech recognition, or natural language processing, and it requires a lot of data, computing power, and patience (unless you can leverage a pre-trained neural network, as we will see).</p>
</div>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Changes in the Third Edition"><div class="sect1" id="idm46324234812112">
<h1>Changes in the Third Edition</h1>

<p>This third edition has four main objectives:</p>
<ol>
<li>
<p>Cover additional ML topics: (TODO write the list of new ML topics)</p>
</li>
<li>
<p>Discuss some of the latest important results from Deep Learning research.</p>
</li>
<li>
<p>Update the code examples to use the latest versions of Scikit-Learn, NumPy, Pandas, Matplotlib, and other libraries.</p>
</li>
<li>
<p>Clarify some sections and fix some errors, thanks to plenty of great feedback from readers.</p>
</li>

</ol>

<p>See <a href="https://homl.info/changes3"><em class="hyperlink">https://homl.info/changes3</em></a> for more details on what changed in the third edition. (TODO write changes page and link to it)</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Code Examples"><div class="sect1" id="idm46324228313856">
<h1>Code Examples</h1>

<p>All the code examples in this book are open source and available online at <a href="https://github.com/ageron/handson-ml3"><em class="hyperlink">https://github.com/ageron/handson-ml3</em></a>, as Jupyter notebooks. These are interactive documents containing text, images, and executable code snippets (Python in our case). The easiest and quickest way to get started is to run these notebooks using Google Colab: this is a free service that allows you to run any Jupyter notebook directly online, without having to install anything on your machine. All you need is a web browser and a Google account.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In this book, I will assume that you are using Google Colab, but I have also tested the notebooks on other online platforms such as Kaggle or Binder, so you can use those if you prefer. Alternatively, you can install the required libraries and tools (or the Docker image for this book) and run the notebooks directly on your own machine. See the instructions at <a href="https://github.com/ageron/handson-ml3"><em class="hyperlink">https://github.com/ageron/handson-ml3</em></a>.</p>
</div>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Other Resources"><div class="sect1" id="idm46324233602224">
<h1>Other Resources</h1>

<p>Many excellent resources<a data-type="indexterm" data-primary="Machine Learning (ML)" data-secondary="additional resources" id="idm46324233600640"/> are available to learn about Machine Learning. For example, Andrew Ng’s <a href="https://homl.info/ngcourse">ML course on Coursera</a> is amazing, although it requires a significant time investment.</p>

<p>There are also many interesting websites about Machine Learning, including of course Scikit-Learn’s exceptional <a href="https://homl.info/skdoc">User Guide</a>. You may also enjoy <a href="https://www.dataquest.io/">Dataquest</a>, which provides very nice interactive tutorials, and ML blogs such as those listed on <a href="https://homl.info/1">Quora</a>. Finally, there are many excellent YouTube channels about Machine Learning, including <a href="https://youtube.com/c/MachineLearningStreetTalk/">ML Street Talk</a>, <a href="https://youtube.com/c/YannicKilcher">Yannic Kilcher</a> and many others, some of which are listed at <a href="https://homl.info/youtubechannels"><em class="hyperlink">https://homl.info/youtubechannels</em></a>.</p>

<p>There are many other introductory books about Machine Learning. In particular:</p>

<ul>
<li>
<p>Joel Grus’s <a href="https://homl.info/grusbook" class="orm:hideurl"><em>Data Science from Scratch</em></a>, 2nd edition (O’Reilly) presents the fundamentals of Machine Learning and implements some of the main algorithms in pure Python (from scratch, as the name suggests).</p>
</li>
<li>
<p>Stephen Marsland’s <em>Machine Learning: An Algorithmic Perspective</em>, 2nd edition (Chapman &amp; Hall) is a great introduction to Machine Learning, covering a wide range of topics in depth with code examples in Python (also from scratch, but using NumPy).</p>
</li>
<li>
<p>Sebastian Raschka’s <em>Python Machine Learning</em>, 3rd edition (Packt Publishing) is also a great introduction to Machine Learning and leverages Python open source libraries (Pylearn 2 and Theano).</p>
</li>
<li>
<p>François Chollet’s <em>Deep Learning with Python</em>, 2nd edition (Manning) is a very practical book that covers a large range of topics in a clear and concise way, as you might expect from the author of the excellent Keras library. It favors code examples over mathematical theory.</p>
</li>
<li>
<p>Andriy Burkov’s <em>The Hundred-Page Machine Learning Book</em> is very short and covers an impressive range of topics, introducing them in approachable terms without shying away from the math equations.</p>
</li>
<li>
<p>Yaser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin’s <em>Learning from Data</em> (AMLBook) is a rather theoretical approach to ML that provides deep insights, in particular on the bias/variance trade-off (see <a data-type="xref" href="ch04.xhtml#linear_models_chapter">Chapter 4</a>).</p>
</li>
<li>
<p>Stuart Russell and Peter Norvig’s <em>Artificial Intelligence: A Modern Approach</em>, 4th Edition (Pearson), is a great (and huge) book covering an incredible amount of topics, including Machine Learning. It helps put ML into perspective.</p>
</li>
<li>
<p>Jeremy Howard and Sylvain Gugger’s <em>Deep Learning for Coders with fastai and PyTorch</em> (O’Reilly) provides a wonderfully clear and practical introduction to Deep Learning using the fastai and PyTorch libraries.</p>
</li>
</ul>

<p>Finally, joining ML competition websites such as <a href="https://www.kaggle.com/" class="orm:hideurl">Kaggle.com</a> will allow you to practice your skills on real-world problems, with help and insights from some of the best ML professionals out there.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Conventions Used in This Book"><div class="sect1" id="idm46324235123232">
<h1>Conventions Used in This Book</h1>

<p>The following typographical conventions are used in this book:</p>
<dl>
<dt><em>Italic</em></dt>
<dd>
<p>Indicates new terms, URLs, email addresses, filenames, and file extensions.</p>
</dd>
<dt><code>Constant width</code></dt>
<dd>
<p>Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements and keywords.</p>
</dd>
<dt><strong><code>Constant width bold</code></strong></dt>
<dd>
<p>Shows commands or other text that should be typed literally by the user.</p>
</dd>
<dt><em><code>Constant width italic</code></em></dt>
<dd>
<p>Shows text that should be replaced with user-supplied values or by values determined by context.</p>
</dd>
</dl>
<div data-type="tip"><h6>Tip</h6>
<p>This element signifies a tip or suggestion.</p>
</div>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>This element signifies a general note.</p>
</div>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>This element indicates a warning or caution.</p>
</div>
</div></section>













<section data-type="sect1" data-pdf-bookmark="O’Reilly Online Learning"><div class="sect1" id="idm46324233844512">
<h1>O’Reilly Online Learning</h1>
<div data-type="note" epub:type="note" class="ormenabled"><h6>Note</h6>
<p>For almost 40 years, <a href="https://oreilly.com" class="orm:hideurl"><em class="hyperlink">O’Reilly Media</em></a> has provided technology and business training, knowledge, and insight to help companies succeed.</p>
</div>

<p>Our unique network of experts and innovators share their knowledge and expertise through books, articles, and our online learning platform. O’Reilly’s online learning platform gives you on-demand access to live training courses, in-depth learning paths, interactive coding environments, and a vast collection of text and video from O’Reilly and 200+ other publishers. For more information, please visit <a href="https://oreilly.com" class="orm:hideurl"><em>https://oreilly.com</em></a>.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="How to Contact Us"><div class="sect1" id="idm46324228704768">
<h1>How to Contact Us</h1>

<p>Please address<a data-type="indexterm" data-primary="comments and questions" id="idm46324228703200"/><a data-type="indexterm" data-primary="questions and comments" id="idm46324228702464"/> comments and questions concerning this book to the publisher:</p>
<ul class="simplelist">
  <li>O’Reilly Media, Inc.</li>
  <li>1005 Gravenstein Highway North</li>
  <li>Sebastopol, CA 95472</li>
  <li>800-998-9938 (in the United States or Canada)</li>
  <li>707-829-0515 (international or local)</li>
  <li>707-829-0104 (fax)</li>
</ul>

<p>We have a web page for this book, where we list errata, examples, and any additional information. You can access this page at <a href="https://homl.info/oreilly2"><em class="hyperlink">https://homl.info/oreilly2</em></a>.</p>

<p>To comment or ask technical questions about this book, send email to <a class="email" href="mailto:bookquestions@oreilly.com"><em>bookquestions@oreilly.com</em></a>.</p>

<p>For news and more information about our books and courses, see our website at <a href="https://www.oreilly.com"><em class="hyperlink">https://www.oreilly.com</em></a>.</p>

<p>Find us on Facebook: <a href="https://facebook.com/oreilly"><em class="hyperlink">https://facebook.com/oreilly</em></a></p>

<p>Follow us on Twitter: <a href="https://twitter.com/oreillymedia"><em class="hyperlink">https://twitter.com/oreillymedia</em></a></p>

<p>Watch us on YouTube: <a href="https://www.youtube.com/oreillymedia"><em class="hyperlink">https://www.youtube.com/oreillymedia</em></a></p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Acknowledgments"><div class="sect1" id="idm46324234868816">
<h1>Acknowledgments</h1>

<p>Never in my wildest dreams did I imagine that the first and second editions of this book would get such a large audience. I received so many messages from readers, many asking questions, some kindly pointing out errata, and most sending me encouraging words. I cannot express how grateful I am to all these readers for their tremendous support. Thank you all so very much! Please do not hesitate to <a href="https://homl.info/issues3">file issues on GitHub</a> if you find errors in the code examples (or just to ask questions), or to submit <a href="https://homl.info/errata3">errata</a> (TODO define link) if you find errors in the text. Some readers also shared how this book helped them get their first job, or how it helped them solve a concrete problem they were working on. I find such feedback incredibly motivating. If you find this book helpful, I would love it if you could share your story with me, either privately (e.g., via <a href="https://www.linkedin.com/in/aurelien-geron/">LinkedIn</a>) or publicly (e.g., tweet me at @aureliengeron or write an <a href="https://homl.info/amazon3">Amazon review</a>). (TODO define link)</p>

<p>(TODO: add thanks to 3rd edition reviewers: Hannes Hapke, Sara Robinson, Laurence Moroney, Kyle Gallatin, Eric Lebigot, Jason Mayes, Soonson Kwon and Google for GCE credits)</p>

<p>(TODO: add thanks to Ulf Bissbort for ML discussions)</p>

<p>(TODO update this paragraph for 3rd edition) Many thanks as well to O’Reilly’s fantastic staff, in particular Nicole Taché, who gave me insightful feedback and was always cheerful, encouraging, and helpful: I could not dream of a better editor. Big thanks to Michele Cronin as well, who was very helpful (and patient) at the start of this second edition, and to Kristen Brown, the production editor for the second edition, who saw it through all the steps (she also coordinated fixes and updates for each reprint of the first edition). Thanks as well to Rachel Monaghan and Amanda Kersey for their thorough copyediting (respectively for the first and second edition), and to Johnny O’Toole who managed the relationship with Amazon and answered many of my questions. Thanks to Marie Beaugureau, Ben Lorica, Mike Loukides, and Laurel Ruma for believing in this project and helping me define its scope. Thanks to Matt Hacker and all of the Atlas team for answering all my technical questions regarding formatting, AsciiDoc, and LaTeX, and thanks to Nick Adams, Rebecca Demarest, Rachel Head, Judith McConville, Helen Monroe, Karen Montgomery, Rachel Roumeliotis, and everyone else at O’Reilly who contributed to this book.</p>

<p>I’ll never forget all the wonderful people who helped me with the first and second editions of this book: friends, colleagues, experts, including many members of the TensorFlow team, the list is long: Olzhas Akpambetov, Karmel Allison, Martin Andrews, David Andrzejewski, Paige Bailey, Lukas Biewald, Eugene Brevdo, William Chargin, François Chollet, Robert Crowe, Mark Daoust, Daniel “Wolff” Dobson, Nick Felt, Bruce Fontaine, Justin Francis, Goldie Gadde, Irene Giannoumis, Ingrid von Glehn, Vincent Guilbeau, Sandeep Gupta, Priya Gupta, Kevin Haas, Eddy Hung, Konstantinos Katsiapis, Viacheslav Kovalevskyi, Jon Krohn, Allen Lavoie, Karim Matrah, Grégoire Mesnil, Clemens Mewald, Dan Moldovan, Dominic Monn, Sean Morgan, Tom O’Malley, Haesun Park, Alexandre Passos, Ankur Patel, Josh Patterson, André Susano Pinto, Anthony Platanios, Oscar Ramirez, Anna Revinskaya, Saurabh Saxena, Salim Sémaoune, Ryan Sepassi, Jiri Simsa, Iain Smears, Xiaodan Song, Christina Sorokin, Michel Tessier, Dustin Tran, Todd Wang, Pete Warden, Martin Wicke, Edd Wilder-James, Sam Witteveen, Jason Zaman, Yuefeng Zhou and my dear brother Sylvain.</p>

<p>I would also like to thank my former Google colleagues, in particular the YouTube video classification team, for teaching me so much about Machine Learning. I could never have started the first edition without them. Special thanks to my personal ML gurus: Clément Courbet, Julien Dubois, Mathias Kende, Daniel Kitachewsky, James Pack, Alexander Pak, Anosh Raj, Vitor Sessak, Wiktor Tomczak, Ingrid von Glehn, and Rich Washington. And thanks to everyone else I worked with at YouTube and in the amazing Google research teams in Mountain View.</p>

<p>Last but not least, I am infinitely grateful to my beloved wife, Emmanuelle, and to our three wonderful children, Alexandre, Rémi, and Gabrielle, for encouraging me to work hard on this book. I’m also thankful to them for their insatiable curiosity: explaining some of the most difficult concepts in this book to my wife and children helped me clarify my thoughts and directly improved many parts of it. And they keep bringing me cookies and coffee, who could ask for anything more?</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm46324235039456"><sup><a href="preface01.xhtml#idm46324235039456-marker">1</a></sup> Geoffrey E. Hinton et al., “A Fast Learning Algorithm for Deep Belief Nets,” <em>Neural Computation</em> 18 (2006): 1527–1554.</p><p data-type="footnote" id="idm46324234934272"><sup><a href="preface01.xhtml#idm46324234934272-marker">2</a></sup> Despite the fact that Yann LeCun’s deep convolutional neural networks had worked well for image recognition since the 1990s, although they were not as general-purpose.</p></div></div></section></div></body>
</html>
<!DOCTYPE html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.w3.org/2002/06/xhtml2/ http://www.w3.org/MarkUp/SCHEMA/xhtml2.xsd" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<link href="Styles/Style00.css" rel="stylesheet" type="text/css" />
<link href="Styles/Style01.css" rel="stylesheet" type="text/css" />

<style type="text/css">body{margin:1em;background-color:transparent!important;}#sbo-rt-content *{text-indent:0pt!important;}#sbo-rt-content .bq{margin-right:1em!important;}#sbo-rt-content *{word-wrap:break-word!important;word-break:break-word!important;}#sbo-rt-content table,#sbo-rt-content pre{overflow-x:unset!important;overflow:unset!important;overflow-y:unset!important;white-space:pre-wrap!important;}</style></head>
<body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 5. Support Vector Machines"><div class="chapter" id="svm_chapter">
<h1><span class="label">Chapter 5. </span>Support Vector Machines</h1>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46324204246960">
<h5>A Note for Early Release Readers</h5>
<p>With Early Release ebooks, you get books in their earliest form—the author’s raw and unedited content as they write—so you can take advantage of these technologies long before the official release of these titles.</p>

<p>This will be the 5th chapter of the final book. Notebooks are available on GitHub at <a href="https://github.com/ageron/handson-ml3"><em class="hyperlink">https://github.com/ageron/handson-ml3</em></a>. Datasets are available at <a href="https://github.com/ageron/data"><em class="hyperlink">https://github.com/ageron/data</em></a>.</p>

<p>If you have comments about how we might improve the content and/or examples in this book, or if you notice missing material within this chapter, please reach out to the editor at <a href="mailto:mcronin@oreilly.com">mcronin@oreilly.com</a>.</p>
</div></aside>

<p>A <em>Support Vector Machine</em> (SVM) is<a data-type="indexterm" data-primary="Support Vector Machines (SVMs)" data-secondary="benefits of" id="idm46324204240464"/> a powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even novelty detection. SVMs shine with small to medium-sized nonlinear datasets (i.e., hundreds to thousands of instances), especially for classification tasks. However, they don’t scale very well to very large datasets, as we will see.</p>

<p>This chapter will explain the core concepts of SVMs, how to use them, and how they work. Let’s jump right in!</p>






<section data-type="sect1" data-pdf-bookmark="Linear SVM Classification"><div class="sect1" id="idm46324204238720">
<h1>Linear SVM Classification</h1>

<p>The<a data-type="indexterm" data-primary="Support Vector Machines (SVMs)" data-secondary="linear SVM classification" id="idm46324204237024"/><a data-type="indexterm" data-primary="linear SVM classification" id="idm46324204235968"/><a data-type="indexterm" data-primary="classification problems" data-secondary="linear SVM classification" id="idm46324204235296"/> fundamental idea behind SVMs is best explained with some pictures. <a data-type="xref" href="#large_margin_classification_plot">Figure 5-1</a> shows part of the iris dataset that was introduced at the end of <a data-type="xref" href="ch04.xhtml#linear_models_chapter">Chapter 4</a>. The two classes can clearly be separated easily with a straight line (they are <em>linearly separable</em>). The left plot shows the decision boundaries of three possible linear classifiers. The model whose decision boundary is represented by the dashed line is so bad that it does not even separate the classes properly. The other two models work perfectly on this training set, but their decision boundaries come so close to the instances that these models will probably not perform as well on new instances. In contrast, the solid line in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the two classes but also stays as far away from the closest training instances as possible. You can think of an SVM classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes. This<a data-type="indexterm" data-primary="large margin classification" id="idm46324204232208"/><a data-type="indexterm" data-primary="classification problems" data-secondary="large margin classification" id="idm46324204231488"/> is called <em>large margin classification</em>.</p>

<figure><div id="large_margin_classification_plot" class="figure">
<img src="Images/mls3_0501.png" alt="mls3 0501" width="2912" height="646"/>
<h6><span class="label">Figure 5-1. </span>Large margin classification</h6>
</div></figure>

<p>Notice<a data-type="indexterm" data-primary="support vectors" id="idm46324204227952"/> that adding more training instances “off the street” will not affect the decision boundary at all: it is fully determined (or “supported”) by the instances located on the edge of the street. These instances are called the <em>support vectors</em> (they are circled in <a data-type="xref" href="#large_margin_classification_plot">Figure 5-1</a>).</p>

<figure><div id="sensitivity_to_feature_scales_plot" class="figure">
<img src="Images/mls3_0502.png" alt="mls3 0502" width="2625" height="698"/>
<h6><span class="label">Figure 5-2. </span>Sensitivity to feature scales</h6>
</div></figure>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>SVMs are sensitive to the feature scales, as you can see in <a data-type="xref" href="#sensitivity_to_feature_scales_plot">Figure 5-2</a>: in the left plot, the vertical scale is much larger than the horizontal scale, so the widest possible street is close to horizontal. After<a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="feature scaling" id="idm46324204221728"/> feature scaling (e.g., using Scikit-Learn’s <code>StandardScaler</code>), the decision boundary in the right plot looks much better.</p>
</div>








<section data-type="sect2" data-pdf-bookmark="Soft Margin Classification"><div class="sect2" id="idm46324204220016">
<h2>Soft Margin Classification</h2>

<p>If<a data-type="indexterm" data-primary="soft margin classification" id="idm46324204218160"/><a data-type="indexterm" data-primary="classification problems" data-secondary="soft margin classification" id="idm46324204217392"/> we strictly impose that all instances must be off the street and on the correct side, this<a data-type="indexterm" data-primary="hard margin classification" id="idm46324204216208"/><a data-type="indexterm" data-primary="classification problems" data-secondary="hard margin classification" id="idm46324204215520"/> is called <em>hard margin classification</em>. There are two main issues with hard margin classification. First, it only works if the data is linearly separable. Second, it is sensitive to outliers. <a data-type="xref" href="#sensitivity_to_outliers_plot">Figure 5-3</a> shows the iris dataset with just one additional outlier: on the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the one we saw in <a data-type="xref" href="#large_margin_classification_plot">Figure 5-1</a> without the outlier, and it will probably not generalize as well.</p>

<figure><div id="sensitivity_to_outliers_plot" class="figure">
<img src="Images/mls3_0503.png" alt="mls3 0503" width="2912" height="716"/>
<h6><span class="label">Figure 5-3. </span>Hard margin sensitivity to outliers</h6>
</div></figure>

<p>To<a data-type="indexterm" data-primary="margin violations" id="idm46324204209968"/> avoid these issues, use a more flexible model. The objective is to find a good balance between keeping the street as large as possible and limiting the <em>margin violations</em> (i.e., instances that end up in the middle of the street or even on the wrong side). This is called <em>soft margin classification</em>.</p>

<p>When<a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="SVM models" id="idm46324204207376"/> creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including a regularization hyperparameter called <code>C</code>. If you set it to a low value, then you end up with the model on the left of <a data-type="xref" href="#regularization_plot">Figure 5-4</a>. With a high value, you get the model on the right. As you can see, reducing <code>C</code> makes the street larger, but it also leads to more margin violations. In other words, reducing <code>C</code> results in more instances supporting the street, so there’s less risk of overfitting. But if you reduce it too much, then the model ends up underfitting, as seems to be the case here: the model with <code>C=100</code> looks like it will generalize better than the one with <code>C=1</code>.</p>

<figure><div id="regularization_plot" class="figure">
<img src="Images/mls3_0504.png" alt="mls3 0504" width="2912" height="715"/>
<h6><span class="label">Figure 5-4. </span>Large margin (left) versus fewer margin violations (right)</h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>
<p>If your SVM model is overfitting, you can try regularizing it by reducing <code>C</code>.</p>
</div>

<p>The following Scikit-Learn code loads the iris dataset, and trains a linear SVM classifier to detect <em>Iris virginica</em> flowers. The pipeline first scales the features, then uses a <code>LinearSVC</code> with <code>C=1</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_iris</code>
<code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">make_pipeline</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>
<code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">LinearSVC</code>

<code class="n">iris</code> <code class="o">=</code> <code class="n">load_iris</code><code class="p">(</code><code class="n">as_frame</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code><code class="p">[[</code><code class="s2">"petal length (cm)"</code><code class="p">,</code> <code class="s2">"petal width (cm)"</code><code class="p">]]</code><code class="o">.</code><code class="n">values</code>
<code class="n">y</code> <code class="o">=</code> <code class="p">(</code><code class="n">iris</code><code class="o">.</code><code class="n">target</code> <code class="o">==</code> <code class="mi">2</code><code class="p">)</code>  <code class="c1"># Iris virginica</code>

<code class="n">svm_clf</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">StandardScaler</code><code class="p">(),</code>
                        <code class="n">LinearSVC</code><code class="p">(</code><code class="n">C</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">))</code>
<code class="n">svm_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

<p>The resulting model is represented on the left in <a data-type="xref" href="#regularization_plot">Figure 5-4</a>.</p>

<p>Then, as usual, you can use the model to make predictions:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">X_new</code> <code class="o">=</code> <code class="p">[[</code><code class="mf">5.5</code><code class="p">,</code> <code class="mf">1.7</code><code class="p">],</code> <code class="p">[</code><code class="mf">5.0</code><code class="p">,</code> <code class="mf">1.5</code><code class="p">]]</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">svm_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code>
<code class="go">array([ True, False])</code></pre>

<p>The first plant is classified as an Iris virginia, while the second is not. Let’s look at the scores that the SVM used to make these predictions. These measure the signed distance between each instance and the decision boundary:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">svm_clf</code><code class="o">.</code><code class="n">decision_function</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code>
<code class="go">array([ 0.66163411, -0.22036063])</code></pre>

<p>Unlike the <code>LogisticRegression</code> classifier, <code>LinearSVC</code> does not have a <code>predict_proba()</code> method to estimate the class probabilities. That said, if you use the <code>SVC</code> class (discussed shortly) instead of <code>LinearSVC</code>, and if you set its <code>probability</code> to <code>True</code>, then the model will fit an extra model at the end of training to map the SVM decision function scores to estimated probabilities. Under the hood, this requires using 5-fold cross-validation to generate out-of-sample predictions for every instance in the training set, then training a <code>LogisticRegression</code> model, so it will slow down training considerably. After that, the <code>predict_proba()</code> and <code>predict_log_proba()</code> methods will be available.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Nonlinear SVM Classification"><div class="sect1" id="idm46324204238384">
<h1>Nonlinear SVM Classification</h1>

<p>Although<a data-type="indexterm" data-primary="Support Vector Machines (SVMs)" data-secondary="nonlinear SVM classification" id="SVMnonlinear05"/><a data-type="indexterm" data-primary="nonlinear SVM classification" id="nonlinSVM05"/><a data-type="indexterm" data-primary="classification problems" data-secondary="nonlinear SVM classification" id="CPnonlinear05"/> linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to being linearly separable. One approach to handling nonlinear datasets is to add more features, such as polynomial features (as you did in <a data-type="xref" href="ch04.xhtml#linear_models_chapter">Chapter 4</a>); in some cases this can result in a linearly separable dataset. Consider the left plot in <a data-type="xref" href="#higher_dimensions_plot">Figure 5-5</a>: it represents a simple dataset with just one feature, <em>x</em><sub>1</sub>. This dataset is not linearly separable, as you can see. But if you add a second feature <em>x</em><sub>2</sub> = (<em>x</em><sub>1</sub>)<sup>2</sup>, the resulting 2D dataset is perfectly linearly separable.</p>

<figure><div id="higher_dimensions_plot" class="figure">
<img src="Images/mls3_0505.png" alt="mls3 0505" width="2627" height="794"/>
<h6><span class="label">Figure 5-5. </span>Adding features to make a dataset linearly separable</h6>
</div></figure>

<p>To implement this idea using Scikit-Learn, create a pipeline containing a <code>PolynomialFeatures</code> transformer (discussed in <a data-type="xref" href="ch04.xhtml#polynomial_regression">“Polynomial Regression”</a>), followed by a <code>StandardScaler</code> and a <code>LinearSVC</code>. Let’s test this on the moons dataset: this is a toy dataset for binary classification in which the data points are shaped as two interleaving crescent moons (see <a data-type="xref" href="#moons_polynomial_svc_plot">Figure 5-6</a>). You can generate this dataset using the <code>make_moons()</code> function:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_moons</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">PolynomialFeatures</code>

<code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">make_moons</code><code class="p">(</code><code class="n">n_samples</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">noise</code><code class="o">=</code><code class="mf">0.15</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>

<code class="n">polynomial_svm_clf</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code>
    <code class="n">PolynomialFeatures</code><code class="p">(</code><code class="n">degree</code><code class="o">=</code><code class="mi">3</code><code class="p">),</code>
    <code class="n">StandardScaler</code><code class="p">(),</code>
    <code class="n">LinearSVC</code><code class="p">(</code><code class="n">C</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">max_iter</code><code class="o">=</code><code class="mi">10</code><code class="n">_000</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="p">)</code>
<code class="n">polynomial_svm_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

<figure class="smallerfifty"><div id="moons_polynomial_svc_plot" class="figure">
<img src="Images/mls3_0506.png" alt="mls3 0506" width="1703" height="1099"/>
<h6><span class="label">Figure 5-6. </span>Linear SVM classifier using polynomial features</h6>
</div></figure>








<section data-type="sect2" data-pdf-bookmark="Polynomial Kernel"><div class="sect2" id="idm46324203886784">
<h2>Polynomial Kernel</h2>

<p>Adding<a data-type="indexterm" data-primary="polynomial features" id="idm46324203885344"/> polynomial features is simple to implement and can work great with all sorts of Machine Learning algorithms (not just SVMs). That said, at a low polynomial degree, this method cannot deal with very complex datasets, and with a high polynomial degree it creates a huge number of features, making the model too slow.</p>

<p>Fortunately, when<a data-type="indexterm" data-primary="kernel trick" id="idm46324203884096"/> using SVMs you can apply an almost miraculous mathematical technique called the <em>kernel trick</em> (which is explained later in this chapter). The kernel trick makes it possible to get the same result as if you had added many polynomial features, even with a very high-degree, without actually having to actually add them. This means there’s no combinatorial explosion of the number of features. This trick is implemented by the <code>SVC</code> class. Let’s test it on the moons dataset:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">SVC</code>

<code class="n">poly_kernel_svm_clf</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">StandardScaler</code><code class="p">(),</code>
                                    <code class="n">SVC</code><code class="p">(</code><code class="n">kernel</code><code class="o">=</code><code class="s2">"poly"</code><code class="p">,</code> <code class="n">degree</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">coef0</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">C</code><code class="o">=</code><code class="mi">5</code><code class="p">))</code>
<code class="n">poly_kernel_svm_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

<p>This code trains an SVM classifier using a third-degree polynomial kernel. It is represented on the left in <a data-type="xref" href="#moons_kernelized_polynomial_svc_plot">Figure 5-7</a>. On the right is another SVM classifier using a 10th-degree polynomial kernel. Obviously, if your model is overfitting, you might want to reduce the polynomial degree. Conversely, if it is underfitting, you can try increasing it. The hyperparameter <code>coef0</code> controls how much the model is influenced by high-degree terms versus low-degree terms.</p>

<figure><div id="moons_kernelized_polynomial_svc_plot" class="figure">
<img src="Images/mls3_0507.png" alt="mls3 0507" width="3059" height="1093"/>
<h6><span class="label">Figure 5-7. </span>SVM classifiers with a polynomial kernel</h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>
<p>Although you will typically tune hyperparameters automatically (e.g., using randomized search), it’s good to have a sense of what each hyperparameter actually does, and how it may interact with other hyperparameters: this way, you can narrow the search to a much smaller space.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Similarity Features"><div class="sect2" id="idm46324203854320">
<h2>Similarity Features</h2>

<p>Another technique<a data-type="indexterm" data-primary="similarity functions" id="idm46324203852240"/><a data-type="indexterm" data-primary="landmarks" id="idm46324203851504"/> to tackle nonlinear problems is to add features computed using a similarity function, which measures how much each instance resembles a particular <em>landmark</em>, as we did in <a data-type="xref" href="ch02.xhtml#project_chapter">Chapter 2</a> when we added the geographic similarity features. For example, let’s take the 1D dataset discussed earlier and add two landmarks to it at <em>x</em><sub>1</sub> = –2 and <em>x</em><sub>1</sub> = 1 (see the left plot in <a data-type="xref" href="#kernel_method_plot">Figure 5-8</a>). Next, let’s define<a data-type="indexterm" data-primary="Gaussian Radial Basis Function (RBF)" id="idm46324203846896"/><a data-type="indexterm" data-primary="Radial Basis Function (RBF)" id="idm46324203846224"/> the similarity function to be the Gaussian RBF with <em>γ</em> = 0.3. This is a bell-shaped function varying from 0 (very far away from the landmark) to 1 (at the landmark).</p>

<p>Now we are ready to compute the new features. For example, let’s look at the instance <em>x</em><sub>1</sub> = –1: it is located at a distance of 1 from the first landmark and 2 from the second landmark. Therefore its new features are <em>x</em><sub>2</sub> = exp(–0.3 × 1<sup>2</sup>) ≈ 0.74 and <em>x</em><sub>3</sub> = exp(–0.3 × 2<sup>2</sup>) ≈ 0.30. The plot on the right in <a data-type="xref" href="#kernel_method_plot">Figure 5-8</a> shows the transformed dataset (dropping the original features). As you can see, it is now linearly <span class="keep-together">separable</span>.</p>

<figure><div id="kernel_method_plot" class="figure">
<img src="Images/mls3_0508.png" alt="mls3 0508" width="3058" height="1104"/>
<h6><span class="label">Figure 5-8. </span>Similarity features using the Gaussian RBF</h6>
</div></figure>

<p>You may wonder how to select the landmarks. The simplest approach is to create a landmark at the location of each and every instance in the dataset. Doing that creates many dimensions and thus increases the chances that the transformed training set will be linearly separable. The downside is that a training set with <em>m</em> instances and <em>n</em> features gets transformed into a training set with <em>m</em> instances and <em>m</em> features (assuming you drop the original features). If your training set is very large, you end up with an equally large number of features.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Gaussian RBF Kernel"><div class="sect2" id="idm46324203793408">
<h2>Gaussian RBF Kernel</h2>

<p>Just like the polynomial features method, the similarity features method can be useful with any Machine Learning algorithm, but it may be computationally expensive to compute all the additional features, especially on large training sets. Once again the kernel trick does its SVM magic, making it possible to obtain a similar result as if you had added many similarity features, but without actually doing so. Let’s try the <code>SVC</code> class with the Gaussian RBF <span class="keep-together">kernel</span>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">rbf_kernel_svm_clf</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">StandardScaler</code><code class="p">(),</code>
                                   <code class="n">SVC</code><code class="p">(</code><code class="n">kernel</code><code class="o">=</code><code class="s2">"rbf"</code><code class="p">,</code> <code class="n">gamma</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">C</code><code class="o">=</code><code class="mf">0.001</code><code class="p">))</code>
<code class="n">rbf_kernel_svm_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

<p>This model is represented at the bottom left in <a data-type="xref" href="#moons_rbf_svc_plot">Figure 5-9</a>. The other plots show models trained with different values of hyperparameters <code>gamma</code> (<em>γ</em>) and <code>C</code>. Increasing <code>gamma</code> makes the bell-shaped curve narrower (see the lefthand plots in <a data-type="xref" href="#kernel_method_plot">Figure 5-8</a>). As a result, each instance’s range of influence is smaller: the decision boundary ends up being more irregular, wiggling around individual instances. Conversely, a small <code>gamma</code> value makes the bell-shaped curve wider: instances have a larger range of influence, and the decision boundary ends up smoother. So <em>γ</em> acts like a regularization <span class="keep-together">hyperparameter</span>: if your model is overfitting, you should reduce <em>γ</em>; if it is underfitting, you should increase <em>γ</em> (similar to the <code>C</code> hyperparameter).</p>

<figure><div id="moons_rbf_svc_plot" class="figure">
<img src="Images/mls3_0509.png" alt="mls3 0509" width="3059" height="1996"/>
<h6><span class="label">Figure 5-9. </span>SVM classifiers using an RBF kernel</h6>
</div></figure>

<p>Other kernels<a data-type="indexterm" data-primary="string kernels" id="idm46324203766448"/><a data-type="indexterm" data-primary="string subsequence kernel" id="idm46324203765712"/><a data-type="indexterm" data-primary="Levenshtein distance" id="idm46324203765072"/> exist but are used much more rarely. Some kernels are specialized for specific data structures. <em>String kernels</em> are sometimes used when classifying text documents or DNA sequences (e.g., using the <em>string subsequence kernel</em> or kernels based on the <em>Levenshtein distance</em>).</p>
<div data-type="tip"><h6>Tip</h6>
<p>With so many kernels to choose from, how can you decide which one to use? As a rule of thumb, you should always try the linear kernel first. The <code>LinearSVC</code> class is much faster than <code>SVC(kernel="linear")</code>, especially if the training set is very large. If it is not too large, you should also try kernelized SVMs, starting with the Gaussian RBF kernel; it often works really well. Then if you have spare time and computing power, you can experiment with a few other kernels, using hyperparameter search. If there are kernels specialized for your training set’s data structure, make sure to give them a try too.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="SVM Classes and Computational Complexity"><div class="sect2" id="idm46324203760816">
<h2>SVM Classes and Computational Complexity</h2>

<p>The <code>LinearSVC</code> class<a data-type="indexterm" data-primary="liblinear library" id="idm46324203758608"/> is based on the <code>liblinear</code> library, which implements an <a href="https://homl.info/13">optimized algorithm</a> for linear SVMs.⁠<sup><a data-type="noteref" id="idm46324203756640-marker" href="ch05.xhtml#idm46324203756640">1</a></sup> It does not support the kernel trick, but it scales almost linearly with the number of training instances and the number of features. Its training time complexity is roughly ݓm_ × <em>n</em>). The<a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="tolerance hyperparameter" id="idm46324203716032"/> algorithm takes longer if you require very high precision. This is controlled by the tolerance hyperparameter <em>ϵ</em> (called <code>tol</code> in Scikit-Learn). In most classification tasks, the default tolerance is fine.</p>

<p>The <code>SVC</code> class<a data-type="indexterm" data-primary="libsvm library" id="idm46324203713104"/> is based on the <code>libsvm</code> library, which implements <a href="https://homl.info/14">an algorithm</a> that supports the kernel trick.⁠<sup><a data-type="noteref" id="idm46324203711232-marker" href="ch05.xhtml#idm46324203711232">2</a></sup> The training time complexity is usually between ݓm_<sup>2</sup> × <em>n</em>) and ݓm_<sup>3</sup> × <em>n</em>). Unfortunately, this means that it gets dreadfully slow when the number of training instances gets large (e.g., hundreds of thousands of instances). This algorithm is best for small or medium-sized nonlinear training sets. It scales well with the number of features, especially with <em>sparse features</em> (i.e., when each instance has few nonzero features). In this case, the algorithm scales roughly with the average number of nonzero features per instance. <a data-type="xref" href="#svm_classification_algorithm_comparison">Table 5-1</a> compares Scikit-Learn’s SVM classification classes.<a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="SVM classification classes" id="idm46324203706480"/><a data-type="indexterm" data-primary="" data-startref="SVMnonlinear05" id="idm46324203705520"/><a data-type="indexterm" data-primary="" data-startref="nonlinSVM05" id="idm46324203704576"/><a data-type="indexterm" data-primary="" data-startref="CPnonlinear05" id="idm46324203703632"/></p>

<p>Lastly, the <code>SGDClassifier</code> class also performs large-margin classification by default, and its hyperparameters can be adjusted to produce similar results as the linear SVMs, especially the regularization hyperparameters (<code>alpha</code> and <code>penalty</code>) and the <code>learning_rate</code>. For training, it uses Stochastic Gradient Descent (see <a data-type="xref" href="ch04.xhtml#linear_models_chapter">Chapter 4</a>), which allows incremental learning and uses little memory. Moreover, it scales very well, as its computational complexity is ݓm_ × <em>n</em>).</p>
<table id="svm_classification_algorithm_comparison">
<caption><span class="label">Table 5-1. </span>Comparison of Scikit-Learn classes for SVM classification</caption>
<thead>
<tr>
<th>Class</th>
<th>Time Complexity</th>
<th>Out-of-core support</th>
<th>Scaling required</th>
<th>Kernel trick</th>
</tr>
</thead>
<tbody>
<tr>
<td><p><code>LinearSVC</code></p></td>
<td><p>ݓm_ × <em>n</em>)</p></td>
<td><p>No</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
</tr>
<tr>
<td><p><code>SVC</code></p></td>
<td><p>ݓ<em>m<em>² × _n</em>) to ݓ<em>m</em>³ × _n</em>)</p></td>
<td><p>No</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
</tr>
<tr>
<td><p><code>SGDClassifier</code></p></td>
<td><p>ݓm_ × <em>n</em>)</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
</tr>
</tbody>
</table>

<p>Now let’s see how the SVM algorithm can also be used for linear and nonlinear regression.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="SVM Regression"><div class="sect1" id="idm46324204049328">
<h1>SVM Regression</h1>

<p>To use SVMs for regression instead of classification, the trick is to tweak the objective: instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM Regression tries to fit as many instances as possible <em>on</em> the street while limiting margin violations (i.e., instances <em>off</em> the street). The width of the street is controlled by a hyperparameter, <em>ϵ</em>. <a data-type="xref" href="#svm_regression_plot">Figure 5-10</a> shows two linear SVM Regression models trained on some linear data, one with a small margin (<em>ϵ</em> = 0.5) and the other with a large margin (<em>ϵ</em> = 1.5).</p>

<figure><div id="svm_regression_plot" class="figure">
<img src="Images/mls3_0510.png" alt="mls3 0510" width="2617" height="1100"/>
<h6><span class="label">Figure 5-10. </span>SVM Regression</h6>
</div></figure>

<p>Reducing <em>ϵ</em> increases the number of support vectors, which regularizes the model. Moreover, if you add more training instances within the margin, it will not affect the model’s predictions; thus, the model is said to be <em>ϵ-insensitive</em>.</p>

<p>You can use Scikit-Learn’s <code>LinearSVR</code> class to perform linear SVM Regression. The following code produces the model represented on the left in <a data-type="xref" href="#svm_regression_plot">Figure 5-10</a>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">LinearSVR</code>

<code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># a linear dataset</code>
<code class="n">svm_reg</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">StandardScaler</code><code class="p">(),</code>
                        <code class="n">LinearSVR</code><code class="p">(</code><code class="n">epsilon</code><code class="o">=</code><code class="mf">0.5</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">))</code>
<code class="n">svm_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

<p>To tackle nonlinear regression tasks, you can use a kernelized SVM model. <a data-type="xref" href="#svm_with_polynomial_kernel_plot">Figure 5-11</a> shows SVM Regression on a random quadratic training set, using a second-degree polynomial kernel. There is some regularization in the left plot (i.e., a small <code>C</code> value), and much less in the right plot (i.e., a large <code>C</code> value).</p>

<figure><div id="svm_with_polynomial_kernel_plot" class="figure">
<img src="Images/mls3_0511.png" alt="mls3 0511" width="2616" height="1093"/>
<h6><span class="label">Figure 5-11. </span>SVM Regression using a second-degree polynomial kernel</h6>
</div></figure>

<p>The following code uses Scikit-Learn’s <code>SVR</code> class (which supports the kernel trick) to produce the model represented on the left in <a data-type="xref" href="#svm_with_polynomial_kernel_plot">Figure 5-11</a>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">SVR</code>

<code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># a quadratic dataset</code>
<code class="n">svm_poly_reg</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">StandardScaler</code><code class="p">(),</code>
                             <code class="n">SVR</code><code class="p">(</code><code class="n">kernel</code><code class="o">=</code><code class="s2">"poly"</code><code class="p">,</code> <code class="n">degree</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">C</code><code class="o">=</code><code class="mf">0.01</code><code class="p">,</code> <code class="n">epsilon</code><code class="o">=</code><code class="mf">0.1</code><code class="p">))</code>
<code class="n">svm_poly_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

<p>The <code>SVR</code> class is the regression equivalent of the <code>SVC</code> class, and the <code>LinearSVR</code> class is the regression equivalent of the <code>LinearSVC</code> class. The <code>LinearSVR</code> class scales linearly with the size of the training set (just like the <code>LinearSVC</code> class), while the <code>SVR</code> class gets much too slow when the training set grows very large (just like the <code>SVC</code> class).</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>SVMs can also be used for novelty detection, as we will see in <a data-type="xref" href="ch09.xhtml#unsupervised_learning_chapter">Chapter 9</a>.</p>
</div>

<p>The rest of this chapter explains how SVMs make predictions and how their training algorithms work, starting with linear SVM classifiers. If you are just getting started with Machine Learning, you can safely skip this and go straight to the exercises at the end of this chapter, and come back later when you want to get a deeper understanding of SVMs.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Under The Hood of Linear SVM Classifiers"><div class="sect1" id="idm46324203680272">
<h1>Under The Hood of Linear SVM Classifiers</h1>

<p>The<a data-type="indexterm" data-primary="Support Vector Machines (SVMs)" data-secondary="decision function and prediction" id="idm46324203584864"/> linear SVM classifier model predicts the class of a new instance <strong>x</strong> by first computing the decision function <strong>θ</strong><sup>⊺</sup> <strong>x</strong> = <em>θ</em><sub>0</sub> <em>x</em><sub>0</sub> + ⋯ + <em>θ</em><sub><em>n</em></sub> <em>x</em><sub><em>n</em></sub>, where <em>x</em><sub>0</sub> is the bias feature (always equal to 1). If the result is positive, then the predicted class <em>ŷ</em> is the positive class (1), otherwise it is the negative class (0). This is exactly like <code>LogisticRegression</code> (discussed in <a data-type="xref" href="ch04.xhtml#linear_models_chapter">Chapter 4</a>).</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Up to now, I have used the convention of putting all the model parameters in one vector <strong>θ</strong>, including the bias term <strong>θ</strong><sub>0</sub> and the input feature weights <strong>θ</strong><sub>1</sub> to <strong>θ</strong><sub><em>n</em></sub>. This required adding a bias input <em>x</em><sub>0</sub> = 1 to all instances. Another very common convention is to separate the bias term <em>b</em> (equal to <strong>θ</strong><sub>0</sub>), and the feature weights vector <strong>w</strong> (containing <strong>θ</strong><sub>1</sub> to <strong>θ</strong><sub><em>n</em></sub>). In this case, no bias feature needs to be added to the input feature vectors, and the linear SVM’s decision function is equal to <strong>w</strong><sup>⊺</sup> <strong>x</strong> + <em>b</em> = <em>w</em><sub>1</sub> <em>x</em><sub>1</sub> + ⋯ + <em>w</em><sub><em>n</em></sub> <em>x</em><sub><em>n</em></sub> + <em>b</em>. I will use this convention throughout the rest of this book.</p>
</div>

<p>So making predictions with a linear SVM classifier is quite straightforward. How about training? Well, it requires finding the weights vector <strong>w</strong> and the bias term <em>b</em> that make the “street” (i.e., the margin) as wide as possible while limiting the number of margin violations. Let’s start with the width of the street: to make it larger, we need to make <strong>w</strong> smaller. This may be easier to visualize in 2D, as shown in <a data-type="xref" href="#small_w_large_margin_plot">Figure 5-12</a>. Let’s define the borders of the street as the points where the decision function is equal to –1 or +1. On the left plot, the weight <em>w<sub>1</sub></em> is 1, so the points at which <em>w</em><sub>1</sub> <em>x</em><sub>1</sub> = –1 or +1 are <em>x</em><sub>1</sub> = –1 and +1: therefore the margin’s size is 2. On the right plot, the weight is 0.5, so the points at which <em>w</em><sub>1</sub> <em>x</em><sub>1</sub> = –1 or +1 are <em>x</em><sub>1</sub> = –2 and +2: the margin’s size is 4. So we need to keep <strong>w</strong> as small as possible. Note that the bias term <em>b</em> has no influence on the size of the margin: tweaking it just shifts the margin around, without affecting its size.</p>

<figure><div id="small_w_large_margin_plot" class="figure">
<img src="Images/mls3_0512.png" alt="mls3 0512" width="2564" height="856"/>
<h6><span class="label">Figure 5-12. </span>A smaller weight vector results in a larger margin</h6>
</div></figure>

<p>We also want to avoid margin violations, so we need the decision function to be greater than 1 for all positive training instances and lower than –1 for negative training instances. If we define <em>t</em><sup>(<em>i</em>)</sup> = –1 for negative instances (when <em>y</em><sup>(<em>i</em>)</sup> = 0) and <em>t</em><sup>(<em>i</em>)</sup> = 1 for positive instances (when <em>y</em><sup>(<em>i</em>)</sup> = 1), then we can write this constraint as <em>t</em><sup>(<em>i</em>)</sup>(<strong>w</strong><sup>⊺</sup> <strong>x</strong><sup>(<em>i</em>)</sup> + <em>b</em>) ≥ 1 for all instances.</p>

<p>We<a data-type="indexterm" data-primary="constrained optimization" id="idm46324203473024"/> can therefore express the hard margin linear SVM classifier objective as the constrained optimization problem in <a data-type="xref" href="#hard_margin_objective">Equation 5-1</a>.</p>
<div id="hard_margin_objective" data-type="equation"><h5><span class="label">Equation 5-1. </span>Hard margin linear SVM classifier objective</h5><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <munder><mo form="prefix">minimize</mo> <mrow><mi mathvariant="bold">w</mi><mo>,</mo><mi>b</mi></mrow></munder>
          <mspace width="1.em"/>
          <mrow>
            <mfrac><mn>1</mn> <mn>2</mn></mfrac>
            <msup><mi mathvariant="bold">w</mi> <mo>⊺</mo> </msup>
            <mi mathvariant="bold">w</mi>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <mtext>subject</mtext>
          <mspace width="4.pt"/>
          <mtext>to</mtext>
          <mspace width="1.em"/>
          <msup><mi>t</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
          <mrow>
            <mo>(</mo>
            <msup><mi mathvariant="bold">w</mi> <mo>⊺</mo> </msup>
            <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
            <mo>+</mo>
            <mi>b</mi>
            <mo>)</mo>
          </mrow>
          <mo>≥</mo>
          <mn>1</mn>
          <mspace width="1.em"/>
          <mtext>for</mtext>
          <mspace width="4.pt"/>
          <mi>i</mi>
          <mo>=</mo>
          <mn>1</mn>
          <mo>,</mo>
          <mn>2</mn>
          <mo>,</mo>
          <mo>⋯</mo>
          <mo>,</mo>
          <mi>m</mi>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
</div>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>We are minimizing ½ <strong>w</strong><sup>⊺</sup> <strong>w</strong>, which is equal to ½∥ <strong>w</strong> ∥<sup>2</sup>, rather than minimizing ∥ <strong>w</strong> ∥ (the norm of <strong>w</strong>). Indeed, ½∥ <strong>w</strong> ∥<sup>2</sup> has a nice, simple derivative (it is just <strong>w</strong>), while ∥ <strong>w</strong> ∥ is not differentiable at <strong>w</strong> = 0. Optimization algorithms often work much better on differentiable functions.</p>
</div>

<p>To get the soft margin objective, we need to<a data-type="indexterm" data-primary="slack variables" id="idm46324203434368"/> introduce a <em>slack variable</em> <em>ζ</em><sup>(<em>i</em>)</sup> ≥ 0 for each instance:⁠<sup><a data-type="noteref" id="idm46324203431584-marker" href="ch05.xhtml#idm46324203431584">3</a></sup> <em>ζ</em><sup>(<em>i</em>)</sup> measures how much the <em>i</em><sup>th</sup> instance is allowed to violate the margin. We now have two conflicting objectives: make the slack variables as small as possible to reduce the margin violations, and make ½ <strong>w</strong><sup>⊺</sup> <strong>w</strong> as small as possible to increase the margin. This is where the <code>C</code> hyperparameter comes in: it allows us to define the tradeoff between these two objectives. This gives us the constrained optimization problem in <a data-type="xref" href="#soft_margin_objective">Equation 5-2</a>.</p>
<div id="soft_margin_objective" data-type="equation"><h5><span class="label">Equation 5-2. </span>Soft margin linear SVM classifier objective</h5><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <munder><mo form="prefix">minimize</mo> <mrow><mi mathvariant="bold">w</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>ζ</mi></mrow></munder>
          <mspace width="1.em"/>
          <mrow>
            <mstyle scriptlevel="0" displaystyle="true">
              <mfrac><mn>1</mn> <mn>2</mn></mfrac>
            </mstyle>
            <msup><mi mathvariant="bold">w</mi> <mo>⊺</mo> </msup>
            <mi mathvariant="bold">w</mi>
            <mo>+</mo>
            <mi>C</mi>
            <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi> </munderover>
            <msup><mi>ζ</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <mtext>subject</mtext>
          <mspace width="4.pt"/>
          <mtext>to</mtext>
          <mspace width="1.em"/>
          <msup><mi>t</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
          <mrow>
            <mo>(</mo>
            <msup><mi mathvariant="bold">w</mi> <mo>⊺</mo> </msup>
            <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
            <mo>+</mo>
            <mi>b</mi>
            <mo>)</mo>
          </mrow>
          <mo>≥</mo>
          <mn>1</mn>
          <mo>-</mo>
          <msup><mi>ζ</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
          <mspace width="1.em"/>
          <mtext>and</mtext>
          <mspace width="1.em"/>
          <msup><mi>ζ</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
          <mo>≥</mo>
          <mn>0</mn>
          <mspace width="1.em"/>
          <mtext>for</mtext>
          <mspace width="4.pt"/>
          <mi>i</mi>
          <mo>=</mo>
          <mn>1</mn>
          <mo>,</mo>
          <mn>2</mn>
          <mo>,</mo>
          <mo>⋯</mo>
          <mo>,</mo>
          <mi>m</mi>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
</div>

<p>The hard margin and soft margin problems are both convex quadratic optimization problems with linear constraints. Such<a data-type="indexterm" data-primary="Quadratic Programming (QP) problems" id="idm46324203382592"/> problems are known as <em>Quadratic Programming</em> (QP) problems. Many off-the-shelf solvers are available to solve QP problems by using a variety of techniques that are outside the scope of this book.⁠<sup><a data-type="noteref" id="idm46324203381376-marker" href="ch05.xhtml#idm46324203381376">4</a></sup></p>

<p>So using a QP solver is one way to train an SVM. Another is to use Gradient Descent to minimize the <em>Hinge loss</em> or the <em>squared Hinge loss</em> (see <a data-type="xref" href="#hinge_plot">Figure 5-13</a>). Given an instance <strong>x</strong> of the positive class (i.e., with <em>t</em> = 1), the loss is zero if the output <em>s</em> of the decision function (<em>s</em> = <strong>w</strong><sup>⊺</sup> <strong>x</strong> + <em>b</em>) is greater or equal to 1. This happens when the instance is off the street and on the positive side. Given an instance of the negative class (i.e., with <em>t</em> = –1), the loss is zero if <em>s</em> ≤ –1. This happens when the instance is off the street and on the negative side. The further away an instance is from the correct side of the margin, the higher the loss: it grows linearly for the Hinge loss, and quadratically for the squared Hinge loss. This makes the squared Hinge loss more sensitive to outliers. However, if the dataset is clean, it tends to converge faster. By default, <code>LinearSVC</code> uses the squared Hinge loss, while <code>SGDClassifier</code> uses the Hinge loss. Both classes let you choose the loss by setting the <code>loss</code> hyperparameter to <code>"hinge"</code> or <code>"squared_hinge"</code>. The <code>SVC</code> class’s optimization algorithm finds a similar solution as minimizing the Hinge loss.</p>

<figure><div id="hinge_plot" class="figure">
<img src="Images/mls3_0513.png" alt="mls3 0513" width="2370" height="878"/>
<h6><span class="label">Figure 5-13. </span>The Hinge loss (left) and the Squared Hinge loss (right)</h6>
</div></figure>

<p>There’s yet another way to train a linear SVM classifier: solving the dual problem.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="The Dual Problem"><div class="sect1" id="idm46324203585936">
<h1>The Dual Problem</h1>

<p>Given<a data-type="indexterm" data-primary="Support Vector Machines (SVMs)" data-secondary="dual problem" id="idm46324203365712"/><a data-type="indexterm" data-primary="dual problem" id="idm46324203364640"/><a data-type="indexterm" data-primary="primal problem" id="idm46324203363968"/> a constrained optimization problem, known as the <em>primal problem</em>, it is possible to express a different but closely related problem, called its <em>dual problem</em>. The <span class="keep-together">solution</span> to the dual problem typically gives a lower bound to the solution of the primal problem, but under some conditions it can have the same solution as the primal problem. Luckily, the SVM problem happens to meet these conditions,⁠<sup><a data-type="noteref" id="idm46324203361584-marker" href="ch05.xhtml#idm46324203361584">5</a></sup> so you can choose to solve the primal problem or the dual problem; both will have the same solution. <a data-type="xref" href="#svm_dual_form">Equation 5-3</a> shows the dual form of the linear SVM objective. If you are interested in knowing how to derive the dual problem from the primal problem, see the extra material section in the notebook.</p>
<div id="svm_dual_form" data-type="equation"><h5><span class="label">Equation 5-3. </span>Dual form of the linear SVM objective</h5><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><munder><mtext>minimize </mtext><mi mathvariant="bold">α</mi></munder><mo> </mo><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mo> </mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mi>α</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><msup><mi>α</mi><mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup><msup><mi>t</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><msup><mi>t</mi><mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup><msup><msup><mi mathvariant="bold">x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>⊺</mo></msup><msup><mi mathvariant="bold">x</mi><mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup><mo>  </mo><mo>-</mo><mo>  </mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mi>α</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mspace linebreak="newline"/><mtext>subject to </mtext><msup><mi>α</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>≥</mo><mn>0</mn><mtext> for all </mtext><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>…</mo><mo>,</mo><mi>m</mi><mtext> and </mtext><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mi>α</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><msup><mi>t</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>=</mo><mn>0</mn></math>
</div>

<p>Once you find the vector 
<math xmlns="http://www.w3.org/1998/Math/MathML">
  <mover accent="true"><mi mathvariant="bold">α</mi> <mo>^</mo></mover>
</math> that minimizes this equation (using a QP solver), use <a data-type="xref" href="#from_alpha_to_w_and_b">Equation 5-4</a> to compute <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mover accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover>
</math> and <math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>b</mi><mo>^</mo></mover></math> that minimize the primal problem.</p>
<div class="fifty-percent" id="from_alpha_to_w_and_b" data-type="equation"><h5><span class="label">Equation 5-4. </span>From the dual solution to the primal solution</h5><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <mover accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover>
          <mo>=</mo>
          <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi> </munderover>
          <msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
          <msup><mi>t</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
          <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <mover accent="true"><mi>b</mi> <mo>^</mo></mover>
          <mo>=</mo>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mn>1</mn> <msub><mi>n</mi> <mi>s</mi> </msub></mfrac>
          </mstyle>
          <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mstyle scriptlevel="1" displaystyle="false"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mstyle> <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>&gt;</mo><mn>0</mn></mrow></mstyle></mfrac> <mi>m</mi> </munderover>
          <mfenced separators="" open="(" close=")">
            <msup><mi>t</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
            <mo>-</mo>
            <mrow>
              <msup><mrow><mover accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover></mrow> <mo>⊺</mo> </msup>
              <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
            </mrow>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
  </math>
  </div>

<p>Where <em>n</em><sub><em>s</em></sub> is the number of support vectors.</p>

<p>The dual problem is faster to solve than the primal one when the number of training instances is smaller than the number of features. More importantly, the dual problem makes the kernel trick possible, while the primal does not. So what is this kernel trick, anyway?</p>








<section data-type="sect2" data-pdf-bookmark="Kernelized SVMs"><div class="sect2" id="idm46324203279936">
<h2>Kernelized SVMs</h2>

<p>Suppose<a data-type="indexterm" data-primary="Support Vector Machines (SVMs)" data-secondary="kernelized SVM" id="idm46324203278272"/><a data-type="indexterm" data-primary="kernelized SVM" id="idm46324203277296"/> you want to apply a second-degree polynomial transformation to a two-dimensional training set (such as the moons training set), then train a linear SVM classifier on the transformed training set. <a data-type="xref" href="#example_second_degree_polynomial_mapping">Equation 5-5</a> shows the second-degree polynomial mapping function <em>ϕ</em> that you want to apply.</p>
<div class="fifty-percent" id="example_second_degree_polynomial_mapping" data-type="equation"><h5><span class="label">Equation 5-5. </span>Second-degree polynomial mapping</h5><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
    <mi>ϕ</mi>
    <mfenced open="(" close=")">
      <mi mathvariant="bold">x</mi>
    </mfenced>
    <mo>=</mo>
    <mi>ϕ</mi>
    <mfenced open="(" close=")">
      <mfenced open="(" close=")">
        <mtable>
          <mtr>
            <mtd>
              <msub><mi>x</mi> <mn>1</mn> </msub>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
              <msub><mi>x</mi> <mn>2</mn> </msub>
            </mtd>
          </mtr>
        </mtable>
      </mfenced>
    </mfenced>
    <mo>=</mo>
    <mfenced open="(" close=")">
      <mtable>
        <mtr>
          <mtd>
            <msup><mrow><msub><mi>x</mi> <mn>1</mn> </msub></mrow> <mn>2</mn> </msup>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <msqrt>
                <mn>2</mn>
              </msqrt>
              <mspace width="0.166667em"/>
              <msub><mi>x</mi> <mn>1</mn> </msub>
              <msub><mi>x</mi> <mn>2</mn> </msub>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <msup><mrow><msub><mi>x</mi> <mn>2</mn> </msub></mrow> <mn>2</mn> </msup>
          </mtd>
        </mtr>
      </mtable>
    </mfenced>
  </mrow>
</math>
</div>

<p>Notice that the transformed vector is 3D instead of 2D. Now let’s look at what happens to a couple of 2D vectors, <strong>a</strong> and <strong>b</strong>, if we apply this second-degree polynomial mapping and then compute the dot product⁠<sup><a data-type="noteref" id="idm46324203248480-marker" href="ch05.xhtml#idm46324203248480">6</a></sup> of the transformed vectors (See <a data-type="xref" href="#kernel_trick_for_second_degree_polynomial_mapping">Equation 5-6</a>).</p>
<div id="kernel_trick_for_second_degree_polynomial_mapping" data-type="equation"><h5><span class="label">Equation 5-6. </span>Kernel trick for a second-degree polynomial mapping</h5>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mi>ϕ</mi>
          <msup><mrow><mo>(</mo><mi mathvariant="bold">a</mi><mo>)</mo></mrow> <mo>⊺</mo> </msup>
          <mi>ϕ</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">b</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mspace width="1.em"/>
          <mo>=</mo>
          <msup><mfenced open="(" close=")"><mtable><mtr><mtd><msup><mrow><msub><mi>a</mi> <mn>1</mn> </msub></mrow> <mn>2</mn> </msup></mtd></mtr><mtr><mtd><mrow><msqrt><mn>2</mn></msqrt><mspace width="0.166667em"/><msub><mi>a</mi> <mn>1</mn> </msub><msub><mi>a</mi> <mn>2</mn> </msub></mrow></mtd></mtr><mtr><mtd><msup><mrow><msub><mi>a</mi> <mn>2</mn> </msub></mrow> <mn>2</mn> </msup></mtd></mtr></mtable></mfenced> <mo>⊺</mo> </msup>
          <mfenced open="(" close=")">
            <mtable>
              <mtr>
                <mtd>
                  <msup><mrow><msub><mi>b</mi> <mn>1</mn> </msub></mrow> <mn>2</mn> </msup>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mrow>
                    <msqrt>
                      <mn>2</mn>
                    </msqrt>
                    <mspace width="0.166667em"/>
                    <msub><mi>b</mi> <mn>1</mn> </msub>
                    <msub><mi>b</mi> <mn>2</mn> </msub>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msup><mrow><msub><mi>b</mi> <mn>2</mn> </msub></mrow> <mn>2</mn> </msup>
                </mtd>
              </mtr>
            </mtable>
          </mfenced>
          <mo>=</mo>
          <msup><mrow><msub><mi>a</mi> <mn>1</mn> </msub></mrow> <mn>2</mn> </msup>
          <msup><mrow><msub><mi>b</mi> <mn>1</mn> </msub></mrow> <mn>2</mn> </msup>
          <mo>+</mo>
          <mn>2</mn>
          <msub><mi>a</mi> <mn>1</mn> </msub>
          <msub><mi>b</mi> <mn>1</mn> </msub>
          <msub><mi>a</mi> <mn>2</mn> </msub>
          <msub><mi>b</mi> <mn>2</mn> </msub>
          <mo>+</mo>
          <msup><mrow><msub><mi>a</mi> <mn>2</mn> </msub></mrow> <mn>2</mn> </msup>
          <msup><mrow><msub><mi>b</mi> <mn>2</mn> </msub></mrow> <mn>2</mn> </msup>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <mspace width="1.em"/>
          <mo>=</mo>
          <msup><mfenced separators="" open="(" close=")"><msub><mi>a</mi> <mn>1</mn> </msub><msub><mi>b</mi> <mn>1</mn> </msub><mo>+</mo><msub><mi>a</mi> <mn>2</mn> </msub><msub><mi>b</mi> <mn>2</mn> </msub></mfenced> <mn>2</mn> </msup>
          <mo>=</mo>
          <msup><mfenced separators="" open="(" close=")"><msup><mfenced open="(" close=")"><mtable><mtr><mtd><msub><mi>a</mi> <mn>1</mn> </msub></mtd></mtr><mtr><mtd><msub><mi>a</mi> <mn>2</mn> </msub></mtd></mtr></mtable></mfenced> <mo>⊺</mo> </msup><mfenced open="(" close=")"><mtable><mtr><mtd><msub><mi>b</mi> <mn>1</mn> </msub></mtd></mtr><mtr><mtd><msub><mi>b</mi> <mn>2</mn> </msub></mtd></mtr></mtable></mfenced></mfenced> <mn>2</mn> </msup>
          <mo>=</mo>
          <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">a</mi> <mo>⊺</mo> </msup><mi mathvariant="bold">b</mi><mo>)</mo></mrow> <mn>2</mn> </msup>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
</div>

<p>How about that? The dot product of the transformed vectors is equal to the square of the dot product of the original vectors: <em>ϕ</em>(<strong>a</strong>)<sup>⊺</sup> <em>ϕ</em>(<strong>b</strong>) = (<strong>a</strong><sup>⊺</sup> <strong>b</strong>)<sup>2</sup>.</p>

<p>Here is the key insight: if you apply the transformation <em>ϕ</em> to all training instances, then the dual problem (see <a data-type="xref" href="#svm_dual_form">Equation 5-3</a>) will contain the dot product <em>ϕ</em>(<strong>x</strong><sup>(<em>i</em>)</sup>)<sup>⊺</sup> <em>ϕ</em>(<strong>x</strong><sup>(<em>j</em>)</sup>). But if <em>ϕ</em> is the second-degree polynomial transformation defined in <a data-type="xref" href="#example_second_degree_polynomial_mapping">Equation 5-5</a>, then you can replace this dot product of transformed vectors simply by <math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup><mrow><mo>(</mo><msup><mrow><msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup></mrow> <mo>⊺</mo> </msup><msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow> </msup><mo>)</mo></mrow> <mn>2</mn> </msup>
</math>. So, you don’t need to transform the training instances at all; just replace the dot product by its square in <a data-type="xref" href="#svm_dual_form">Equation 5-3</a>. The result will be strictly the same as if you had gone through the trouble of transforming the training set then fitting a linear SVM algorithm, but this trick makes the whole process much more computationally efficient.</p>

<p>The function <em>K</em>(<strong>a</strong>, <strong>b</strong>) = (<strong>a</strong><sup>⊺</sup> <strong>b</strong>)<sup>2</sup> is a<a data-type="indexterm" data-primary="polynomial kernels" id="idm46324203148672"/><a data-type="indexterm" data-primary="kernels" id="idm46324203148064"/> second-degree polynomial kernel. In Machine Learning, a <em>kernel</em> is a function capable of computing the dot product <em>ϕ</em>(<strong>a</strong>)<sup>⊺</sup> <em>ϕ</em>(<strong>b</strong>), based only on the original vectors <strong>a</strong> and <strong>b</strong>, without having to compute (or even to know about) the transformation <em>ϕ</em>. <a data-type="xref" href="#common_kernels">Equation 5-7</a> lists some of the most commonly used kernels.</p>
<div id="common_kernels" data-type="equation"><h5><span class="label">Equation 5-7. </span>Common kernels</h5>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mtext>Linear:</mtext>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mspace width="1.em"/>
          <mi>K</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">a</mi>
            <mo>,</mo>
            <mi mathvariant="bold">b</mi>
            <mo>)</mo>
          </mrow>
          <mo>=</mo>
          <msup><mi mathvariant="bold">a</mi> <mo>⊺</mo> </msup>
          <mi mathvariant="bold">b</mi>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mtext>Polynomial:</mtext>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mspace width="1.em"/>
          <mi>K</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">a</mi>
            <mo>,</mo>
            <mi mathvariant="bold">b</mi>
            <mo>)</mo>
          </mrow>
          <mo>=</mo>
          <msup><mfenced separators="" open="(" close=")"><mi>γ</mi><msup><mi mathvariant="bold">a</mi> <mo>⊺</mo> </msup><mi mathvariant="bold">b</mi><mo>+</mo><mi>r</mi></mfenced> <mi>d</mi> </msup>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mtext>Gaussian</mtext>
          <mspace width="4.pt"/>
          <mtext>RBF:</mtext>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mspace width="1.em"/>
          <mi>K</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">a</mi>
            <mo>,</mo>
            <mi mathvariant="bold">b</mi>
            <mo>)</mo>
          </mrow>
          <mo>=</mo>
          <mo form="prefix">exp</mo>
          <mrow>
            <mo>(</mo>
            <mstyle scriptlevel="0" displaystyle="true">
              <mrow>
                <mo>-</mo>
                <mi>γ</mi>
                <msup><mfenced separators="" open="∥" close="∥"><mi mathvariant="bold">a</mi><mo>-</mo><mi mathvariant="bold">b</mi></mfenced> <mn>2</mn> </msup>
              </mrow>
            </mstyle>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mtext>Sigmoid:</mtext>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mspace width="1.em"/>
          <mi>K</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">a</mi>
            <mo>,</mo>
            <mi mathvariant="bold">b</mi>
            <mo>)</mo>
          </mrow>
          <mo>=</mo>
          <mo form="prefix">tanh</mo>
          <mfenced separators="" open="(" close=")">
            <mi>γ</mi>
            <msup><mi mathvariant="bold">a</mi> <mo>⊺</mo> </msup>
            <mi mathvariant="bold">b</mi>
            <mo>+</mo>
            <mi>r</mi>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46324203087664">
<h5>Mercer’s Theorem</h5>
<p>According<a data-type="indexterm" data-primary="Mercer's theorem" id="idm46324203086336"/><a data-type="indexterm" data-primary="Mercer's conditions" id="idm46324203084912"/> to <em>Mercer’s theorem</em>, if a function <em>K</em>(<strong>a</strong>, <strong>b</strong>) respects a few mathematical conditions called <em>Mercer’s conditions</em> (e.g., <em>K</em> must be continuous and symmetric in its arguments so that <em>K</em>(<strong>a</strong>, <strong>b</strong>) = <em>K</em>(<strong>b</strong>, <strong>a</strong>), etc.), then there exists a function <em>ϕ</em> that maps <strong>a</strong> and <strong>b</strong> into another space (possibly with much higher dimensions) such that <em>K</em>(<strong>a</strong>, <strong>b</strong>) = <em>ϕ</em>(<strong>a</strong>)<sup>⊺</sup> <em>ϕ</em>(<strong>b</strong>). You can use <em>K</em> as a kernel because you know <em>ϕ</em> exists, even if you don’t know what <em>ϕ</em> is. In the case of the Gaussian RBF kernel, it can be shown that <em>ϕ</em> maps each training instance to an infinite-dimensional space, so it’s a good thing you don’t need to actually perform the mapping!</p>

<p>Note<a data-type="indexterm" data-primary="sigmoid kernel" id="idm46324203085392"/> that some frequently used kernels (such as the sigmoid kernel) don’t respect all of Mercer’s conditions, yet they generally work well in practice.</p>
</div></aside>

<p>There is still one loose end we must tie up. <a data-type="xref" href="#from_alpha_to_w_and_b">Equation 5-4</a> shows how to go from the dual solution to the primal solution in the case of a linear SVM classifier. But if you apply the kernel trick, you end up with equations that include <em>ϕ</em>(<em>x</em><sup>(<em>i</em>)</sup>). In fact, <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mover accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover>
</math> must have the same number of dimensions as <em>ϕ</em>(<em>x</em><sup>(<em>i</em>)</sup>), which may be huge or even infinite, so you can’t compute it. But how can you make predictions without knowing <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mover accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover>
</math>? Well, the good news is that you can plug the formula for <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mover accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover>
</math> from <a data-type="xref" href="#from_alpha_to_w_and_b">Equation 5-4</a> into the decision function for a new instance <strong>x</strong><sup>(<em>n</em>)</sup>, and you get an equation with only dot products between input vectors. This makes it possible to use the kernel trick (<a data-type="xref" href="#making_predictions_with_a_kernelized_svm">Equation 5-8</a>).</p>
<div id="making_predictions_with_a_kernelized_svm" data-type="equation"><h5><span class="label">Equation 5-8. </span>Making predictions with a kernelized SVM</h5><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <msub><mi>h</mi> <mrow><mover accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover><mo>,</mo><mover accent="true"><mi>b</mi> <mo>^</mo></mover></mrow> </msub>
          <mfenced separators="" open="(" close=")">
            <mi>ϕ</mi>
            <mo>(</mo>
            <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow> </msup>
            <mo>)</mo>
          </mfenced>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mspace width="0.166667em"/>
          <msup><mover accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover> <mo>⊺</mo> </msup>
          <mi>ϕ</mi>
          <mrow>
            <mo>(</mo>
            <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow> </msup>
            <mo>)</mo>
          </mrow>
          <mo>+</mo>
          <mover accent="true"><mi>b</mi> <mo>^</mo></mover>
          <mo>=</mo>
          <msup><mfenced separators="" open="(" close=")"><munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi> </munderover><msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><msup><mi>t</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mi>ϕ</mi><mrow><mo>(</mo><msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>)</mo></mrow></mfenced> <mo>⊺</mo> </msup>
          <mi>ϕ</mi>
          <mrow>
            <mo>(</mo>
            <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow> </msup>
            <mo>)</mo>
          </mrow>
          <mo>+</mo>
          <mover accent="true"><mi>b</mi> <mo>^</mo></mover>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mspace width="0.166667em"/>
          <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi> </munderover>
          <msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
          <msup><mi>t</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
          <mfenced separators="" open="(" close=")">
            <mi>ϕ</mi>
            <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>)</mo></mrow> <mo>⊺</mo> </msup>
            <mi>ϕ</mi>
            <mrow>
              <mo>(</mo>
              <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow> </msup>
              <mo>)</mo>
            </mrow>
          </mfenced>
          <mo>+</mo>
          <mover accent="true"><mi>b</mi> <mo>^</mo></mover>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mstyle scriptlevel="1" displaystyle="false"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mstyle> <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>&gt;</mo><mn>0</mn></mrow></mstyle></mfrac> <mi>m</mi> </munderover>
          <msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
          <msup><mi>t</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
          <mi>K</mi>
          <mrow>
            <mo>(</mo>
            <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
            <mo>,</mo>
            <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow> </msup>
            <mo>)</mo>
          </mrow>
          <mo>+</mo>
          <mover accent="true"><mi>b</mi> <mo>^</mo></mover>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
</div>

<p>Note that since <em>α</em><sup>(<em>i</em>)</sup> ≠ 0 only for support vectors, making predictions involves computing the dot product of the new input vector <strong>x</strong><sup>(<em>n</em>)</sup> with only the support vectors, not all the training instances. Of course, you need to use the same trick to compute the bias term <math xmlns="http://www.w3.org/1998/Math/MathML"><mover><mi>b</mi><mo>^</mo></mover></math> (<a data-type="xref" href="#bias_term_using_the_kernel_trick">Equation 5-9</a>).</p>
<div id="bias_term_using_the_kernel_trick" data-type="equation"><h5><span class="label">Equation 5-9. </span>Using the kernel trick to compute the bias term</h5>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mover accent="true"><mi>b</mi> <mo>^</mo></mover>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mn>1</mn> <msub><mi>n</mi> <mi>s</mi> </msub></mfrac>
          </mstyle>
          <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mstyle scriptlevel="1" displaystyle="false"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mstyle> <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>&gt;</mo><mn>0</mn></mrow></mstyle></mfrac> <mi>m</mi> </munderover>
          <mfenced separators="" open="(" close=")">
            <msup><mi>t</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
            <mo>-</mo>
            <msup><mrow><mover accent="true"><mi mathvariant="bold">w</mi> <mo>^</mo></mover></mrow> <mo>⊺</mo> </msup>
            <mi>ϕ</mi>
            <mrow>
              <mo>(</mo>
              <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
              <mo>)</mo>
            </mrow>
          </mfenced>
          <mo>=</mo>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mn>1</mn> <msub><mi>n</mi> <mi>s</mi> </msub></mfrac>
          </mstyle>
          <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mstyle scriptlevel="1" displaystyle="false"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mstyle> <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>&gt;</mo><mn>0</mn></mrow></mstyle></mfrac> <mi>m</mi> </munderover>
          <mfenced separators="" open="(" close=")">
            <msup><mi>t</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
            <mo>-</mo>
            <msup><mrow><mfenced separators="" open="(" close=")"><munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi> </munderover><msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow> </msup><msup><mi>t</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow> </msup><mi>ϕ</mi><mrow><mo>(</mo><msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow> </msup><mo>)</mo></mrow></mfenced></mrow> <mo>⊺</mo> </msup>
            <mi>ϕ</mi>
            <mrow>
              <mo>(</mo>
              <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
              <mo>)</mo>
            </mrow>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mn>1</mn> <msub><mi>n</mi> <mi>s</mi> </msub></mfrac>
          </mstyle>
          <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mstyle scriptlevel="1" displaystyle="false"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mstyle> <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>&gt;</mo><mn>0</mn></mrow></mstyle></mfrac> <mi>m</mi> </munderover>
          <mfenced separators="" open="(" close=")">
            <msup><mi>t</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
            <mo>-</mo>
            <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mstyle scriptlevel="1" displaystyle="false"><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></mstyle> <mstyle scriptlevel="1" displaystyle="false"><mrow><msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow> </msup><mo>&gt;</mo><mn>0</mn></mrow></mstyle></mfrac> <mi>m</mi> </munderover>
            <mrow>
              <msup><mrow><mover accent="true"><mi>α</mi> <mo>^</mo></mover></mrow> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow> </msup>
              <msup><mi>t</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow> </msup>
              <mi>K</mi>
              <mrow>
                <mo>(</mo>
                <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
                <mo>,</mo>
                <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow> </msup>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
</div>

<p>If you are starting to get a headache, it’s perfectly normal: it’s an unfortunate side effect of the kernel trick.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>It is also possible to implement online kernelized SVMs, capable of incremental learning, as described in the papers <a href="https://homl.info/17">“Incremental and Decremental Support Vector Machine Learning”</a>⁠<sup><a data-type="noteref" id="idm46324202871360-marker" href="ch05.xhtml#idm46324202871360">7</a></sup> and <a href="https://homl.info/18">“Fast Kernel Classifiers with Online and Active Learning”</a>.⁠<sup><a data-type="noteref" id="idm46324202869248-marker" href="ch05.xhtml#idm46324202869248">8</a></sup> These kernelized SVMs are implemented in Matlab and C++. But for large-scale nonlinear problems, you may want to consider using Random Forests (see <a data-type="xref" href="ch07.xhtml#ensembles_chapter">Chapter 7</a>) or neural networks (see <a data-type="xref" href="part02.xhtml#neural_nets_part">Part II</a>).</p>
</div>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="idm46324203279312">
<h1>Exercises</h1>
<ol>
<li>
<p>What is the fundamental idea behind Support Vector Machines?</p>
</li>
<li>
<p>What is a support vector?</p>
</li>
<li>
<p>Why is it important to scale the inputs when using SVMs?</p>
</li>
<li>
<p>Can an SVM classifier output a confidence score when it classifies an instance? What about a probability?</p>
</li>
<li>
<p>How can you choose between <code>LinearSVC</code>, <code>SVC</code>, or <code>SGDClassifier</code>?</p>
</li>
<li>
<p>Say you’ve trained an SVM classifier with an RBF kernel, but it seems to underfit the training set. Should you increase or decrease <em>γ</em> (<code>gamma</code>)? What about <code>C</code>?</p>
</li>
<li>
<p>What does it mean for a model to be <em>ϵ-insensitive</em>?</p>
</li>
<li>
<p>What is the point of using the kernel trick?</p>
</li>
<li>
<p>Train a <code>LinearSVC</code> on a linearly separable dataset. Then train an <code>SVC</code> and a <code>SGDClassifier</code> on the same dataset. See if you can get them to produce roughly the same model.</p>
</li>
<li>
<p>Train an SVM classifier on the Wine dataset, which you can load using <code>sklearn.datasets.load_wine()</code>. This dataset contains the chemical analysis of 178 wine samples produced by 3 different cultivators: the goal is to train a classification model capable of predicting the cultivator based on the wine’s chemical analysis. Since SVM classifiers are binary classifiers, you will need to use one-versus-all to classify all 3 classes. What accuracy can you reach?</p>
</li>
<li>
<p>Train and fine-tune an SVM regressor on the California housing dataset. You can use the original dataset rather than the tweaked version we used in Chapter 2. The original dataset can be fetched using <code>sklearn.datasets.fetch_california_housing()</code>. The targets represent hundreds of thousands of dollars. Since there are over 20,000 instances, SVMs can be slow, so for hyperparameter tuning you should use much less instances (e.g., 2,000), to test many more hyperparameter combinations. What is your best model’s RMSE?</p>
</li>

</ol>

<p>Solutions to these exercises are available at the end of this chapter’s notebook, at <a href="https://homl.info/colab3"><em class="hyperlink">https://homl.info/colab3</em></a>.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm46324203756640"><sup><a href="ch05.xhtml#idm46324203756640-marker">1</a></sup> Chih-Jen Lin et al., “A Dual Coordinate Descent Method for Large-Scale Linear SVM,” <em>Proceedings of the 25th International Conference on Machine Learning</em> (2008): 408–415.</p><p data-type="footnote" id="idm46324203711232"><sup><a href="ch05.xhtml#idm46324203711232-marker">2</a></sup> John Platt, “Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines” (Microsoft Research technical report, April 21, 1998), <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-98-14.pdf"><em class="hyperlink">https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-98-14.pdf</em></a>.</p><p data-type="footnote" id="idm46324203431584"><sup><a href="ch05.xhtml#idm46324203431584-marker">3</a></sup> Zeta (<em>ζ</em>) is the sixth letter of the Greek alphabet.</p><p data-type="footnote" id="idm46324203381376"><sup><a href="ch05.xhtml#idm46324203381376-marker">4</a></sup> To learn more about Quadratic Programming, you can start by reading Stephen Boyd and Lieven Vandenberghe’s book <a href="https://homl.info/15"><em>Convex Optimization</em></a> (Cambridge University Press, 2004) or watch Richard Brown’s <a href="https://homl.info/16">series of video lectures</a>.</p><p data-type="footnote" id="idm46324203361584"><sup><a href="ch05.xhtml#idm46324203361584-marker">5</a></sup> The objective function is convex, and the inequality constraints are continuously differentiable and convex functions.</p><p data-type="footnote" id="idm46324203248480"><sup><a href="ch05.xhtml#idm46324203248480-marker">6</a></sup> As explained in <a data-type="xref" href="ch04.xhtml#linear_models_chapter">Chapter 4</a>, the dot product of two vectors <strong>a</strong> and <strong>b</strong> is normally noted <strong>a</strong> · <strong>b</strong>. However, in Machine Learning, vectors are frequently represented as column vectors (i.e., single-column matrices), so the dot product is achieved by computing <strong>a</strong><sup>⊺</sup><strong>b</strong>. To remain consistent with the rest of the book, we will use this notation here, ignoring the fact that this technically results in a single-cell matrix rather than a scalar value.</p><p data-type="footnote" id="idm46324202871360"><sup><a href="ch05.xhtml#idm46324202871360-marker">7</a></sup> Gert Cauwenberghs and Tomaso Poggio, “Incremental and Decremental Support Vector Machine Learning,” <em>Proceedings of the 13th International Conference on Neural Information Processing Systems</em> (2000): 388–394.</p><p data-type="footnote" id="idm46324202869248"><sup><a href="ch05.xhtml#idm46324202869248-marker">8</a></sup> Antoine Bordes et al., “Fast Kernel Classifiers with Online and Active Learning,” <em>Journal of Machine Learning Research</em> 6 (2005): 1579–1619.</p></div></div></section></div></body>
</html>